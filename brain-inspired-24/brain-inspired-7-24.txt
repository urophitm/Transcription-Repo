 Why do things feel like stuff? Why do we engage in the behaviors we behave in? You know, not why in the normally scientific, you know, reductionist sense, what are the mechanisms, once we hold the behavior fixed? But instead, if we don't hold the behavior fixed, what are you or any other organism going to choose and why that choice instead of something else? There's sort of this problem where in Neuro, we are often doing paradigms or tasks that from a pure AI point of view might be considered almost trivial. But from a biological plausibility point of view, that often makes them hard again. I had actually been prepared for the concept that you might walk arrogantly into experimentation with some grand theory and think this is going to totally be right and you do your first experiment and it's totally wrong. And in fact, that happened. This is Brain Inspired, powered by the transmitter. Good day to you. I am Paul. This is Brain Inspired podcast, as you just heard. Eli Sanesh is a postdoc at Vanderbilt University, one of my old stomping grounds. Eli is currently in the lab of Andre Bostos. Andre's lab focuses on understanding brain dynamics within cortical circuits, particularly how communication between brain areas is coordinated in things like perception and cognition and behavior. Eli is busy doing work along those lines these days as you'll hear more about in a moment. But the original impetus for having him on this podcast is his recently published proposal for how predictive coding might be implemented in brains. In that sense, this episode builds on the last episode with Rajesh Rao, where we discussed Raj's active predictive coding account of predictive coding. I've said predictive coding multiple times now, so as a super brief refresher, predictive coding is the proposal that the brain is constantly predicting what's about to happen. Then stuff happens and the brain uses the mismatch between its predictions and the actual stuff that's happening to then learn how to make better predictions moving forward. So I refer you to the previous episode for more gruesome details about that process. Eli's account of predictive coding and how it might be implemented in brains, along with his co-authors, of course, they call it, quote unquote, divide and conquer predictive coding. And you'll hear why in our discussion. The divide and conquer approach among other things uses a probabilistic approach to account for how predictive coding might be implemented in brains. But we also talk quite a bit about the difference between practicing theoretical and experimental neuroscience and Eli's experience moving into the experimental side from the theoretical side, which, well, you'll hear, it turns out everything has its own challenges, let's say. All right, show notes for this episode, our braininspired.co slash podcast slash 200 and two. As always, thank you for being here. Thank you for listening. Thank you to the transmitter for your support of this podcast. And thank you to the patrons who also reach out and support. Here's Eli. So Eli, are you ready? Yes. We were just chatting about how you are just a few floors up from where I did my postdoc. And this is your first postdoc, right? Yes. Yeah. And in Nashville, Tennessee. And I'm curious. And so this is kind of a, in some sense, a follow-up episode, a follow-up episode, because I just had Roger Shroud on to talk about his active predictive coding work from, which updates the original predictive coding framework from 1999, that focused all on sensory. Right. And so what he did here was like basically bring in an action part of the story into the predictive coding story. Yeah, very lucky timing. We actually just read his APC paper in journal club. Oh, really? We did it a few weeks ago. Yeah. It was helpful. But so, and so we'll get to your related work, compare, contrast, et cetera. But first, I kind of, I know you have a computer science background. And I kind of, I'm trying to understand your world view. People ask me, well, my world view, and I can't describe it because it assumes I have a world view. But like how you approach the world, because I know you have that background in computer science. And so that's, there's kind of a computational, I don't mean dry in a bad way, but a very computational kind of algorithm centric approach that I thought, well, maybe that's kind of where he's coming from. But then I know you did some work with Lisa Feldman Barrett. It's all about feelings and how that drives so much of what, how we interact with the world. So I'm just curious what your kind of world view in the neuroscience is. And those sciences is, oh man, he's getting comfortable. Yes. Okay, so I mean, there is actually, I don't know if I have a world view, but I sort of have a direction and vibe. Okay, oh, I like that. That's a good way to phrase it. Like I feel things out for what I think could be a workable scientific approach to try and address, you know, the questions I'm sort of interested in. And overall, I feel like, you know, the question I'm interested in is like, this is going to sound even sillier than saying consciousness. I'm sorry to say, but why do things feel like stuff? Oh, why do we engage in the behaviors we behave it? But why in the, you know, not why in the normally scientific, you know, reductionist sense, what are the mechanisms, what to be hold the behavior fixed? But instead, if we don't hold the behavior fixed, what are you going, you know, what are you or any other organism going to choose and why that choice instead of something else? What does that mean hold the behavior fixed? Oh, okay. So now I just get to channel these up straight up. Okay. So often in neuroscience experiments, and I'm thinking particularly actually, you know, some of the animal experiments we do here at Vandy, we basically, you know, head fix your animal. You know, like first you chair the animal, head fix, then you train them to fix it on a dot on screen. Monkeys are much smarter than mice so they can do this in exchange for juice. Right. Yeah, just described my entire academic career there. Yeah, an increasing portion of mine. Right. And you know, then you basically have them move their eyes as the only motor output of whatever you're having them do. And most highly constrained lab experimental setup as you can so that when you're asking the question does, for example, frontal I field do in code, some sort of decision process related to the behavior, you don't have to worry about all of the other conflating factors that are involved in the other behaviors. Yes. And so, you know, I'm very, I'm a strong believer that experiment is theory laden. And this means that if you're doing one of these highly constrained experiments and you have a theory about what frontal I field is doing, great. You've controlled everything else so that you can test your theory about frontal I field. Now back to channeling Lisa. You know, if what you're trying to investigate is not something that you can leave, you know, something that you've left unconstrained in your setup, then you can't actually test theories about it. So I would say for instance, you know, like can you use a head fixed monkey and a chair to strongly test, you know, these physiological theories about allostasis, interception, these other nice keywords that I wrote about. With like Lisa and Karen. Questionable. That is really how I ended up. Gosh, there's a whole story actually of how I ended up working with Lisa and Karen. Yeah, let's hear it. If you're willing to divulge. Yeah. So the short version is the stars aligned in a way that they never have before since. Obviously, it doesn't every academic have that story. That's right. But, you know, really it's that I had this computer science background. And then late in my master's actually, I started getting interested in like cognitive science, sort of very like Brendan Lake, Josh Tenenbaum type of stuff. And so I had to like spend a couple years studying things to sort of go back and try and give myself the background to engage with any of this, try to change direction. And I discovered that like what I was really interested in was sort of the feeling and the why. And so I started trying to figure out, well, okay, who has an approach that is kind of like this to these subjects. Where the kind of like this is sort of, you know, very probabilistic. You know, they were using like probabilistic programs to model concept learning. And this was all working very nicely for them. You know, they had that science paper in 2015. I was so impressed. You know, I started reading neuroscience. And you decided to continue. Oh, no, it just gets so much worse. I'm kind of embarrassing myself here. You're going to, you could probably guess now if someone's looking for probabilistic approaches in neuroscience mid 20, mid to late 2010s, who are they running into? Oh, well, not many people, but you've already named some of them. But that I was about to ask you why probabilistic. Maybe we could start there. Because I was just, I mean, at the time I was a amateur and I was just sort of vibing and trying to go. You know, trying to go from one thing that I felt like I could kind of understand to another thing I felt like I could kind of understand. And, you know, at least partly at the beginning, you know, the interesting part about the 10 and bomb and late kind of work to me was, hey, unlike that old field of AI that I took a course in in undergrad. And hated it. This is symbolic style old field or connectionism old field. Oh, I mean, I went to undergrad at UMass Amherst. So the AI class was symbolic search heuristics, all of that. Yeah. Logic, then the machine learning classes that I didn't take at the time were like random forest SVMs. You know, I think there was some neural networks, but they've hired a lot more people doing neural networks since then. You know, and like to be, oh, RL, RL was absolutely huge at UMass Amherst to the point that they hosted the RL conference, you know, in Amherst this past year. And like, I eventually realized like, oh, those big guys, Sutton and Bart, wait, Bart, oh, like Andy Bart, who would just walk through the hall. Yeah, that in Bart. That's isn't that a super interesting thing about academia when you meet, I don't know, you know, maybe hero is the wrong word, but these sort of God heads of classic things. And then, oh, they're just regular folks. I mean, that's why I say this is so embarrassing is like, I actually went to school in a department full of such great people. When you don't know, you don't know. Right, and I just like, I was honestly kind of dismissive about it because I was like, okay, this is all just, you know, heuristic search. Like it's heuristic search, you throw a lot of processing power at it, and maybe sometimes it kind of works, but like this is really, like this is not actually how a brain or a mind would work. You know, this isn't the real thing. And then when I started reading those like 10 and down and Blake things, you know, they were saying, well, we've fit to behavior. You know, we've done an actual experiment and checked. So we're not just defining some toy task that we can then solve computationally with reasonable ease and then go back and forth between approximations and heuristics. You know, for the rest of our careers until an AI winter hits and wipes us out. Gosh, maybe I did take something from you mess at first, actually. Like maybe I took some residual post trauma from the AI will turn. But yeah, they were, they were fitting behavior and actually fitting old white variety or a reasonable variety of experimental tasks with human participants. I said, OK, now there's something here. Like now there's a real world to compare against. So then how, how did that take you to like Lisa? Oh, sorry, yes. So I was trying to prompt you for the name, Friston. Yeah, sure. Carl Friston just to be. Yes, Carl. Yeah, so actually via, let's see, I was working with this embedded electronics company. I still have the hoodie over there. And like they had an MIT post doc. He mentioned some of the Carl Friston stuff around the same time that Andy Clark's book came out. So that surfing uncertainty. Is that that one? And that one had a lot of citations to people that I already, you know, names I already recognized. So I read it. I went absolutely wild for it. And he was sort of mentioning in the book, like, you know, there's some people who are actually applying this approach to emotion. I see. And even better. The people who were applying this approach to emotion, you know, Lisa and Karen. Or at least Lisa and Karen locally to me, right? I was in Boston at the time. You know, had a collaborator in this big interdisciplinary group that they had tried to form and maintain with varying success. It really shown for a while. And I think the pandemic might have done it in a little bit. But you know, they had a collaborator, Jan Villam Vandamaint, who actually did the computational side, probably holistic programming. So of all damn things, I wrote a cold email. Knowing no better way to go about this. Yeah. And they actually answered. Well, they were probably fairly thirsty for someone, you know, interested in it. Because it's not still not that widespread, right? Yeah, none. I mean, as far as I know, like none of this is widespread. If you take the first and stuff too seriously, people say you're in a cult. I actually didn't join the cult until later. When I met Maxwell Ramsted, he's like eventually convinced me of a lot of free energy stuff. Wait, so, so, yeah, Carl was famous for the free energy principle. And he considers it a framework, not a theory, by the way. When, when pressed, at least a couple years ago, he considers it a framework for thinking about the globe overall function of the brain instead of a theory for what it's worth. And, and it has a lot of detractors and a lot of cheerleaders. But, and so you drank the cool aid eventually. I sift the cool aid didn't go all in. And at the time I was being given it to sift, I had sort of been around Lisa and Karen enough that I had really absorbed like, nope, you gotta have your evo divo, your neuro anatomy. You're mapping onto actual biology. It's the biology that really, really, really counts. Right. Okay. So I drunk my advisors cool aid instead of the cult's cool aid. Okay. Good for, you know, good for them, good for your advisors. But, but some people with your background would then instead of embracing the biological neuro plausibility would go the other direction, right? Back to where it feels safer. I mean, I, I'll just say like right before, you know, we spoke for two minutes before I press record and you were talking about how neuro AI is hard. Is it the neuro part that's hard? So I'm, I don't want to make like the public announcement, right? But like, I mean, it's not like, okay, you know what? No one cares. No one is ever thinking about you, right? You're on camera. You're in front of an audience, but no one's ever thinking about you. Sure. Okay. I think I might well end up heading in sort of the neuro AI direction. I don't want to say as just like a career, like just for the money or, you know, for a career prospects thing. But what I have noticed is that a lot of computational neuroscientists are sort of renaming their work that now. So yeah, it's a big, I just, I just got back from a brain initiative workshop called neuro AI. I just got back from a Norway workshop called neuro AI and that term is really being embraced. Because it sounds cool, I think, mostly. Yeah. And I've sort of got this impression that like the real difference between one thing and another is basically what was your training and what department are you looking for a job in? And the number of departments, I think the number of departments that would do my completely ideal thing is null. You know, and I'm sure most people end up saying that well before they go on the job market, but. And I'm not going on the job market right now. So luckily lucky me, but. What would that be? Can you describe what that would be? Ideally, like really, really question based or question driven science, something close to cognitive science in my PhD, I used to make up a fantasy field computational affective science. Okay. By, you know, by analogy to computational cognitive science. Now, computational cognitive science is already a fairly small subfield. And that often overlaps, you know, into the computer science departments, because that's who will give some of them jobs. And the number of cognitive science departments at universities. You know, that do like the full six discipline, you know hexagonal multi handshake thing. Is a handful or less, yeah, or less like. So there's psychology departments who want you to do psychology experiments. There's neuroscience departments who often want you to do neuroscience, either theory or experiment, but they're defining the discipline. Often quite narrowly. Like I had a culture shock when I came to Vanderbilt and found out that what they mean by computational modeling or theory. Is basically like biophysical or bust. And then I found out that I was not really a biophysical scientist. And I was like, you know, I'm not sure if you're talking with, right? Because you have people like Gordon Logan there also who that's that's I'm not sure if you run around run past him much. If he, I'll act if he even is still. I don't run into him, but yeah, yeah, you know, at least I'm talking. Let's say about my lab and a couple other labs that I interact with. You know, biophysical or don't do anything at all. Or biophysical or give up theory and become an experimenter. So how it's okay. So then where do you sit in relation to to that push, right? I'm trying to suss out your like your level of abstraction and what you think is important. So my level of abstraction is that when I reached the end of my PhD, I said, okay, I formally did my PhD in a computer science department. If I'm ever going to really investigate questions, I need to go get experimental training. Yeah, I said, tell me this a while back. Yeah. Yeah. Yeah. So, you know, I basically said, all right, I'm going to go get as hardcore a post doc as I can. And that was the biggest mistake you've made. No, just kidding. But, but is that what is that what you're saying? Like the difficulty of nor AI is the joining of the two kind of like that experimental and computational approach. Yeah. Like it's not a mistake to go and yet experimental experience, but is a culture shock. It took me about six months to really be able to make progress on absolutely anything on the experimental side. Why is that? Why is that? So I just, I mean, I know these things and people who do experimental work, we all kind of cry together, you know, I'm going to talk about how hard everything is and, you know, but it, but it, you know, in my case, they're, you know, I'd rather not talk about it. It's sort of private to allow stuff. Sure. Okay. You know, I don't want to. But suffice it to say that you run into way more problems than you would imagine you might with that PS summary of it. Yes. Yeah. Yes. Way, way, way more. And the thing is I had been prepared for the, I had actually been prepared for the concept that you might walk arrogantly into experimentation with. Some grand theory and think this is going to totally be right and you do your first experiment and it's totally wrong. Complete no result. And in fact, that happened. But I was prepared for that. The part that I was much less prepared for is, how do I even connect a theory to an experiment? So the part that I wasn't, you know, no results were sort of a thing that I like, steeled myself, you know, work it out on, you know, work it out exercising basically, just try to sweat until you can't be frustrated anymore that your theory is wrong. Oh, well, the theory is wrong. Even while you're submitting a theory paper about it. But see, that is in the pop area in sense, that is the best kind of progress, right? Because it's an answer. It's an answer, though, I hate to say it, but now that I'm looking at another way of analyzing the data, it might get more complicated again. Sure. Yeah. So let me tell you about the actual experiment that we have in both mice and McCaxx. You know, we have this thing called the glow paradigm global local oddball. So, you know, first to give three identical stimuli per trial, AA, this used to be done in auditory. Now we're doing it in, you know, we've been doing it in visual. And then, you know, the local oddball is that for stimulus is B. It's something different. Well, okay, what the heck is a global oddball? You know, in our manuscripts, we describe it as more complex oddballs. Well, a global oddball is where we set up the expectation for the animal, right? We try to intervene on the internal model and, you know, make it think there's a bee coming, but then we give it an A. So let's say what we end up doing is testing. These are intermixed for the animal about 80 20. So 80% local 20% glow. So you're really setting up the expectation? Yes, actually, there's, you know, 50, there's like days and days of habituation followed by 50 trials of pure local oddball at the beginning of recording. So that we're basically, you know, habituating and queuing the expectations as powerfully as we can. And so what we're trying to do is disentangle, you know, what happens if you have a predictable change versus an unpredictable repetition. And the idea is from a neurophysiologist's point of view is that, you know, then at the end, you're going to have a bunch of controls, like those come after the main block. So after the main block, we record a series of essentially control sequences that are going to allow us to do statistical contrasts. And the idea is to then eventually say, all right, well, if you can figure out, if you can control for every other mechanism, you can think of. So adaptation of the sensory neurons and R1. You know, like, this is where it starts to get really messy and hard also. Well, not just messy and hard, but like if you can control for everything you think of, and there's still some difference between global oddball, AAA, unexpected A, and just pure repetition or adaptation AAAA. Then AAA, now you've found a signature of surprise processing. And for a long time, I have just been staring at this experimental set of going, how is that surprise processing? Or like what theory have we articulated about predictive coding in the R1 ballard sense that says this is surprise processing, you know, rather than... I mean, you know, who says the brain is tuned to look at angled gradings, moving angled gradings on a screen that flash on and off? Well, you can... Okay, so in other words, you can't control for everything. Or it's not just that you can't control for everything. It's that, as I said, I believe, you know, experiments is theory laden. And if your theory is about the brain, you know, predicting the continuous stream of sensory input, then flashing a series of angled gradings that are optimized essentially to drive, you know, V1 to a maximum degree. Well, under predictive coding theory, that's saying you're trying to drive... you're trying to optimize prediction error. So how do we expect to simultaneously optimize prediction error while also provoking another kind of prediction error? That being a surprise? Yeah. Or like... Well, that's the thing. Our setup, you know, conflates, like prediction error, surprise, you know, visual change. Yeah, right, because you're using that oddball. There's a visual difference in oddball that you're using. Oh, and I didn't get... I should have said, actually, this is pretty much the standard paradigm as it turns out for studying predictive coding and goes back to about 09. Can you... You said that surprise and prediction errors are often conflated. So what is the difference then between surprise and prediction error? Theoretically, perhaps, maybe if... Yeah, so I would say you need to commit yourself to a theory in order for there to be a difference. But then the problem is if you're trying to test a particular theory, you should use the definitions from within that theory. Okay. So prediction errors within predictive coding theory, you know, they're the residual when you subtract the prediction from the data. Yeah, what the organism expects, top down signals, then it gets some observational data, bottom up signals, and then there's a difference in the mismatch between the prediction and the actual observable data, and that's what gets passed forward. Exactly. And... I guess I would... Well, okay, so how to relate that to surprise. You know, I would reach for my information theoretic definition because I'm a quant person, and say, okay, well, surprise is the negative log probability of stimulus. You know, and essentially those would be two different quantities. You know, when I eventually wrote my own computational modeling paper, prediction error was the gradient of surprise. So they're related, but distinct, and you sort of have to use math to talk about how. But, you know, I guess I'm trying to just describe the culture shock of going from, you know, sort of this environment that was... It wasn't oil and water, you know, we mixed, but like there was a very quantitative side that I worked on, and a very biological side. And then, you know, I come to this, like, glow paradigm, this experiment, and I find that, oh, the quantitative side is just moved out from underneath. I have to reconstruct it entirely myself. So that's what you were getting at when you were talking about how... What we'll call it, neuro AI is hard. Yeah. Like actually taking... There's sort of this problem where in neuro, we are often doing paradigms or tasks that from a pure AI point of view might be considered almost trivial. Yeah. But from a biological plausibility point of view, that often makes them hard again. And then, if you're actually trying to explain neuronal data. Or worse, trying to map some real theory of the brain onto neuronal data. Rather than just, you know, suggest that there could exist some mechanism explaining this behavior. Because, you know, like, there's been multiple computational models of same, you know, of the same behaviors. I'm sort of thinking of like the famous drift diffusion models of, you know, decision making. Yeah. Like, how do you know if the brain is doing a drift diffusion, you know, accumulate evidence to a threshold and then decide algorithm for decision making or, you know, resource constrained reinforcement learning algorithm for decision making. There are experiments that have been fit with both these kinds of models. Yeah. That's right. How do you know? Massive shock for me that there's just like, oh, wait, does everyone just pretending? What do you mean pretending? Pretending that what they're doing is valid and what everyone else is doing is not or what? Well, pretending that, like, just taking data and... Oh. You know, fitting it such that you can claim to use your theory to explain behavior. But you haven't actually tested it against substantive alternative theories rather than some kind of null hypothesis. Like, what the heck is our null hypothesis regarding behavior in the brain? Or alternative hypotheses. It doesn't even have to be null, just a clear alternative. Yeah. Yeah. There's something that actually that Jeff Shawl, I'll just elevate him in this regard. Like, every year when I was a postdoc, there's a fundamental set of papers, one of which is like the method of alternative hypotheses, where we tried to base. But I think because of these things, because it's hard. Like, you mentioned drift diffusion. And I was doing drift diffusion work essentially stochastic accumulator work, which is exactly what you're saying. Does the neuron like ramp up to some threshold? And then that actuates the behavior. And that's one of the things that Jeff Shawl is famous for. And so, you know, his idea is to look in the brain and test it and ask it, right, through recordings. And of course, it's not super clean. Because we're dealing with different kinds of stimuli in this very controlled environment, the frontal I field. As we know now, any given brain area doesn't just have a single function. Right. So there's, you know, mixed selectivity in brain areas where they're doing overlapping populations of neurons or doing overlapping functions things. So, but anyway, oh yeah, I mean any talk of like frankly any talk of selectivity slightly makes me want to scream. And I've just been reoculturating myself to an environment where like the word degeneracy and, you know, to an environment where these things are not the assumptions anymore. Wait, where degeneracy is not an assumption where degeneracy isn't the assumption, you know, top down influences often aren't the assumption. Like it's a very, and I'm not saying this as a negative thing. Oh, certain way I like it, even though I don't think I can make a career out of it. Like very, Andy Clark quoting, fine, you know, had this thing about desert landscapes. Like a neurophysiologist point of view is a very desert landscape point of view. There's the things I can measure nothing else. Nothing else exists. I'll talk about selectivity because I think I can measure it. And if you tell me that that's actually caused by what I do rather than an observation of a causally independent system, then I will get an argument with you because I think I'm measuring something real. I see. So what you're describing, it's interesting that you find yourself in that world now because in some sense, that's kind of the old school world, which is still very much alive and thriving. Whereas there's been this recent push into a much more, you know, more naturalistic types of tasks and removing the constraints from the lab, you know, the lab based experimental stuff. And that's hard in a very different way. So let me, you know, make some applause or give some applause to Andre here, right? I think he doesn't do that kind of experiment yet because he's actually pushing something that's already very risky and innovative. He calls it Madeline, multi area high density laminar electrophysiology. Okay. Which basically amounts to saying, you know, let's have like not just one neuropixels probe in one area. Let's just cover the brain in neuropixels probes. Yeah. So neuropixels probes are like these really high density multi electrode probes. So that when you put them in any given area of the brain, you're getting recordings of hundreds to sometimes thousands of neurons. And actually, yeah. You know, and all of our work includes the LFP, the local field potential as well as, you know, the individual spiking signals. And then we analyze both together, which, you know, I won't say who, but like someone I really, really respect a lot. I went and visited their lab. Actually one of my scientific heroes. I went and visited their lab at one point. Can't say who you can't say who. Yeah, I'm realizing I can't even specify this little, well, the point being at one point I asked, you know, do you analyze the LFP? And they said, no, we just look at the spiking. You know, and I think, you know, respect to Andre, like he's. I didn't talk about it before because like it's not as native a part of my worldview. It's what I'm learning. You know, this is actually a very ambitious thing. You know, even for a simple experiment, we'll have like two full neuropixels probes taking multi unit activity, individual spikes that we sort with kill us or, you know, then LFP. Sort of LFP is what people talk about. As measuring when people use the term oscillations. Sorry, I said, yeah. No, I was saying population level signal. Oh, there's that too. Yeah, but, but it's a different, it's a kind of a complimentary signal. The other thing is spikes are definitely the outputs of neurons, whereas LFP is thought to more, more closely track that population level input. Yeah. So then we also, you know, and then we like analyze both. So we're often doing, you know, cross correlation or coherence measures of like LFP to spike. And this actually tells you quite a lot. And it's, you know, it's difficult to send vicious. And my understanding is that it's also not easy to get grants in. Like I think Andre won his NSF career just this year. And that was the first grant that the lab had gotten in, I think, possibly three years of operation. Four joint is specifically for joint spike LFP analyses for Madeline as a whole. Yeah. Okay. For like this research program of. You know, let's measure in multiple areas. Let's measure the LFPs and the spikes. Let's try to capture as much as we can. So to speak as many times as we can. Let's really try to push the limits on how dense the sampling can be in electrophysiology. Because of, you know, essentially the resolution in issues with imaging. Or EEG that you would not want to use those. You would want to use electrophysiology. Yeah. So, okay. So backing up here. So I was just at this brain initiative workshop. And it was brought up multiple times. You know, so. The idea was to think in terms of like, well, what would we need in 10 years? Lesson ambitious goal for 10 years in early eye. And two people, one person suggested this. And then it was echoed by another person that what we need is to be able to record synaptic strengths. So, you know, for example, neural networks. The strength between the units is where all the parameters are. That's those billions and billions of parameters that in these large language models, etc. Those are what get changed. That that's strength between in the connections. And if there was just a way for us to measure that in the brain, then that's an ambitious goal. And it's a worthwhile goal. My immediate thought was. You know, there's that age old question like, what would you do if you could measure all of the spiking from all of the neurons? Would you even know what to do with it? And no, the questions. No, we don't because because you because it goes back to the theory ladenness. Like you have to have you have to come from some sort of framework or theory. To then ask questions of that data. So just collecting the data is not going to get you that right? Yes. And I think that's where. That's where I'm just going to put my cards on the table and say I think that's an open challenge for the field. And I'm happy to be working on it. What what is the open challenge? Sorry to to figure out how the heck you analyze your data in a properly, you know, theory driven or question driven way. Rather than just. I don't want to say this like it's too bad of a thing, right? But rather than just running statistics and then saying I found an effect. Well, that's that's interesting interesting because that's kind of what the AI side does in neuro AI. It's like throwing a bunch of statistics at the data and even Terry Sinowski brought this up at the workshop. Like what what principles have we learned? What what principles are there to gain from this approach? Right? Yes. Here's where I would sort of reach back into my training with young Billum as a probabilistic programmer and say for God's sakes, we need to be writing down generative models, fitting them to data and then doing model comparison. You know, we need to actually have some measure of how well does something fit the data? What theory motivates it? And then you know compare them in a principled way. And I think that you know machine learning can actually help with that. And I've seen a lot of very, very productive. You know, and like a flurry of new work. Essentially in just analyzing neural data. But then you also have to convince here's the hard part. Those things can get published in machine learning conferences. And then you have to both teach the experimenters to use them and convince them to use them and teach it to them in such a way that. They don't need you as a statistician or machine learner. To actually, you know, stand over their shoulder telling them how to encode every little hypothesis because you want them to use it a dozen different times. And they can't just keep you around forever as some kind of consulting machine learner. Right. Well, you know, actually, so I'm going to. It's not name dropping because I wasn't like talking with them, but I remember Jeff Hawkins years ago at a giving a keynote. The annual Society for neuroscience lecture and I'm sure he's made this point over and over again. You know, the traditional physics approach is you have your theorists and you have your experimentalists and they're sort of happy to play together. And that's not the case necessarily in neuroscience and that we need to get to a point where the experimentalists are happy gathering the data to feed to the theorists who then can analyze it. But that sounds awful to me too. Right. I mean, I so I will actually say I would much rather that experimentalists be capable and happy of analyzing their own with analyzing their own data. And the reason is that, you know, if I say I'm going to be a theorist or a computationalist, then. You know, data analysis is something that pays the bills. Perhaps it's something that can help get a routine number of papers out the door. You know, for like a machine learning person. I'm I am actually thinking of someone Scott Linderman over at Stanford. Like, you know, you'll notice that a lot of his papers are basically just machine learning based data analyses for neural data and that's great. That's the thing like that can build a career. Now, personally is that what I would want to think about as a theorist. How do we analyze data? No. No, like, you know, that is not the thing that I have, you know, a secret manuscript that I've been trying to finish for a year. You know, how do we explain emotion in a quantitative way or affect core affect valence and arousal in a quantitative way by going all the way back to, you know, the herb, bilaterian and then picking c elegans as a model organism. Yeah, good luck with that. Exactly. Yes. See exactly like good luck with that. But people like you mentioned Scott Linderman and so he develops a lot of tools that are being used in these naturalistic kinds of tasks, right? And that skill set is seems seems to be what is really valuable in the academic marketplace at least these days. Do you think I have that right? Yes. Yeah. Like when I was so I'm going to use myself as an example instead of him because, you know, I know myself better, right? And I don't think I could speak for the narrative arc of his career. But I know that when I started my PhD, the starter project that I got put on was here's a new way of analyzing FMRI data in a little bit more theory driven way. And it worked. What was it? Sorry, but what was the, oh, you just needed to employ that method? No, I mean, it wasn't just, oh, there was some method and we employed it. We were building something new because, you know, our collaborator on the psychology side had some data and he wanted to analyze it and the standard ways of analyzing it were inadequate to the theoretical question he wanted to ask. Yeah. So he wanted us to build something new. We built it. We published it. You know, that gets citations. There was a follow up. You know, I think there's now follow ups to the follow up. Like by other groups. Right. You know, that like that stuff is. This is going to sound horrible, but I don't mean it in a bad way. That stuff is good commodity science. But it's also necessary. I can make it sound even better. Yeah, it's like the, it's the Toyota of science, right? Like I drive a Toyota. I only bought a car this past year, but I drive a Toyota because you know what? It's practical. Yeah. You know, that is very practical science that you can reliably like never run out of new reasons to do more of it. And therefore never run out of publications. Well, that's right. That's right. But this goes back to the, to the idea of. So does that contribute to progress in theory, progress in understanding principles or is it just a very practical way to harness and say something about it? And say something about the data that's being generated. I think a lot of I think it has the potential to do both. But by default. It mostly does the second one. And that's not a criticism. That's to say I think the field has, you know, the ingredients for a really great synthesis. Sort of laying around in different people's labs. And what we need is essentially like a small conference or workshops worth of cross pollination where you can get the people with the appropriate skills all in the same room. Give them the incentives to work together. And I think it's actually the incentives that are the hard part. This idea of getting the proper, the people with the proper skill sets. In the same room for a couple days. It's awesome. The proper skill set is a shifting landscape itself. Right now we have a very specific one like people like you and Scott whom you mentioned and stuff like where these commodities, these tools are extremely valuable, widely used. But you know going back to Cuban weasel right there on transparency is they're like putting like just little shapes and trying to listen for the sound of neurons. Even like Jeff Shaw who I mentioned earlier would tell us stories about you know you're in lab you'd make like a little hole out of like a wooden cut out and you'd like put a light up in there. And it is the neuron active or not. It's a very different world back then very different skill set. And so I don't know how we track that and that's a meta problem. Yeah, I mean that's why I say like if you're going to have a division of people's jobs or departments into theorist and experimenter. Then I would want the experimenters to be able to analyze their own data. Because you know then they can do that even if it's a bit quantitative and even if that's something of a moving from tier sometimes. And then the theorists you know they can focus on asking questions like how does the brain actually work now that we've measured it. You know now that we're able to interpret the measurements. Let's get back to predictive coding though. I mean are you so are you. No you don't want to pin yourself into a very narrow corner but I mean where are you in terms of. So the idea of predictive coding predictive processing is that we are constantly predicting what are what is coming into our senses. And so we have to have sort of a model to use the term loosely of what we infer to be causes of things coming into our senses infer to be a cause in the world. So we're making these predictions from our world model. So the solution brain hypothesis is one way to say it. Free energy principle is another sort of framework implementation. So are you on board with like this being the function of the brain a major function of the brain. Where does this say major function of sensory cortex major function of sensory cortex. Yes. Why sensory. So there's going to lapse into neurophysiology vocabulary for a little bit. You know sensory cortex is usually well laminated like let there's laminar sensory cortex down in these low areas and then as you move. Both up the hierarchy towards cognitive areas what we think of as cognitive areas. Yeah nice. And also sideways over to motor you get different patterns of lamination. So the cortex is a laminar structure meaning it has a fairly repeated well very repeated motif of like six layers like a layer. Now the one that raw sensory stimulus comes into is layer four. And the thing is that when we talk about different lamination patterns, you know we're talking about. I believe they're called a granular and disc granular. And those have either much less layer four or they're entirely missing it. I think that that's right. I think a granular is has no layer four and disc granular maybe has a weaker either. Yeah like a weaker layer four. But now if you were asking yourself, you know, okay, so if I'm doing Bayesian computation. Then my observed random variable, which is the stimulus, it has to come in somewhere. And if I'm using this hypothesis about the laminar micro circuit doing predictive coding. Then where's that coming in? It's coming in in a layer four. So what is the circuit doing if it doesn't have layer four? That's where the generative network is right. Maybe like logically it can't be doing, you know, variable by variable Bayesian inference. It could just store priors. But then why does it have, you know, a layer two three? Because that's the one that, you know, computes errors and thereby updates the predictions. No. So actually really, you know, since we're following on Rajesh Rao's episode. I actually really like his hypothesis that oh, two three is the one that handles sensory data. You know, five six is actually handling chiefly motor data. And when you compute an updated sensory prediction, you might route it through there on its way somewhere else. But then fundamentally he would be saying, okay, now, you know, oh, and he also notes that there's philamic projections into a cortical column that don't have to go through layer four. Right. So the desire is to bypass layer four bypass layer four being a necessary part of predictive coding. Is that when we do well or ways of reformulating the predictive coding hypothesis so that you can still have sensory data coming in even when there isn't a layer four. And then you just have, you know, physiological and evolutionary questions about why are these areas, you know, a granular, disc granular, laminar, what are the differences between them and the similarities. But you've haven't totally abandoned your framework. Whereas if you're committed to layer four being where sensory observations come in, then logically the Bayesian computation can only be done in linear sensory cortex. So when I say I think I'm committed to this being, you know, an explanation of laminar sensory cortex, I'm being kind of minimalist. Sure. Okay. Okay. But so you're on board with Roger's story about like the incoming two, three layer two, three outgoing layer five. And how that's that's one way that it's biologically plausibly could be implemented. But you're divide and conquer predictive coding also strives to be biologically plausible. Maybe we can start with like what is divided and what is conquered in divide and conquer predictive coding and then maybe talk about a possibility. Okay. So if you go look at some of the free energy papers, I think there's even one called the graphical book, like the graphical brain. You know, they tell a story about how a probabilistic graphical model has these different nodes representing different, you know, unobserved random variables and these get mapped onto cortical areas. And then the communication between areas is, you know, a series of messages and a belief propagation algorithm that eventually gets down to primary sensory areas where the random variable is observed. Now, this kind of algorithm makes a very specific assumption that they call the mean field assumption about essentially saying we're going to approximate the posterior distribution with a product of independent representations. So we'll have one representation for the visual one for the audio, you know, one that represents the integration of visual and audio. But they're actually all going to sort of be statistically independent in, you know, the approximate posterior scarecoding as implemented in the brain. And by the way, on the, you know, machine learning side, we know that this is quite a bad representation of a posterior distribution. Why is that? You know, essentially it can't represent correlated posterior. Because because of the independence assumption. Yeah, like it's making a very strong independence assumption that was necessary to simplify the math in like 2003. Literally, the first time variational inference was published was in a PhD thesis from 2003 or so. Like, you know, all my respect to people who are developing new things and make simplifying assumptions, right? But of course, the point of science is that we always want to try and relax our simplifying assumptions and ask, can we come up with a way to essentially, can we assume that the real world is really complex and complexifier models over time? So as to a comedy, the real world. Well, but but then you're also dealing with awkums razor, you're dealing with trying to figure out, well, what can we abstract away? What are the important things that we can abstract? And so when you make assumptions like that mean field assumption, you are, you are making trade offs. It's just whether they're the right trade offs given what you're trying to answer, right? Yeah. And you know, sort of what I have learned through my PhD on the machine learning side was that if you have a complex structured graphical model has might be used in some cognitive science task, then mean field variational inference doesn't work very well. And I thought, well, you know, if I take a theory or sorry, if I take a hypothesized model from neuroscience and I applied an AI and it just doesn't work very well, is that what the brain does? No, I don't think the brain, you know, fails at things that are doable with current AI methods. Or rather, I don't think the brain fails at doing things that we've observed it to be able to do an actual behavior. You know, I think that's a case where the algorithmic model is just inadequate. So I said, okay, let's make a better one. Instead of mean field, you know, independent independence assumptions. Let's instead try to break down the random variables from one another so that you maintain their correlations when you update them. Is this the dividing part? Yeah, yes. So just say it again. So what are you, you're dividing, go ahead. You take this, you know, a probabilistic graphical model. So it's a mathematical analogy to the brains internal model of the world. And you say this consists of a bunch of different variables that are connected to each other in various ways. You know, kind of like cortical columns, we can imagine. So this is actually how I imagined it was, you know, one cortical column, one random variable. And then when they communicate with each other, you know, those are conditional dependencies and all that. And then I said, okay, let's try to divide this so that we can update each random variable in a way that takes into account the correlations with the other random variables. That's the, that's the counter part. That's the divide part. Well, yeah, or yeah, then the update is like the first step of conquer. Then the real conquer is that we have all of these importance weights from the world of like Monte Carlo methods for Bayesian statistics. That then let us eventually write out, you know, here's how good a fit to the joint model we have to the whole probabilistic graphical model. So we're saying we want to do local updates that maintain some kind of global coherence. And it gets called divide and conquer because. Well, frankly, NURPS is ultimately a computer conference. Now, yeah, yeah. And like all computing people have taken an algorithms class where they talk about divide and conquer methods. I see. I didn't realize that. So that this is a well known phrase in the, in the algorithmic world. Yeah. Like if you talk to algorithmicists and say divide and conquer, they'll say, oh, okay. So you're taking some kind of huge data structure and recursively performing the same computation on each component. Before going on to the connected bits. Okay. That makes a lot of sense. You know, and it just sort of happened that like. You needed a lot of Monte Carlo tricks to make this work. But when you do it's very sort of intuitive. Why you would want to do it that way. If you were then going to map your probability probability model onto a physical circuit structure. Where the different random variables are spatially separated. And have to signal to each other. So what is what is in the divide and conquer model? What is required for it to be biologically plausible? So the claim of biological plausibility we made. Is to say the computations are purely local. Right. Instead of the local updates, instead of globally. Right. So, you know, people have talked about, you know, code back propagation be implemented in the brain. Tomaso, our senior author has this paper on using Gaussian predictive coding. To actually implement back propagation as a substrate for back propagation. And in this paper, we're sort of saying. Well, let's assume you can't do back propagation. You don't have any kind of global, you know, computation graph or computation automatic differentiation tape in the brain. But let's assume that, you know, one cortical column can signal to another. And that if you're representing one random variable locally. Then you can do really three things with it. Sample from its distribution. Measure the log density of its distribution. So the log probability density. Where density just means that you're talking about continuous random variables and not discrete ones. And three, take the gradient of the log probability density. And if you can do those three things locally, then you have the primitives necessary for our algorithm. And you can thereby obtain global coherence out of local computations. And jumping back. Since we were talking about that that experiment versus theory metascience topic earlier. I mean, does it does this make clear predictions about what kinds of signals that you would expect to see. Now here's where it gets biologically implausible. These were still rate coded neurons. Right. So they can still cross between positive and negative. Right. So so brains use spikes among other signals like LFPs. But the essentially all of modern machine learning or AI models use rate codes. And there, you know, there are a lot of people working on spiking neural networks also. I assume that if you're going to implement it in like a spiking network, then, you know, you have to go. I mean, it's plausible with the sampling approach, right? Because that's what spikes are all about. Spikes are all about sampling. Well, you can so going back to the old debate on like how probability is implemented in brains. There's a sampling approach versus the approach where the spike spike counts map on to some probability distribution types. Oh, yeah. But with the twist. So with the twist. Yeah. So I know there's a lot of sampling approaches where you essentially say a neuron has a preferred stimulus and implement a likelihood function. Right. And the priors are actually represented in the developmental program of the genome. Not in the neurons themselves. And then those eventually make the prediction that they make the opposite prediction to predictive coding. They say when the posterior probability of what the neuron prefers is higher, the neuron will fire more. And predictive coding actually and the free energy principle and all of those approaches are much more information theoretic. They say that when the stimulus is thoroughly expected, you should see much less neuromal fire. And so we're in that family of theories. Though we do use random sampling. My dispute with spikes being about sampling is that of course if you patch clamp a neuron like in vitro, then what is it like 96% of the variance in its spiking that you know is explicable deterministically. There's still casticity in the real brain, but we don't know that the single neuron isn't transically stochastic that way. Right. That way we do know it's stochastic. But yeah, but it's okay. But then going back to you started by saying that the major this is where it gets into non biologically plausible mechanisms is that it doesn't use spiking. Yeah. And actually, I think Blake Richards group has recently ridden to our rescue with their paper on what is it brain like learning with expedentuated gradients. Brain like because it uses spiking. So in their case, brain like because of a day's wall. So they'll have inhibitory neurons, which are negative and excitatory neurons, which are positive. And the signs will never flip. And they show they use rate, but yeah, they're still using rates there. And you know, how realistic do I think that is? I don't really know like, I mean, there's areas that could use rate codes, but there's also too many experimental findings showing that precise timing matters. So what it could be, you know, and this is not an original thought to me. This is coming from a computational brain behavior paper. I can send you the name. It's from 2020. You know, it could be a prefix free code. Oh, what? Sorry. Say that again. A prefix free code prefix free. What does that mean? So that means that once you send a certain pattern of spikes. And by certain pattern, I mean, you know, the precise timing determines which code word it is. But once you've sent a certain pattern of them. Then that code word is over. So prefix free means that no code word is a prefix of another code word. So if I say a, b, a, b, then that's either a full code word that now tells you something. Or there's no full code word that starts a, b, a, b, accept the one I'm already sending. But what, so what would that mean? Is that just because of rate codes? Go ahead. Sorry. So a rate code would say you listen over a certain period of time. Right. Whereas the timing or you get m spikes, you divide by time t. Yeah. Whereas a timing oriented code is like you get a time, a spike at time t. Now you think, well, I, I, what comes next? Spiked delta t delta t prime delta t prime prime. Right? And you look for, you know, very specific timing, like with musical notes. And your lookup table, you figure out, oh, I was just received this particular message. Yeah, I just received taught tt tt. You know, it's gosh, has someone actually tried using third grade music class on timing codes. But yeah, a prefix free code would then be a timing based code where you say once I've received a full code word, that's it. I know that I've received a full code word. I can interpret the whole message. Clean your cash and move on. Yeah. Clean my cash and move on. Exactly. But really, I mean, gosh, on the other hand, that doesn't. See, this is the thing that bugs me is like there's also evidence that, you know, dendrites, right? I sort of accumulating this precise spike timings into something more like a continuous signal that gets fed up to the cell. So how can it be that there's precise spike timing and there's dendrites that convert from spike timing to spike rate, or less? Well, I don't know that those are necessarily problems, right? I mean, so when you were going to say, you know, is it as a spike timing code or a rate code? And because we know that some things require precise spike timing like the intra-orial differences, right? The underlying how owls here locate sound, for example, timing is very important. But maybe timing is not as important in, I don't know, frontal cortex or prefrontal cortex or something. And it could be both. Yeah. Depending on what you're needing to accomplish, a... An organ like the brain is fairly complicated, it turns out. And it might be implementing lots of that degeneracy you were talking about. That could be the case in terms of how it computes. It's not maybe one or the other, but just depends on what's needed. Yeah, like that's very, very possible that essentially... So actually, not only is that possible, that would go very well with some of our recent preprints that basically say, predictive coding is a much more cognitive computation that can take place in frontal areas. You know, back to our glow paradigm, those global oddballs seem to get detected in frontal areas, but non-inlolar sensory cortex. Oh, interesting. Okay. Maybe the Lemanar cortical column, you know, is something like a big stack of universal computational primitives that don't tell us much from just reading off the anatomy about what it is doing. Oh, God. Yeah. You know, if we broadcast this, the modular minds people are going to crawl out from under the rocks. We spend so much time banishing them. That's all right. That's all right. There's room for everybody. One of the things I wanted to ask you about is, so you're mindful of, you know, what is and what isn't biologically plausible in this. You think it's important if you're going to understand, this sounds silly to say, if you're going to understand the brain that you need to implement through a model, you need to implement something that is biologically plausible. But you're willing to forego the spikes. But so inevitably, any project is going to have hurdles. What what what hung you guys up the most in getting this thing to work and or getting it feared out properly. So two big things. You know, the first time was when I tried to write out all those waiting rules, essentially saying, like how do you accumulate, you know, the weights from doing a dozen successive updates to a random variable over a dozen passes. And I got something that looked really complicated and eventually just exceeded the numerical. The numerical precision of floating point numbers in a computer. Okay. And what I eventually did was just like have a meeting with how and talk out some options. And he pointed out that one of them was essentially just cheating for getting the old importance weights and just saying, you know, I start with some particles. That is, I start with some samples and do a computation step on them. Now I have new samples. I'm going to do the same thing next time. I don't save any weights. And we ended up going with that because it turns out, you know, once we like both prove to ourselves that this was legal to do within all the rules of the game. This just turned out to be the simpler thing that was able to work. And so you're okay. I mean, so storing the weights over time, maybe is not even as biologically plausible as. Recording, yeah. We said so there were two things that you said were. So the other one is that between the first pre print draft and the second one that represents our camera ready. We added like this preconditioner that helps the optimization go in the right directions and respect the geometry of like the latent space. And you know, this very mathematical like technical itchy thing. And the thing is without that stuff doesn't work. And you just don't perform very well on your test tasks. Now, we did manage to rig this up in such a way that it could be biologically plausible. You know, it's effectively like calculating a certain function of the prediction errors. So if the prediction errors are locally available, then this thing is locally available. And you could even, you know, nod to the free energy principle and say, ah, there's that precision of the prediction errors that these free energy guys are always on about. And really it was just motivated by getting the damn thing to work. In the end, you have to have a product at working product. Yeah. And you know, this is where I forget which famous person said that no, two famous people have said this. Richard Feynman and Daniel Dennett have both, you know, said if you want to understand it, you've got to be able to build it. Did Dennett say that also? I mean, Feynman, he said a version of that. Did he? Okay. Yeah. Feynman says, no, understand what I cannot build. No, and then Daniel Dennett says completely different. He actually said at one point, AI keeps philosophy honest. That's what I was remembering. Well, that's interesting, which is a whole other can of worms. So my mistake, but you know, what I would say is. If you want to say that predictive coding is a thing that happens in the brain based on your experimental observations, then it should hypothetically be possible to build an algorithm that does predictive coding and actually works for, you know, some of the toy tasks that we use in AI, which are still vastly more simplified than the tasks we use in neuroscience or rather the task of the brain. You know, an AI image generating network does not have saccades. Well, unless it's one of Rouse, in which case it does have saccades now, but you know, that's very new for AI and completely trivial for neuroscience. And so I think you have to be able to build up AI to the point that it's able to do things that are trivial for neuroscience before you can really say, oh, a computational theory is viable now. It has to do the things that are most trivial for the brain. Well, all right. So then I have to kind of broader questions for you before we end our conversation today. And one, just going off of what you just said and have sort of been building up to this. Do you need to understand the brain or brain processes or implement things in a similar manner to how the brain does things. To build the best artificial intelligence. Do we do we need to mimic the brain and at what level if so. So I think that depends on how you define. I'm sorry to be philosophical about this, but it really does depend on how you. It depends on how you define artificial intelligence, right? And I don't like to commit to a definition of that at all because what I personally want to do is understand the brain. That is the motivation for me. I want to understand the thing that actually exists. Try to draw. You know, so to speak laws and principles from it. And then maybe I could engineer something with those in the same way that you can, you know, engineer a steam engine with Newton's laws and thermodynamics, right? But you do have to do, you know, in my view, the interesting part is to do the fundamental science before the engineering. Now, if you are engineering first, then, you know, an intelligent task is whatever the heck you have a benchmark for. And, you know, sort of there's this alternation between making a harder benchmark and beating the current benchmark. And in that case, do you really need the brain? Well, no, you need to understand your benchmark task. Like, there's a lot of tasks where if you have a very deep understanding of the task itself, you don't necessarily have to understand how the brain would solve that task. But there's all the talk of AGI, right? In the AI world, we're going to get the AGI by next Tuesday. It's going to be the Tuesday after that. No, and then it's like five years. No, it's just 20 years, you know, I mean, I personally go away from definitions again. I don't know what AGI is. But I think that the humans are the wrong benchmark. It's like just what is what's the right analogy? All we're doing is like staring at ourselves in the mirror. And yeah, that's real intelligence. It's only because it's us. We think we're great, I guess. Oh, there I totally agree because, you know, what is it we got, you know, optimal chess playing at superhuman level. Maybe was that a decade before we got, you know, neural networks that could pass image net classification at a human level. Yeah, half a decade maybe. Like 2000s. You know, and at the time, you know, chess was sort of the king. Yes, where we thought if we understand how to play chess, we computationally, we understand cognition computationally. Or we've like built, you know, intelligence. And then. Well, I don't even have to say end then there's a cliche for it. More of it's just paradox. You know, like I am very much a more of a just paradox person where I say, understand embodiment first sensory motor stuff first feeling first. And then maybe later in retrospect, you'll turn around and say, here's all these normative principles we derived from our empirical study. We understand now how those tell us how to build what intelligence is and how to build it. But the term AGI almost feels like it. I admire the people I admire the sheer ambition of the people who are trying to do that and going to conferences like the AGI conference. And the other angle on it is unfortunately that I do think in the era of large language models, there's been a tendency to fool ourselves and define AGI down. So that instead of being a name for something we don't understand and have to come to understand, you know, only through working at it over time. It's become, you know, a name for something that we say has happened. Oh, yeah, yeah. Like the latest, you know, model from wherever is AGI is it? It's go. Yeah. Right. And it's like, okay, but that's because it talks. Like that's because it talks and we know the Eliza effect. We know that if you talk and talk and talk, people will project personhood onto the words. And to be fair to people prior to the invention of LLMs, 100% of all linguistics stimulus we ever received came from other people. Well, except maybe for like bad Markov chains and Eliza and that sort of thing. Right. Yeah, yeah. Alisa is a really well-known game. The overwhelming supermajority. So, you know, for an optimal probabilistic reasoner, if you heard language, then the rational conclusion was that there's a person. Well, we also know that some of us aren't that bright. For example, I've said I think only ever more of that and you say more of that which is it. So more than I do. Oh, really. Oh, I'm sure I'm wrong. I'm sure I'm wrong. Anyway, that's the paradox that it turns out that it's easy to build. I've easy to build. That I blanked from my memory where someone where like Mitch corrected me on this. Oh, I don't know. But anyway, that paradox is that it's the things that we think are hard to do like chess. Yeah. Turn out to be easy and the things that we think we're easy out that are easy to do like. Walking on two legs. Yeah, be like a waiter balancing a train moving, walking through a restaurant. Oh, man, don't list that as easy. Talk to a waiter before you call that. That's hard. Well, what I mean are the sensorimotor everyday things, the continuous sorts of behavior. Yeah, but something is hard even for embodied human beings. So that's one of them. Yeah, talk to me. Go get a friend who works in food service. I've been a server. I've been a waiter. Oh, okay. That was the poor example. See again, I say more of that. I give bad examples. What do you do? Well, maybe not of us. I'm sorry. I'm not. I'm not supposed to be shaming you on your own show. Yeah, what are you shaming me on my own show? Sorry. No, okay. You've been a waiter. Yeah, it's easy for you because you practiced. I also have an ungodly balancing talent. No, that's not true. All right. But I do have another question because you are interested in how did you phrase that earlier? Not consciousness, but. Feeling. I don't know if you can say anything feels the way it does. Right. Another way of. Yeah, to say it is like just subjective experience in general or. Affect, I guess. Affect the affective component of it. Like, why do some things feel. The classical dimensions of core affect our balance and arousal. So why do things feel. Pleasant versus unpleasant. Why do things feel exciting versus relaxing? Okay. So what we could say or arousing versus sedate it. So, okay. So my question then is and I was thinking about a Neil Seth who ties predictive coding into consciousness and that that's going to solve consciousness essentially. What do you think about sort of maybe that, but also it's real predictive coatings relation possible relation to affect the way you just described it. So I have to say I think the second one, the relation to affect through interception, homeostasis, alastasis, this stuff. Is a lot easier to establish than anything about consciousness. And that's why I've sort of said like well, I'm not going to touch consciousness with a 10 foot pole. Right. It's much too hard. Like I'm not everyone's a little bit of a philosopher, but I'm not very much of a philosopher. So I'm just not going there as to the connection between predictive coding and consciousness. I mean, here's one of the reasons I think consciousness is so hard to think about is that. Oh, what is there this like classic thought experiment about consciousness. Like couldn't you imagine a philosophical zombie who has the same input output mapping and the same observable behavior possibly even the same electrophysiological readouts as a real person, but isn't conscious. But what about the affect aspect then they wouldn't have affect either, right. Right. I mean, if they don't have consciousness, it possibly makes sense to right. So that's what I would ask is, you know, does a philosophical zombie have a predictive internal model. Do they have interception? Like and I asked myself, you know, can I imagine someone who has the same internal states and control systems at a physical level. But doesn't experience them at all. The answer is no. Yeah, the answer is just no, because I'm like, but there's a latent variable there. There's like, you know, representations and computations going on. There's internal states maintained over time and internal dynamics. I can't imagine how there could be no one home. And that's past that's, you know, like I said, I don't study consciousness because I recognize that this is very likely a limitation of my imagination rather than. And some kind of answer. It's just the way my intuitions work. And so I prefer to be. At least on the engineering end where like I can bang an intuition against an experiment that doesn't work and bruise it until it's softer and can be re-bolded into another intuition. Now that you're doing experimental work, how do you think about the role of intuition? Sorry, I know this is another question. And I've got I actually have to go in a minute. But do you feel that your intuition has served you better from the computational world for your world or the experimental world? Because it all comes down to that to make any progress. You have to make a guess. And that's from intuition. Actually, I would say I don't know a good way to put those two together right now. I'm sorry, I just don't. I don't know what the intuitions from both ends because maybe if I was doing experiments with naturalistic behaviors, I would develop more of an intuition for how to dry it, let the experimental end drive. But with the highly constrained experiments, you know, I just don't I get an intuition for the task and the setup. Like the way that a particular data set or animal might behave. But not one for how does, you know, not one from for like how do I pass from, you know, these spike trains to psychology. So like the mentalizing I can do about the animal. Right. I have no bridging intuition there whatsoever. Well, see now that I do what I'm quote naturalistic experiments, meaning we just there's just a mouse running around in a box and we measure measure measure measure and now we're trying to relate. Neural activity to that ongoing behavior, which is continuous. They groom slightly differently. They move their paws slightly differently. Are we going to call that the same groom as the other one? How do we define that? My intuitions about experimental neuroscience, which were forged in that controlled constrained environment. I think are not serving me well. So I'm trying to build new intuitions. Yeah, like, you know, if there's one thing I've learned in my life, it's really the limits of raw intuition and how you just kind of have to bang up against experience long enough to start developing. And you know what you let's end on a pun, you know, call it posterior intuition rather than prior intuition, right? Exactly. You have to you have to take action and get get update your posterior. In the way that you phrased it. Yeah. You like, did we miss anything? What we went half-hazard we we quite technical there. We remained out in the forest. Some is there anything crucial that we missed that you want to end on? Oh, actually, yes. So there's this thing I always keep in my Twitter bio abolish the value function. If I'm doing a podcast, I should tell people what that means. Yeah, what does that mean? That's a great way to find out. Okay, so that means that at one point in grad school, Jordan, Tariot, who will probably listen to this high Jordan, you know, recommended this book to me entitled more heat than light by Philip Morovsky. Okay. And Philip Morovsky is like a very philosophical leading part economist part historian. And he wrote this whole book about the analogy between energy and the conservation of energy and economic behavior. So all of this notion of like there's an economic agent who maximizes utility or minimizes cost. That's the value function. Yeah, all that stuff is the value function. And what he pointed out is that essentially if you think like a rigorous physicist, the analogy is bunk. Like it's not, you know, economic value is not a conserved substance. You know, people produce things that are valuable and then consume them. The amount of value is not a fixed constant number that stays the same all through all of this. What thinking like a rigorous physicist, which would have be called an emergent property of production, then what? I mean, like, I'm not sure what Morovsky would say there. But his point was that in order to get all the math that was imported into economics and then by the way into cognitive psychology into reinforcement learning into optimal control into all these things that we use in psychology and neuroscience imported from economics. And to get that from physics to economics in the first place, you have to assume a conserved substance. So a conserved quantity, which represents a physical substance on which you can then have a gradient flow, a certain kind of dynamical system. So absent that, where do we go? What is the, what is the result of abolishing the value function? Right. And so if that's just the wrong metaphor, then I think we need to go into a much more control theoretic frame of mind, where some signals represent references. And they can be directly compared to input signals from the bottom up by a comparator. Yeah. And then when I shift from folks, yeah, I've come around to the theory things as well. So then when I shifted my point of view from, you know, all of these decision making tasks there about grabbing more value imaginary gold coins like in Super Mario. Neuroeconomics. Yes. Yeah. Versus there about measuring the distance between a desired outcome and a actual outcome, much more perceptual control theory. The reference signal is somehow internally generated by which you compare, which is amenable to predict the coding. Yeah. And then I thought, okay, well, now we've gone from substance to distance. These are completely different metaphors. Distance is the superior one, because as soon as you set up a mathematical model, you can measure the distance in the parameter space. Right. And by the way, that's actually the difference between reinforcement learning and active inference. You know, in all of that free energy literature is that the active inference people are saying, let's specify a desired outcomes as target capability distributions, then measure the relative entropy distance from one to the other, and then just try to get closer to the desired outcome distribution. That's abolishing the value function substance to distance. All right, Eli. I appreciate your time. Look forward to more work coming out and good luck with the experience you'll be in touch with good luck with the work more learning more experimental research. The latest analysis seems to draw a very different conclusion than the ones we preprinted. So, you're going to have to reconcile those. Yeah. All right. So, yeah, I knew you have an office mate there also needed to get back in the office. So, thanks. Tell them thank you for letting me take up some of your time. So, thanks for coming on. Yep. Thank you. Brain inspired is powered by the transmitter, an online publication that aims to deliver useful information insights and tools to build bridges across neuroscience and advanced research. Visit the transmitter dot org to explore the latest neuroscience news and perspectives written by journalists and scientists. If you value brain inspired, support it through Patreon to access full-length episodes, join our Discord community, and even influence why invite to the podcast. Go to braininspired.co to learn more. The music you're hearing is little wing performed by Kyle Dunovan. Thank you for your support. See you next time.
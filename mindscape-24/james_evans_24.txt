 You just realized that your business needed to hire someone yesterday. How can you find amazing candidates fast? Easy. Just use Indeed. Because when it comes to hiring, Indeed is all you need. Indeed's Sponsored Jobs will help you stand out and hire fast. With Sponsored Jobs, your post jumps to the top of the page for your relevant candidates so you can reach the people you want faster. And it makes a huge difference. According to Indeed data, Sponsored Jobs posted directly on Indeed have 45% more applications than non-sponsored jobs. There's no need to wait any longer. Speed up your hiring right now with Indeed. And listeners of Mindscape will get a $75 sponsored job credit to get your jobs more visibility at Indeed.com slash Mindscape. Just go to Indeed.com slash Mindscape right now and support our show by saying you heard about Indeed on this podcast. Indeed.com slash Mindscape. Terms and conditions apply. Hiring, Indeed, is all you need. Here's a tip for growing your business. Get the VentureX Business Card from Capital One and start earning unlimited double miles on every purchase. That's right. With unlimited double miles, the more your business spends, the more miles you earn. Plus, the VentureX Business Card has no preset spending limit, so your purchasing power can adapt to meet your business needs. The VentureX Business Card also includes access to over 1,000 airport lounges. Just imagine where the VentureX Business Card from Capital One can take your business. Capital One. What's in your wallet? Terms and conditions apply. Find out more at CapitalOne.com slash VentureX Business. Did you know Fast Growing Trees is the biggest online nursery in the U.S. With thousands of different plants and over 2 million happy customers. They have all the plants your yard needs, like fruit trees, privacy trees, flowering trees, shrubs, and so much more. Whatever plants you're interested in, Fast Growing Trees has you covered. Find the perfect fit for your climate and space. Fast Growing Trees makes it easy to get your dream yard. Order online and get your plants delivered directly to your door in just a few days, without ever leaving home. We've just received a white dogwood tree. We've not had a chance to plant it yet, but are very excited to do so. This spring, they have the best deals for your yard, up to half off on select plants and other deals. And listeners to our show get 15% off their first purchase while using the code MINDSCAPE at checkout. That's an additional 15% off at FastGrowingTrees.com using the code MINDSCAPE at checkout. FastGrowingTrees.com, code MINDSCAPE. Now's the perfect time to plant. Use MINDSCAPE to save today. Offers valid for a limited time. Terms and conditions may apply. Hello, everyone. Welcome to the MINDSCAPE podcast. I'm your host, Sean Carroll. Something I've figured out after doing science for quite a long time is that science is hard. It is hard to do it. It is hard to do it well. That's probably not really news. That's not probably something that is very controversial out there. But I wonder if people appreciate the ways in which it is hard. I mean, of course, the actual doing of the science can be hard, right? Doing a great experiment, doing a difficult calculation, if you're a theorist or something like that. But there are all these pre-existing difficulties that come long before when you get to that. Mostly like, what problem do you choose to work on? What is a promising area of research, right? Resources are finite. The time that you have as a scientist is finite. The money that you have, the students, whatever. You're trying to not only make a big discovery, but also ensure that there are future discoveries to be made, which is the nice way of saying you got to get a job. You have to impress the people who might be hiring you or promoting you or whatever enough that you will be supported in your aspiration to keep doing science. You want to be new and innovative, but you don't want to be so crazy and out there that people don't pay attention to you. And even once you've chosen what to work on, how exactly do you work on it? Do you sit by yourself trying to think very, very hard? You know, there's people like Paul Dirac who very famously thought that all of the good papers ever written in science, all the good science ever done, was done by a single individual, all by themselves. But the world is very different than in Dirac's time right now. There are big collaborations experimentally, of course, but even within theorists who are very nimble and can change what they're doing from month to month, it's so easy to communicate with people over the internet or in person. You can start up collaborations with people who don't know the same things you know and might be helpful to you. So who is your set of collaborators and co-authors and helpers? These are all great questions, and they're next to, but separate from, questions about is science really making the progress that it could be making? Is it moving fast enough? Is it daring enough and bold enough? Or is it stodgy and set in its ways? Are we leaving food on the table, as it were? So, of course, lots of people have opinions about these things. These are issues that many people have talked about for many years. The difference now is we can really collect data. In a more or less objective, at least wide-ranging way. Rather than just sort of looking at this or that key feature in the history of science or key episode, we can actually look at many, many scientific investigations. Many people, many researchers writing papers. Who are they collaborating with? Are they making advances because they're collaborating with the same people or with a different set of people every time, with people in their field, outside the field? Do old people make the advances because they're very wise? Or do young people make them because they're not restricted by the conventional wisdom? These are all great questions, but don't try to answer them just by thinking and by using your intuition. Look at the data. That's what today's guest likes to do. James Evans is technically a sociologist. At least, you know, he's in the sociology department at the University of Chicago. But he does wide-ranging research on computer science topics and all sorts of sociological topics. But in particular, the science of science. Understanding how scientific progress gets made. And thinking of it as an example of collective intelligence. Right? Science is not just one person. And there's lots of scientists doing things. So how do these scientists come together in groups? Where do the groups come from? How big are the groups? What kind of groups make the best decisions moving forward and so forth? And I think that my prediction is that if you're someone who's thought about these things for a little while, what James has to say will, in some ways, reinforce things you already thought. And in other ways, surprise you a little bit. These are tricky questions. That's why data is important. That's why doing the science of science is important. In addition to doing the history of science, the philosophy of science, the sociology of science, the psychology of science, etc. All of these are important. These days, we can do the science of science at an unprecedented level. And we're learning new things. Both about how science works, how it can work. How changes like AI are going to help it and hurt it and things like that. So the world is changing. We've got to keep up with it. Let's go. James Evans, welcome to the Mindscape podcast. Thank you. I'm delighted to be here, Sean. You know, I have to start by saying a couple of months ago we had Blaze Aguera-Iarkas on the podcast talking about the spontaneous emergence of computational life. And it took me a while to ping on the fact that you are a co-author on that paper, even though there's no sociology involved in that paper. So how did that happen? Well, I'm kind of a computational scientist. That's the way I think of myself. Or even really a meta-scientist. I mean, I study how science works as systems. And a lot of that comes through an evolutionary lens. And so I've been involved in following A-Life for a while. I work with Blaze on his team, his paradigms of intelligence team at Google. And there are kind of three levels there where we're looking at kind of the ways in which, you know, biological kind of forms, processes, and organisms shape the way in which we think about AI. And one part is evolution. One part is kind of neuroscience and cognition. And then the third part is collective intelligence. And so I spend even more time, you know, kind of helping to cultivate and catalyze work in this space of using principles of collective intelligence and diversity underlying collective cognitive populations to think about building AI in that space. Well, I think it's a good place to start because it's like a personal spin on the more big picture substantive questions we'll be getting to later. But, you know, someone like you who researches topics like this, sometimes it can be hard to find a department to hire you, right? Like, so what do you do really, right? You must have gotten that question before. Right, right. It's true that right now I'm in—so I think part of it has been that I've been involved in creating a fair, you know, number of departments. So I began my home in sociology, and I think some of my colleagues think some of my work looks like sociology, especially the work where I'm kind of looking about how it is that scientists and people in the world organize around information. But a lot of it doesn't look like that. I'm studying the complex systems underlying, you know, deep neural networks. I'm understanding, you know. So I think that—so I've spent a lot of time—I've built the computational social science program that's been increasing where we built a data science institute and now kind of department at the University of Chicago. We're in the process of trying to create a new college or division about artificial intelligence here. So I think that, you know, not having, like, a fixed home also creates an instability that allows me to create new things, which is fun. But the—but yeah, I mean, I completely agree, and I'm exactly in a very similar position myself. But the journey to get there is tricky. And I mean, maybe one thing we'll talk about along the way is how do we sort of get—make on ramps for young people who want to do that? Like, it seems when people ask me for advice, my advice is always, like, be pretty conventional early on in your career because there's a lot of gatekeeping, a lot of bottlenecks you have to jump through. It's hard to be too quirky as too young an unproven person. Yeah, I think—I mean, I would say the one caveat is I would say you have to be able to present conventional early on. Yeah. Which is to say, I think if you just do conventional early on, you will likely do conventional forever. But if you have a portfolio that you're cultivating, enough of which you can purpose to present conventional leadership, then that's, you know—and I, yeah, and I had to do that in the context of sociology. Right. Even though I would consider myself kind of a more multidisciplinary, post-disciplinary scientist. Yeah, no, that's a very good amendment. So when we are thinking about science and how it works, this is one of the things that you do. Just to get a feeling, obviously this is something that people have talked about for many, many years. A lot of it has been in the context of, like, history or philosophy of science. Are we doing it today in a more data-driven way? Yeah. Yeah. I mean, we have—and I think what's interesting both in science in general and I would say, you know, in artificial intelligence models is their performance, their predictive possibility, the ability to create generative digital doubles is all driven by massive data. And so I think that's a phase transition. It's not just like, oh, you know, we didn't have data and we had some science and we improved a little bit by increasing some data. I mean, it's just the difference between systems that work, that predict, and those that do not, you know. And so, yeah, I think we're doing it with large-scale data. And because scientists are incentivized to leave droppings, which is to say our capital is our credit, you know, in the system, then we leave more droppings than people in business, you know, in an other context where people are covering their tracks. You know, it's really hard to tell stories about how action in those places, microaction, personal action decisions, but in science, because we have to broadcast every step to the world because we're certified and that's how we get promoted. And it provides a rich landscape for thinking about collective knowledge and individual contributions. That is a great point. You know, my wife is a journalist, a science writer, and one thing you notice there is that scientists, when they, you know, give you an interview or whatever, they really care about getting credit in the news article that's being written. And not only for themselves, but like, no, you must list all eight authors of our paper in your 1,000-word article. And the journalists are like, that's just not how we roll. But it's very, very important for academics more broadly, right? Right. I mean, this is our, this is, this is the coin of the realm, you know, is you contributed to this paper and we can, we can basically say all the papers that cited your paper kind of get back propagated to your, you know, enlarging avatar of, you know, of influence. So, so as a scientist, I mean, let's try to help the audience, most of whom are not professional scientists, right? One of the things you got to do, forget about getting jobs or whatever, you have to decide what research to do, right? How to pick a research problem. And, and, and this is hard, both because, you know, ideas are scarce and things like that. But also, there's a style question. Am I going to work on something quirky and innovative? Or am I going to fit in to kind of a bandwagon? Is that something that we can study quantitatively, sociologically? Yeah. Yeah. Yeah. I've been studying this question or variants of this question for 20 years for two reasons. One, we, we want to just understand how it is that, that science happens, how people make decisions. And, you know, this is a valued quantity of life. On the other hand, you know, economic growth happens through scientific advance. You know, I mean, this is, this is, I mean, credit, you know, like financial credit is given because we assume that some magic will happen when we give this money to a corporation, that they will reorganize things in a new way and new values will be generated. Like credit in the ye olde days was extractive. You know, I'm going to give you money and I'm going to take more money from you. And so it's just, so I think as a result, then it becomes really important for nations, which are typically the units that bankroll science, like countries bankroll science. There is international science. People experience it, but where do they get their funds? They get their funds from their country. Most countries, all universities are, are effectively public in the U S even though we have quote unquote private public universities, almost all science research funding is, is well, increasingly it's from private organizations, private and public, but, but certainly a vast, you know, contribution and the most focal element is from, from public contribution. So I think part of this is like, how do we think about if like the country as a whole is relying on, um, its competitive power, its ability to kind of generate new markets and services and, and new, you know, values for life, then they need a collectively to organize people to start exploring different kinds of things, or they're just going to, they're not going to get any return in their investment. Um, and so, uh, yeah, so, so I, so I, so I think it's, it's a big question, even though, you know, it's a micro question, but it has, you know, really collective, uh, implications for, uh, for countries. And so it's kind of an existential question for, for, uh, for countries. So, um, so yeah, so one of the ways in which we've studied it is, is looking precisely at, you know, what it is that people are engaged with relative to the distribution of prior work. And this is where big data matters, right? Because, because think about it, you know, like, uh, if you were to kind of take, if you want the, the, the modal story, the average story, you don't need much data. You know, you can take a very small sample of a few scientists. Um, if you want to tell a story about change between one period and another, you need at least twice as much data, right? Uh, you know, to difference between one period and the next to tell a story about innovation, uh, requires exponentially more data because you have to understand everything that was expected to understand the significance of something that was not expected. You need to understand in theory, everything that was expected by any scientist, um, in the space to understand really what's new relative to that space. So this is, so I would say innovation is something that you can only study with large scale data. It doesn't even make sense to study without it. You need the full distribution of expectations that normal scientists would have so that we can understand when and why they're surprised and how their world of technological and scientific possibilities updates. And so, um, so we've been studying this initially. We looked at this in the perspective of networks where individual concepts or technological components, you know, a method or a gene, you know, or, um, you know, even a theoretical concept were kind of nodes in a complex network. And we would look at how those networks evolved over time, the difference between bridging different parts of the network, the likelihood of bringing a new node into the network. Um, but increasingly with the emergence of, of, of, of artificial intelligence, natural language processing, and now large language models inside those large language models is a very thick, deep representation of meanings. So for example, in, in, you know, the new chat GPT models, you know, there are trillions of parameters. Uh, and those, all those parameters are numbers inside of a graph. Um, that represent a complex representation of all the meanings associated with, uh, with our language. That these language models have, uh, they're kind of the love child between, uh, on the one hand, what I'll call the auto-aggressive language model, where we predict the next word for prior words. And these, and these, and these basically kind of meaning pixels, which is to say, rather than selecting on words, predicting the next word, we're really predicting the next meaning. Right. Uh, and it turns out that the moment they started doing that, all of a sudden these, these models became much more stable because it's not about the particular fragile word that you're using. It's that I'm selecting this meaning followed by that meaning, followed by this meaning, followed by this kind of word function, followed by, and all of a sudden these models worked. Uh, and of course, you know, we added many parameters so that they could self-learn important features. So we use those internal representations as a map of the cultural world. And we can look at the difference between past, for example, research and future research, or we can plot every piece of research in the past and we can, you know, plot a new piece of research and say exactly how surprising is it relative to that past, relative to the past model. Um, and in fact, I just got a $20 million grant from the government, uh, basically build, um, what I call chronologically trained language models. So we're, we're building these models based on the past so that we can evaluate when a new paper or a new patent or a new product emerges, how surprising or improbable was that idea to the, to this model, which is a machine for probability. It's predicting what the most probable next, uh, patent paper or, uh, product description is. So, and when really surprising things happen, what's interesting with these models, we can say not just, oh, that's surprising. Um, everyone knows it's surprising. We can say, well, if this improbable thing is actually now probable, right? If like no one would have thought of this, this paper, but it turns out if you write this paper, you do the research behind this paper, it's true. Then what are all of the other things that were improbable before the become probable now? Yeah. Right. What are all the adjacencies that were like distant before, but now are all of a sudden close to one another inside the space? Um, which allows things like the government to, uh, to take fast breaks, you know, which they don't do historically. You know, if, if one thing becomes true, it's like, oh, okay, here's 10,000 other hypotheses that were impossible or extremely improbable, but now are absolutely the next thing that you should study inside the space. A lot of us start the new year saying that we will learn a new language, but it's hard to actually commit to it. Babbel makes it easy to learn one in less time than you think. Babbel's quick 10-minute lessons, handcrafted by over 200 language experts, get you to begin speaking your new language in three weeks or whatever pace you choose. And because conversing is the key to really understanding each other in new languages, Babbel is designed using practical, real-world conversations. What I love about Babbel is you can either dive in deeply and truly get fluent, or you can just master some of the basics before going on a trip. So let's get more of you talking in a new language. Babbel is gifting our listeners 60% off subscriptions at babbel.com slash mindscape. Get up to 60% off at babbel.com slash mindscape, spelled B-A-B-B-E-L dot com slash mindscape. That's babbel.com slash mindscape. Rules and restrictions may apply. But maybe this sounds like a little scary to me. I mean, if we could predict ahead of time what the next innovation would be, why would we have to do boring things like experiments and stuff? Oh, no, no, no. My whole point, actually, is I'm not building a model that's going to predict the optimal next paper. I'm saying, like, the experiment in this space is that someone did an experiment, and it surprised this whole cultural world model we had. And it allows us to kind of predictively update, okay, how does the whole world change as a result of this surprising thing that came as a result of experiment? Good. Okay. So you're not predicting what the surprise will be. You're sort of bringing the changes on the surprise to figure out how everything shifts in response to it. Exactly. And one other thing that we can do in these models, which I think is exciting, is we can do the same kind of thing that scientists have historically done. So Charles Sanders Peirce, who was, I think, really the great 19th century American philosopher, had this idea of abduction. He was unsatisfied with the philosophical cartoons of reasoning, which were kind of deduction, which kind of comes out of Aristotle's invention of the syllogism, where you kind of build from established, you know, axioms or assumptions or facts. And you kind of say, well, if these are true, then what necessarily must be true? What other things must be true? That was kind of old style science. And then Sir Francis Bacon wrote this book called Novum Organum, The New World. And it had a picture on the front of a big ship going to America and like all this new stuff, you know, like and and it was like, OK, no, we need to scrap that. We need to do induction. We need to basically like learn all the new things in the world and then generalize, you know, from those. It's not going to create something that's provable, but it will allow us to grow and learn collectively. And that was the basis of the scientific revolution and this thing called the experimental philosophy, which is the core of what you're saying. And then Charles Sanders Peirce kind of emerges in the end of the 19th century. He was kind of a failed academic, but but academics were all failed back then. You know, tenure was uncertain. He had a funny personality. And so he bumped from place to place. He was at Johns Hopkins for about a year. He was. He was a lot of places for about a year. The only academic position officially. Yes. So so he had this idea that as knowledge grows, as you learn more about the universe, then your best signal for theoretical development and collective knowledge is surprise. Right. Which is to say when you run an experiment or have an observation that violates your expectations, that violate your growingly accurate world model. And and what's what's I mean, initially he kind of thought, well, where do these surprises come from? Well, they come from, you know, he was also lived at the birth of psychology and was a kind of a proto psychologist. And so he believed that that that the real model was the subconscious, you know, that you would basically people would dream and associations would be made in their subconscious mind. And and he even has a series of stories, including one about he himself. That's very much like Sherlock Holmes, where he is engaged in abduction. You know, someone steals his watch and he like figures out where they are through this set of combination of surprises and coming to the best explanation. One of the things that we've studied recently is, well, how does abduction actually occur? You know, how do people identify surprises? Because it turns out that the people who are most likely to identify a surprise are kind of like the priest class of a scientific field. Like they've read the literature. They know what everyone expects. They know the large language model of the field. And then and then they see a surprise and they realize, hey, this does not fit in. But that class of person is the least likely to have the intellectual resources to resolve the uncertainty, to develop a new pattern. And so we show in recent work with large scale data that on average. That those basically kind of, you know, abductive discoveries are mergers, you know, the conversations between insiders and outsiders. So increasingly you get your resources, not through induction, where you just kind of generalize from your mind about, you know, from from patterns. You have a surprise that occurs in your field and you look, you survey a range of other fields for the intellectual resources that come from the most disconnected fields from yours. You know, you know, you know, it could it could be literary studies, could be astrophysics, it could be molecular biology, people who were not part of your conversational cultural world and who have access to a set of distinct patterns, you know, through theory, through data that is accessible to them. And that those people are systematically kind of like coming in in an expedition to solve your problems. That's where disruptive advance occurs. So so part of this, in some ways, I'm kind of pushing back against the question of like, oh, it's about people choosing where to go. It's it's really it's about this collective conversation that occurs between problem makers and problem takers, you know, inside this market for ideas. I can definitely think of examples in my own fields where some wonderful, brilliant new idea has come along, typically from a young person. And then some older person who knows everything has sort of fitted into the broader context in a way that suddenly everyone gets excited about. That's that's that's that's a common pattern. Another pattern are what I find what I call expeditions, where you have a whole group of people from an outsider field, physicists, you know, coming into molecular biology. And they bring a set of tools, a set of perspectives. And and the reason why in those early works, you don't see teams forming between insiders and outsiders is because teams require a kind of a social contract that this thing makes sense. These expeditions are like it's like blitzkrieg. You know, it's like someone a barbarian coming in, you know, with an approach and and they publish it in in this field that's never seen anyone like them before. And no one from their field has ever seen this field or has ever published in this space before. And those tend to be, on average, the big hits, the things that that really kind of grow and disrupt the knowledge space and really update how people think about problems in advance. It's a tricky thing, I guess, because I think that especially from an outsider's perspective, people are like, well, why isn't there more innovation? Why isn't there more creativity? Isn't there some ossification of your field because you're all working on the same stuff and it's just moving ahead incrementally? Isn't it more fun when someone just completely upsets the apple cart and comes in with the new ideas? And it's not that that's wrong, but I think you need both. Right. I mean, you need some apple carts being upset, but you also need the gradual accumulation of conventional wisdom. Well, exactly. If you didn't have the gradual, I mean, you can imagine a version where it's all disruption all the time. And that is a system with no memory. Yeah. And no accumulation. Right. So it's like, but it's true that there is an oxymoron in creating innovative institutions. Yeah. Right. Right. Because it's like what we need, what we're trying to, we're basically saying, hey, we're going to try to surf the boundary between order and chaos. Right. Very, very hard. And so it's very difficult. I mean, basically, you know, education systems. I mean, these are control systems. Like these are, we've studied syllabi and it turns out the oldest people in the field have the greatest influence on the things that young people learn. And that's a recipe for reproduction and not growth. Yeah. You know? And there's also some worry about bandwagons or bubbles. Like something becomes a popular idea and suddenly everyone has to do it. And then, yeah, a few years later it turns out, well, okay, it wasn't that exciting after all. That's right. Yeah. That's a case where, yeah, there's no memory in the field and everybody jumps to the kind of the hot new thing. We've, of a project that we recently worked on when we looked across countries at this style of exploration. And we find, for example, when we compare China and the U.S. that individually Chinese scientists, for example, move further across topics over their career. Okay. Within any one particular time, China as a whole actually has far fewer topics that are focused on because on average people are following the trend to the center of this big. There are many smaller pockets in this space which are effectively autonomous. People are less likely to move over the course of their career, but that becomes a reservoir of diversity from which abduction becomes collectively possible. I wonder if it's possible to sort of institutionalize and nurture this kind of thing. I mean, we can valorize innovation, but I know just from personal experience, if I write a very, very boring paper in physics that is nevertheless correct in every equation, I can get it published. No problem. There's going to be zero worry that the referee is going to reject it. If I try to be interesting and novel, maybe that's going to be more important down the line, but it's also much harder to get it into the journals because, you know, the referee is like, I've never seen this before. I don't know how to deal with it. And likewise, you kind of feel comfortable hiring people who work on the things you work on or whatever. So, you know, what can we do to nurture the right amount of difference, innovation, thinking outside of the boxes that we built? Yeah. Yeah. So I think, I think that it's, you know, from my perspective and when we've looked at this, you can think of what an economist might call a single equilibrium solution, you know, which is like, what's the right amount for each person? Like, what's the average amount of the balance of innovation versus institutional, you know, kind of history and tradition for each particular person? But again, systematically, we find that it's a multi-equilibrium solution. It's, you know, it's really about a complex balance of people who are much more likely to be the kind of the priests, you know, of the field versus the prophets who are traveling around and coming from outside. So I think one of the challenges is how do we build an ecology that facilitates multiple kinds of career paths that allows people to kind of, and because institutions defend and reproduce themselves, then, then I think, you know, one of the reasons we talk about innovation all the time is because we need to talk about it. And we need to find novel institutions that kind of like resist and push against these natural forces of preservation and memory, which, which, I mean, the entire educational and scholarly enterprise reinforce, you know, journals, you know, method sequences in graduate school, like all these, these things, you know, they're run by thought collectives, which were formed, most of them with, with titles and names, you know, in some cases, two, three, 400 years old. Yeah. Right. So these are very old associations and they're powerful associations and they run things like the Nobel Prize committee. And, you know, they, they have, you know, uh, these have disproportionate amounts of power. So the reason we're talking about innovation all the time is just because we fail at innovation most of the time. Uh, and in fact, one of the things that we were looking at recently, we, I, I, a project I was just writing of this, this morning, um, is just, for example, a scientist age, their, um, their appetite for, for, for, for change, uh, and innovation shifts, as you were suggesting. And one of the things that we find is that, you know, not only, so the average scientist citation. So this is like the distance between the paper they're publishing and the papers they cite age at about, uh, a month a year. Uh, this is in all fields, you know, over all time, except for the one field whose methods haven't changed in 200 years, uh, mathematics, uh, which age at two months a year. Because, and it turns out everyone's favorite paper was published on average a year before their first paper. Uh, and what happens at about year 10 is, is scientists basically start, we looked at all the citations and all the contexts. And on average, they basically start policing their fields. So they basically start criticizing other papers disproportionately and almost always young papers, uh, young people's papers, bringing new ideas and new methods from outside the field or the new to the field. So basically there's this, so one of the challenges here, uh, you know, it's like, how do we, how do we influence this innovation? It turns out that just the demography of your field dramatically impacts the likelihood of churn, uh, of new ideas in the field. And it, and it's not the reverse causal direction. It's not like, oh, fields are stalling out. And so young people stop coming in. It's the opposite. It's, it's, you know, basically if you have a high proportion of old people in your field or even in your department, you can see this at a micro level. Then the field hits rigor mortis and no new ideas come in and new ideas are getting shot down systematically inside the papers that exist. And, and, and, and those new ideas that are getting shot down are in the same context as, uh, like citation context, like paragraph as, as the, as the citations, which are their favorite citations, which are on average, the paper that was published the year before their first paper. Right. So it's, it's like, um, so I think part of it is like, we actually need to, and you see across countries, for example, India has the youngest, um, scientific workforce. And there's a lot of innovation there. China has a slightly older, but much younger than the U S workforce. And there's a lot of innovation of a certain kind, um, there, um, the U S scientific workforce is getting older. Japan's scientific workforce is getting even older. Um, and you can see the reflection of those field level within country, uh, environments, dramatically impacting the likelihood that new ideas will enter and thrive. Um, so one of the ways we manage this is actually by managing the demography and the U S the Supreme court made it illegal for tenure track, uh, professors and institutions, um, to, uh, to have a, uh, uh, an age cap for retirement. So it's, you know, became illegal for ageism. And we can show that in 1994, when that happened, there was just a linear increase, uh, in the age of citations on average and field and a decrease in the associated, uh, churn of ideas within those fields. So I think that's one of the ways we, we, we, we, we have to think about like the big picture, managing the whole environment, the whole gene pool of ideas. This episode of mindscape is sponsored by better help. When it comes to relationships, we often hear about the red flags we should avoid, but what if we focus more on looking for the green flags in friends and partners? If you're not sure what green flags look like therapy can help you identify them, actively practice them in your relationships and embody the green flag energy yourself, whether you're dating, married, building a friendship, or just working on yourself, it's time to form relationships that love you back. One of the great things about therapy is by looking inside yourself, you can both learn to take those warning signs seriously, but also learn to be open to new experiences and new things to know when something might be worth pursuing. Better help is a fully online service that makes therapy affordable and convenient serving over 5 million people worldwide. You can easily switch therapists anytime at no extra cost. So discover your relationship green flags with better help. Visit better help.com slash mindscape today to get 10% off your first month. That's better help. H E L P.com slash mindscape. It is interesting because I've noticed that the best popular music is what I was listening to as a teenager. I think that's just an objective fact, right? Of course. Yeah, yeah. And I think we're probably about the same era. So yeah, the 80s danceable music. I mean, that was like, where is that? Yeah. And am I remembering correctly these studies of where funding dollars go these days or where prizes are awarded? At least in the US, they are going to older people than they used to. Absolutely. Yeah. And that's, you know, they're going to be able to raise money. Absolutely. Yeah. That's, that's, there's a, a linear increase in the age of first NIH grant, for example, uh, that, uh, that, uh, that people experience, um, the prizes on average are highly conservative. Um, which is to say, uh, you know, and the reason is because they're given by context. You know, they're given by the physics community or by, you know, they're, they're given by associations like which systematically, um, undervalue, uh, work that violates the boundaries between those contexts. Um, so, so the things that are the most disruptive in, in citation that, that attract the most, um, kind of burst, the biggest bursts of attention are, are not the same things on average that are, uh, getting prizes, the Nobel prizes, et cetera. Yeah. So back to the, like individual scientists, I know you've written one paper that was kind of provocative to me about why scientists disagree with each other. I mean, the thing about science is, uh, of course you've established some things as, as reliable, but then you're also speculating about what's going to happen next, what is interesting, where to go and so forth. And it's remarkable how people can be really devoted and even passionate about insisting that it's going to be going a certain way. Or, uh, I guess the way that I like to say it that is, that is most, um, you know, generous is every approach has its looming obstacles and every advocate of every approach says, oh, but our obstacles will be overcome. I can see that. Whereas your obstacles are absolutely going to make you stuck. Where does that come from? It's, it's not purely objective. Is it like personality? Are there some cognitive traits that individual scientists have that get in the way? Yeah. I think, I think, I think there's, that there are many forces. I mean, so one are the paper that you're referring to that's, um, that's forthcoming, uh, in nature, human behavior is one where we basically look at, at deep dispositional, um, traits. So we, we look at psychologists, um, we run the same kind of psychological profiles on psychologists that they run on, on college students around the world. Yeah. And, uh, and, uh, and it turns out that they're really stable and strong associations between their relative, uh, you know, acceptance of ambiguity and whether or not they're, uh, you know, interested in, for example, like multimodal findings, you know, that aren't just supported by, for example, you know, a certain kind of statistical or mathematical model, you know, if they can. So, so, so there's definitely, um, the, the, the, they're, you know, the background proclivities, which shape the relative likelihood, uh, of, of whether or not something's, uh, attended to, I would say another approach really is the zeitgeist of an age of a moment. Uh, there's a, there's one paper that I love, uh, it's by a guy named Paul Forman in 1971. It's a book length paper, um, called Weimar culture, causality and quantum theory. Uh, it basically kind of looks at this, uh, it's, it's a beautiful paper. It basically kind of shows, um, it tells a story and it shows that, okay, we, we've got a whole host of, um, of people in the wake in the Weimar Republic in the wake of the loss of World War I. And by Germany who kind of are against causality as, as an approach. Uh, and, and so philosophers are kind of writing against this nihilism as emerging in artistic space. Um, and it turns out that the physicists, there are some physicists who were reading this philosophy, who were writing philosophical tracks against causality. And those were the ones who embraced the kind of the quantum revolution, which is arguably an a causal physics. And so it's kind of like, you know, this moment, this, you know, this kind of post-war, uh, depression, malaise, you know, um, anti and causal enthusiasm, um, allowed the relative selection of what was otherwise, uh, you know, epistemically very unpopular and very unsatisfying from the ways in which. So, uh, you know, epistemic standards in science are, are, are the standards by which we credit something as knowledge, right? If, if we've got standards and, and that, that, that crediting there, there's some strict criteria. Oh, you've got to, you know, we have to demonstrate some empirical accuracy, maybe through experiments or through observations or for quasi, you know, synthetic experiments. But there are a host of other hidden epistemic standards. Uh, and those hidden standards come from what is familiar to us, what we associate with advanced, what's beautiful, you know, what's simple. Um, so we have all these deep preferences about what science should like. And so when science shows up that looks like that, then we are ready to give it awards. We're ready to credit it, uh, with advance. And if it doesn't look like that, then it's, it's often hard. And one of the reasons I'm interested in studying that is because I really am interested in collective advance and the way in which science is and can be an engine. And that means we need, uh, to basically critically observe the impact of these kinds of hidden epistemic standards on, on locking in, uh, a certain kind of scientific, uh, slow, gradual progress. It's a great point because I think I, I came across that, uh, paper you're referring to when I was an undergraduate. And I was very dismissive of it because, you know, quantum mechanics works because it fits the data in some sense. And I wanted to say, like, who cares about the philosophical predispositions in post-Weimar Germany? But now in my, in my, uh, twilight years when I'm more sophisticated, I, I can absolutely see that there's certain ideas you're open to, uh, certain speed at which you're willing to change your mind about things which will be affected by the wider world. And I'm actually, in retrospect, super impressed with the scientists of the twenties that they were so quick to embrace this very, very different view of the world. And no doubt the wider context had a lot to do with that. Right. Yeah. No, and I think, I think, you know, one of the reasons we can study this is just to, to identify it. But I think another generative and constructive way in which we can use this. So I spend a lot of time talking with science funders, you know, and advising science funders in the U S and in Europe and in China and elsewhere. And, um, and so, you know, part of this is, is the ability to do what I'll care, you know, it's science fiction. Right. So, so if, if we didn't have a predisposition against this approach, then let us now turn the crank and see what other kinds of approaches would or could have been, or could now be probable. So if this is the flow of the scientific river, um, then what are other tributaries that, that let's just imagine that some of the decisions that were made to pursue or not pursue areas were not rational. Let's just assume that, that, I mean, maybe many of them were obviously, you know, science moves forward, but let's just assume that some of them have to do with these complexities of history. Now we can basically build, we can use these, these models, large network models are now, you know, kind of like tuned large language models to basically run science fictions. And my group spends a lot of time now creating conferences that never have and never could exist, uh, generating paper topics, patents, proposals that, that, that are, that, that the people don't exist in the population of our world to have created those. But if we had a different structure of education system, then these proposals would be very likely to exist. And this allows us to think about like, you know, with an expanded view, you know, uh, what are places we could pivot to? What, what is the field of action, uh, that we can engage in? So I think of this as not just an act of humanities, but an act of speculation in the, in the most generative tradition of speculation. Can we build engines that facilitate and illuminate new possibilities? I'm very curious as to what are the fun conferences that could have been held in other possible worlds that we weren't in. And do they teach us anything about what conferences we should organize? Well, this is, this actually, so one of the nice things about, um, science again, is there's lots of data. People say on their CVs, every conference they participated in since they were a baby, you know? And so, uh, and, and every year there are new conferences. And I would say, you know, about, uh, about 25% of science funding is convening. You know, it's pulling together people in different places for summer schools. And, and so we have basically experiments continuously that basically, um, allow us to see what's the, you know, what are the effects of the, of these convenings? Basically in the unfolding space of science. And, and we have identified, you know, so for example, one, one, uh, project we're, we're working on is, is there is, you know, there's a kind of a attentional and status order of science, which is to say, you know, like electrical engineers, site physicists, more than physicists, electrical engineers, CS computer scientists, site electric, you know, uh, you know, um, uh, EE, more than EE sites. CS, uh, information science, uh, information science, site computer science, more than computer science sites. So there, there are flows and institutions of information. Uh, and we show basically systematically on average, when something gets cited against that flow, um, it's associated with much more disruptive attention, uh, in the field, which, and it, and it turns out, um, well, you know, maybe those were the best things to explore. Well, it turns out that it's much more likely, for example, uh, for the things that get discovered against the grain to be accidentally discovered from a person at your institution. It's even more likely for you to be married to the person in the lower status field that you get exposed to the, so it's not like just the best ideas are flowing against the grain. It's like, I, occasionally when we randomly violate the status order, then, um, systematically we found, uh, disruptive and disruptively useful things to, to these fields. So, so, um, so there's a whole host of conferences where, uh, in this world, people from this camp are too good, uh, to associate with people from this camp. You know, they have epistemic standards that are different. They have, and we're building these, these, uh, playful, uh, you know, large language model agents that are having conversations and disagreeing with one another and coming up with, uh, and have, who have different dispositions and personalities. And we're building simplexes of these things. Those are kind of spaces where you can basically steer these models, not just through text, but through, just through the, through the, the, the simplex of the model space to pick exactly this particular kind of person, you know, a stoic, you know, physicist with a view of, uh, this from, you know, 1950. And we can put them in a room with someone who has a very different view of the world and, and we can see sparks fly. Uh, and, uh, so I know it's, it's, it's ridiculous. Uh, and yet at the same time, it's, it's just another extension of, of this question of expanding the space of speculation, you know. Thumbtack presents the ins and outs of caring for your home. Out. Uncertainty, self-doubt, stressing about not knowing where to start. In, plans and guides that make it easy to get home projects done. Out. Word art. Sorry, live, laugh lovers. In, knowing what to do, when to do it, and who to hire. Start caring for your home with confidence. Download Thumbtack today. Well, yeah, and it's, uh, it's very useful if you, if you want to go forward and figure out how to do things better. I mean, one thing is that science is just bigger now, right? There are more scientists, there are larger collaborations, there are, there are teams that are trying to get things done. Um, you know, what, what can we say about that in general? And in particular, like, are big collaborations better in some ways than smaller teams, et cetera? So, um, you know, so there's, there's kind of a few layers to this, uh, you know, this morass, right? So one is, is that the field level, as fields get larger, we find, um, there is a kind of caring capacity to, to a field. So you can think of as a field as a conversation. It's a sustained conversation that occurs in departments and in journals and in all these environments. And, um, as the field, as fields get larger, there's an exponential decline in the likelihood that new ideas will enter the canon of kind of most cited ideas. Sure. So if you have two ideas, two fields that merge, they won't retain the kind of the intellectual diameter of, of the independent fields union, you know, those two added to one another, they will collapse. All of a sudden there will be forced competitions between ideas and over eyeballs and attention and, uh, and they'll collapse to kind of one part of the space. So I think one challenge we see is like, as we grow fields, as we have greater and greater success, as we recruit, you know, new ideas, uh, to these, to these, you know, kind of mega fields, then, uh, they, you know, that, that size, uh, is massively sublinear relative to the relative new ideas that those, those new individuals bring. So, um, um, within teams, we see kind of, um, uh, uh, a similar, but slightly different dynamic because teams are, are, you know, constructed, you know, project by project often sometimes, you know, into larger institutions that have, you know, some greater stability. But we find that, that big teams, um, you know, they tend to not produce highly disruptive work. Um, and, you know, so they, uh, and, and there are a few reasons, uh, that appears for this, you know, so what, what do these teams do systematically? Well, they produce papers really quickly. So they're more productive on average. Um, that's one of the things that, that, that teams have to demonstrate. And, and how do they, how do they perform that productivity, uh, and still get, you know, a measure of attention? Well, they tend to do what like a big production company does. You know, if they're trying to decide between, uh, you know, producing Transformers 9 and, you know, Slumdog Millionaire, it's like, they're going to predict, they're going to do slow, you know, uh, they're, they're, they're going to do Transformers 9. Like they, they, they're going to bet on the winning horse. And this is what happens in science. They basically take huge popular ideas of yesterday and they basically momentum invest. They take like the next step and they're going to get, you know, like Transformers 9 is going to get Transformers 8 receipts minus Epsilon. And, you know, and they're going to get, you know, some, all, you know, most of the market that existed for the prior finding. And so small teams are much more nimble and, um, and part of it is because they have to be, because they can't produce Transformers 9. They can't be the biggest hit that builds on Alpha Fold 2. Like they can't do that. So they have to do something different. So they dig deeper into the past. They dig deeper across fields or farther across fields. Uh, and, um, and so they're much more predictive of advances, uh, that are likely if they succeed to occur and be important in five or 10 years. Basically when this, when whatever this bet we've made has really run dry, like we've, we've mined all the good stuff out of this vein. It's like, that's when like, we're like, oh wow, those small teams had, you know, other ideas. And it turns out that, um, when we look at the structure of teams underlying those papers, so we can build hierarchies, uh, within those, those projects, we find the flatter the teams, which is to say the more people that are involved in like the design of the ideas. It slows down the production of papers, but it increases the instability of the papers. It makes it more likely for those papers to have fused ideas fundamentally from different fields inside these spaces. So it's a, it's a, it's a complex bet. You know, if, if, uh, it turns out that people do more innovation on average than they're, if they just were trying to maximize their citations would recommend, because as you said, you can publish something if it's true, but boring, um, and familiar. Uh, but, um, but at the same time, uh, you know, every time we measure this, like how much innovation is happening relative to what the optimal amount would be to maximize discovery while retaining some memory in the, in the, in the system, we find that more is more, you know, more, more is, is better. We're not, we haven't reached this optimal limit of innovation inside almost, uh, all the fields that we're looking at. Um, because, you know, we may have a plan. Oh, Hey, we're going to, we're going to build, um, an institution where 15% of the grants go to completely new things. And, you know, 85% of the grants go to completely established things. But then even within those proposals, you have established people who are running both of the, both the innovative and the non-innovative. Like it, it just, the, the, the conservatism goes all the way down. Right. And so, uh, but there's also the, the fact that most things that might qualify as innovative are bad, right? Like there's a lot more ways to be wrong than to be right. Absolutely. Yeah, absolutely. Yeah. Yeah. Most, yeah. So it's, it's really, so this point becomes critical. It's like, if you want to increase the likelihood of disruptive success, um, you have to increase your failure rate. Yeah. Good. Like you, you, you, you, like if you're, if you're in a lab or in a field that is consistently making good bets, then a robot could have made those bets. Yeah. Like you're not succeeding. Like succeeding is violating expectations, um, which, uh, and most of those violations will obviously be wrong because we know things like, because a lot of this physics, a lot of these other fields, uh, you know, have insights that, that work or that are highly descriptive. Um, um, I will say, um, this is where, you know, another place where kind of AI comes in, um, fields that are kind of like fundamentally multi-scale that are, that are complex. And by complex, I mean, like self-dissimilar, you know, it's not fractal. It's not like if you see it at this level, it's also true at this level. It's, it's different at these different levels. Like these kinds of complex systems have, uh, you know, in many cases really have not even been the subject of intensive scientific study because, because we don't have scientific traction. And these are the kinds of places where the kind of function, arbitrary function fitting that large AI style models are, um, actually bringing a really big delta of inference. And so it's, it's less likely for them to discover new things about simple and basic physics and about some of these other things, which have been, you know, we've had great minds on them for, you know, centuries. Uh, but, but areas which are kind of like, okay, the intersection of, you know, physics and chemistry and biology, complex systems, self-dissimilar systems. These are the spaces where, um, you know, like the right, most parsimonious equation might be like 500 pages long. Like that, the most parsimonious, I'm not saying just the most overfit, you know, like that might be the best description and it might actually be quite predictive and quite general. So, um, so these are, so all of a sudden, you know, science, um, leaves the capacity kind of, of the, of, uh, potentially of, of the human brain. I think this is, this is one of those science fictions that we have to consider. You know, alpha fold two is learning a huge complex function that does not look like, uh, any equation that has sit inside biology techs ever. Um, and it kind of killed, you know, structure prediction. I mean, it just does it better. Uh, and it, and it, and it's not reducible. We're not going to be like, we're going to search through the neuroscience of that model and find, you know, like, oh, it's doing this one thing like really well. Like, no, it's, it's, we've squeezed down those models and they're still large. And, uh, and, uh, so it's kind of like, you know, yeah, I think, I think we're entering an, an interesting era where we're considering the fact that, you know, there are going to be different forms and shapes of, of knowledge, science, and kind of technology or control. And there are increasingly, there are going to be some things like, you know, alpha fold two, which is not science. I mean, it's, it's, it is a regularity. Um, the machine knows what it knows. So it has machine science. We don't have access to that science. Um, is that there's a great paper called crumbs from the table. It's at the last page of, uh, of nature magazine in 2000 by Ted Chiang, where he basically says, he kind of describes a world of metahumans. At that time, he was kind of talking about these pharmaceutical, you know, uh, creatures that we couldn't understand anymore. And, and he kind of talks about, you know, he imagines a world in which all of science is hermeneutics. We're just all interpreting the artifacts that have kind of come to us without understanding. And we're trying to do neuroscience on these artifacts. And I find myself and a lot of other scientists, this is in fact, precisely what we're doing. We're trying to make sense of, of brains that look like they know things. And, and, uh, you know, that's a great point. I recently talked to Jeff Lickman, who is, um, a Harvard professor, neuroscientist, who is a leader in, uh, mapping the connectome of the human brain. And one of the things I thought was interesting and it was unexpected in our conversation was he pushed back on the idea that we're trying to understand the brain. Because he said, look, if you could understand the brain in sort of, in the sense of coming up with some simplified description of it, then the brain would be simpler. Like, the brain is as, as simple as it can be. And it's super duper complex because it's doing these things. We need to sort of figure out what it does, but we're not going to find a, you know, a short description that encapsulates the brain. Right. Yeah. So, and this, but this is, this is, this represents a new era. If we take that seriously, um, if like, that's what our intellectual, um, you know, uh, ventures become about, then, then, then science changes its character. It changes its standard. It changes its meaning. And, and, you know, and the epistemic standard of, does something feel right? Does it look right? Does the equation, you know, have the right number of variables? Is it elegant? You know, all these things, um, you know, I won't say they go by the wayside. I think they're important. Obviously, pruned models and simplified understandings allow them to travel like, like kind of components, new areas. So there's all kinds of reasons that simplification is useful. Um, it just may be, it, it, it, it, it's likely to be for many of these complex self-dissimilar systems that, um, that the components that we learn, you know, from these systems are much larger in themselves, more complex than we had previously imagined. And they, and, you know, and they become objects in the world, like a fab facility. I was in Silicon Valley when they, uh, they moved the first big, uh, chip fabrication facility Intel from, from there, you know, in the kind of San Jose area to Ireland. And they had done so many changes on that facility. Like they had, you know, they, they'd fixed, you know, something like 15 million errors. So they had no idea why the fab facility worked at the tolerance levels it did. And so when they moved it, they were like, okay, we need to retain everything. Like it's orientation to magnetic North. It's like, you know, like the relative density of the air, you know, it's, it's elevation. Like they had no idea and they took it over and it, and it broke for reasons again, that they didn't understand and had to, had to kind of fine tune out. But that it's like our, our new theories are looking more like this, this fab facility that produces like chips with a high accuracy, low tolerance level. And it's unsatisfying as a scientist in some ways to look at that fab facility and, and say, wow, that's, that's what we know. You know, what is that? I don't even know what that is. Like, like, uh, yeah, I mean, I, I guess I kind of get it and I have very mixed feelings about it. I mean, it's, yeah, no, no, my feelings are not going to affect, uh, what, what's going on, but, but to, to sort of, I mean, put it in other words and you can tell me whether I'm capturing it correctly. I mean, there's a whole new way of understanding things, of, of, um, modeling things, of being a scientist. Once you have, uh, large language models probably isn't the right word, but, you know, these, these deep learning, um, high, many, many number of parameters things, you can capture complexity in a way that is unprecedented and impossible if you, if you need a one line equation to, to count as understanding. So you can do that. Um, but so much of science is sort of counterfactual, right? Is sort of saying, well, if things were different, what would happen? Um, and without that simple understanding, I mean, maybe the, the example of the fab factory is, is, um, uh, an important one. Uh, how good are we at saying we truly understand, or even the AI truly understands if we can't say, well, when we change things, what are the effects of those changes? Uh, so we, we, I've got a paper that's, uh, that, where we, we look at the impact of AI on science over the last 25 years. And of course it's gone through different generations. First it was machine learning and then it was kind of deep learning. And now more recently it's these large transformer style, uh, kind of foundation models. And we look at the impact on scientists and on science. And we find that basically on scientists, it increases their mobility across scientific ideas. It increases their speed of getting papers done, decreases the number of people in this papers, increases their rate of promotion. Um, uh, for science as a whole right now, uh, it actually narrows the scope of ideas that are discussed because these are big data seeking models. And where do we have big data? One problems that we've been generating data forever about problems we already know about. So, so exactly as you suggest on average, right now, these models are not being too, too, too. Origins questions. We're going to actually engage in, uh, kind of, too. We're going to actually engage in, uh, kind of like continuously steering counterfactuals in a way that was unprecedented before, you know, creating a factory for counterfactuals. And, um, so I actually think it's possible for it to enter some of these, these new areas, but it's going to require a new range of conceptual thinking. And scientists effectively will have to become in some ways philosophers of science, you know, where they're selecting between philosophies. They're navigating, you know, what their tolerance for kinds of, you know, philosophies of hypotheses are. And then, you know, the machines will produce them and in some cases will test them. Well, I guess creativity is a big sticking point with the AIs. I guess there's a school of thought which will say that they're just not creative in the standard way. They're remixing all these various ideas that they've been fed from their training data or whatever. But I see no reason in principle that an AI couldn't be creative. I mean, if only because you could throw some random numbers in there. But then there's a question of how do you cleverly throw the random numbers in there? Like, how do you nudge the AI towards being creative in useful ways? Is this just my lack of understanding or is this a frontier here? This is an interesting frontier. I've got a DARPA grant with Law Varshney, who's a computer scientist and a really creative engineer at the University of Illinois at Urbana-Champaign. He was at IBM when he created this kind of Blue Chef system that kind of created really creative combinations of ingredients and recipes that then, you know, would automatically kind of create. And so we're spending time looking at the sciences and also building kind of theories of creativity that will facilitate the kinds of abductive creativity that we see in the world. But I think a critical piece of this is what we were talking about earlier. I mean, outsiders and insiders, abductions, surprises that are because like our expectations that come from literature and like the data that come from experiments and observations are separate. They're not. So what does that mean? We, you know, large, you know, language and large multimodal models and these deep learning models, they're not one model. They're many models. And so we basically have to exploit that underlying diversity and accentuate that underlying diversity. We have to separate some of these in turn. So if you just train these models in prediction, you'll find that they actually create themselves little schools of thought that then they recombine at the last moment to make predictions. So they're doing science inside themselves for us to use them to kind of push the boundaries of science. We need to use them to generate really deep expectations. And then we need to use other models to survey the space of experiments and observations. And we need to like identify dynamically, like what are the surprises and how should that. So absolutely, if we just, you know, use these models to create more data and then use that data to train the model, we'll have a collapse of attention and the models will not get better, they'll get worse. But if we basically build an ecology of models in the same way that we need to cultivate an ecology of scientists with different temperaments, from different fields, with different expertises, those ecologies of models have the potential to kind of create a new world of creativity in the same way that like when we think about AI safety and it's like, no, okay, we've got to, we have to put the thumbscrews, the, you know, restraining bots on these robots. No, we, like we do the things that we do with unpredictable intelligences in the past. You know, we make them sign the Magna Carta. We create a set of independent, you know, institutions, checks and balances. Like we need to create an ecology of regulatory AIs, you know, an independent court system, you know, an independent, you know, legislative body, you know, I mean, all these things are, that's how like safety works. That's how innovation works, even though those systems are kind of in some ways opposed to each other. We do it collectively. And so we need to basically harness ecologies of kind of artificial intelligence models and agents to kind of create the same recipe basically for knowledge generation that we see working or have, you know, has worked really increasingly works in the human system. I mean, that seems to be a common thread here. I mean, one thing we didn't get to is a paper you wrote on political polarization in editors of Wikipedia articles and how you get the best articles when it's not written from a single point of view, when there's some team of rivals aspect there. And in some sense, you're saying that's just as true for the computers as it is for the human beings. That's right. And the problem is that, you know, the computer model generators would love to be monopolists. And so, you know, they're trying to sell a product that is the one true AI product, you know, that has all diversity completely within itself, embedded. And the answer is that cannot possibly work as a long-term generator of surprise in the same way that if you have two communities and you force them into one conference room, they will talk about half of the ideas that they talked about when the two communities were separate. Like we need to create an ecology of AIs that, you know, that basically enhance the population genetics, you know, from which future innovation becomes possible. I mean, I'm glad you just said that because I think you said it before, a version of it, but it didn't quite sink into me. You need the different communities to talk to each other, but they also need to be different communities. Otherwise, they become just homogeneous. Yeah. And this is where tradition is absolutely critical in creating membranes between communities that can basically self-evaluate and have their own tastes. Like, so, so I'm not a, I'm not an anti-field. In fact, I think, I think fields are absolutely critical in this connected age. Like they need to hold their own standards. Those standards become resources from which other fields can draw. And if you get to the point where everything is interdisciplinary, everything is post-disciplinary, then you destroy the very intellectual, dispositional, and epistemic assets that themselves become building blocks and new knowledge. It's just mush rather than a bunch of interesting little things. Just mush. No structure. That's right. Yeah, block hole. Okay, good. It's the end of the podcast, so I'm just going to completely change gears and give you a couple of minutes to talk about the fact that you and my old physics buddy, Daniel Holtz, teach a course at the University of Chicago on the end of the world, on the forthcoming doom. This seems like a, you know, bleak topic for a college course. Give us the sales pitch for why we should be thinking about these ideas. Well, I think if you look at the mammalian kind of historical, you know, prehistorical record, you find that on average, like a species lasts for about 3.5 million years. And we're about, you know, I don't know, one, 200,000 years into our lifespan. That means we should have 3.3 million years left. If we're average, we might be better than average. We're smarter, you know, than many of the creatures that went before. But who thinks, you know, who's listening to my voice now that we can project a thriving world in 3 million years, right? We haven't, you know, we can't think 20 years. We're making decisions on, like, you know, a next year basis, on two years, on our competition between some other country in 15 years. You know, the biggest plans are 2050. You know, that's 25 years. I mean, that's, you know, like, I think Dan Holtz, who, you know, is a physicist and worked at Los Alamos National Lab and is very sensitive about, you know, the potential for, you know, nuclear arms to get in the wrong hands, the wrong place, and to set off a cascade of, you know, so I think he's thinking things could happen on a very short time frame that could be cataclysmic. And there are an increasing number of those things because they're the flip side of powers. Yeah. Artificial intelligence and environmental and, you know, nuclear powers that we're controlling can get out of hand. There can be lab leaks. There can be, you know, nuclear meltdowns, right? So the more powers we have, the more ways we can destroy ourselves. From my perspective is even if we destroyed ourselves in 10,000 years, it would still be 3.3 million years too soon. You know, we need to build. And why don't we think longer? You know, why don't we have a longer time horizon? It's because of the very specific evidence-based epistemic standards that drive scientific advance. Like we have standards for what represents knowledge. And if we, if, you know, for many fields, you know, in modern economics, if you haven't done a causal identification of a certain flavor, then it is just, it's just not knowledge. It's like, you just, you can't even make the comment, like just get out of the room. You know, it's just not, and so we've got like this great scientific, scientific standards, which have been very historically useful for advance in our fields, but are ill fit for thinking about alternative futures. And these singularity events where something melts down or something explodes or something takes over and it's all over. I mean, we have a single experiment that we're running, which is humanity. And, and so we have to imagine alternatives in a way and with a precision that we have never done before, which invites us to explore new epistemic approaches, include, and including things like narrative, including things like science fiction and stories to think about possible futures and harms because of the risk that we're creating because of our increased power over the world. So I think, and so I think, and so I think I actually found the class therapeutic students came in with anxiety about climate and about other issues and, and in talking about them and facing them and talking about, you know, policy possibilities and thinking about alternatives. We actually, I think it's, and, and, and, and, and, and also a long view. You know, we don't think about this unfolding set of future possibilities and people and environments that, that will be the inheritors of our choices today. And so, anyway, that was the reason for the class. There's a tricky balance, right? Because you want to emphasize the very real worry that disastrous things could happen with the lesson that, and there's still things we can do about it, right? There's still tools we have to prevent it. So, you know, be alarmed, but don't despair. Right, exactly. And I think talking through with experts from around the world about, you know, some of whom were despairing, some of whom were busily engaged in, in their particular projects, I think gave us an expanded view for how to think about talking about knowledge extrapolating from the singular experiment, which is humanity and this world and this universe, you know? All right. You've given us an expanded view. James Evans, thanks very much for being on the Mindscape podcast. Thank you, Sean. It's, it's been a pleasure. Thank you.
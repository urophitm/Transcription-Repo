 What are the most important insights of the 20th century, my opinion, was the finding that with a very simple set of rules, you can achieve what's called universal computation. It's common wisdom that our models of computation achieve universality, but it's wrong. I'll explain why. When you take a step back, you see that there are these molecules within cells that resemble strings of symbols, and they also fold up into these tree-like structures that kind of would be very useful for doing computational stuff. This is Brain Inspired, powered by the transmitter. Hello, I'm Paul. My guest today is Hesum Agluck-Poor. Hesum is a postdoctoral researcher at Rockefeller University in the Mame on Lab. His experimental work is in fly neuroscience, mostly studying spatial memories in fruit flies. However, we're going to be talking about a different, although somewhat related, side of his postdoctoral research. This aspect of his work involves theoretical explorations of molecular computation, which are deeply inspired by Randy Galistol and Adam King's book, Memory in the Computational Brain. Randy Galistol has been on the podcast before to discuss his ideas that memory needs to be stored in something more stable than in the synapses between neurons, and how that something could be genetic material like RNA. When Hesum read this book, as you'll hear him describe, he was reinspired to think of the brain the way he used to think of it before experimental neuroscience challenged his views. So it reinspired him to think of the brain as a computational system, but it also led to what we discussed today, the idea that RNA has the capacity for universal computation. So we discussed that background and story. Why universal computation hasn't been discovered in organisms yet, since surely evolution would have stumbled upon it by now, and how RNA and combinatorial logic could implement universal computation in nature. And a little bit about how Hesum developed the ideas for how this could all come together. Show notes are at braininspired.co slash podcast slash 199. If you enjoyed this episode, you might also like episodes with Randy Galistol and David Glandsman. Those episodes are 126 and 172 respectively, which I also linked to in the show notes. Thank you to all past present and future Patreon supporters. One of whom actually just created a brain inspired search engine, which was shared in the discord. So thank you for that Brian. I hope it's a useful resource for our little community here. Okay, here we go with Hesum. Last time, well, I guess we were off the boat. So I was at this comfort, this workshop in Norway that you were, you're right. And that's where we met and you were talking combinatorial logic and RNA then. And that's what we're going to talk about now. So, so it was fun on the boat with you, getting to know you a little bit and good to see you again. Yeah, good to see you too. Yeah, I'm super excited about this opportunity to talk to you. I told you that I was a long, I was an old fan of this, this show I started listening to it like very early in my podcast and to imagine that I'd be speaking on it is a very exciting thing. Well, I would be remiss to say you actually had mentioned the brain science podcast by Dr. Ginger Campbell to me and how that was an early influence. And she was like an early, I loved her podcast too and that was part of the inspiration eventually when I started brain inspired. So shout out to Ginger. Yeah, yeah, I love that podcast. I wish it was still going on, but yeah, sometimes I just like catch myself going back to listening to really old episodes. Oh, yeah, because she does a really good job. She's a really good host. I'll just leave it at that. Yeah, but yeah, unfortunately she doesn't she doesn't make it anymore, but I mean, I remember going on runs in Nashville, Tennessee. You know, you have that memory of where you were when you heard something or when you were reading something and maybe we'll talk about that with that gallastol book that we'll mention in a few minutes, but I just I remember specific places in Nashville listening to, you know, her podcast and just enjoying it a lot. So but anyway, good to have you here. So what we're going to talk about is what you've been on lately, lately for the past few years, which is the RNA and universal computation, but but that's not how you came here. So I know you've worked with Drosophila. You've done a lot of experimental neuroscience work up to this point. So just what what are you doing? You're what's the right way to say this in real life? What in your day job? Yeah, yeah, my my day job is basically doing experiments on flies. I'm I'm in Gabby namens lap here at Rockefeller and basically I'm doing fly neuroscience, doing behavioral experiments using your postdocs imaging. Yes, yeah, I postdoc. Yeah. Yeah. So okay, so all right. So I just want to bring that up because what we're going to talk about is something that you and I also also shared sort of well, I want you to tell your story of how you came to this, how you came to what you're studying now, just kind of as a background, because I had the same I wonder how many what percentage of graduate students have this sort of what would you call it? Disillusionment? Well, a lot have that, but very specific kind of disillusionment in that like, oh, yeah, is this all wrong? Like what, you know, that that is a pretty major disillusionment, but not necessarily is this wrong, but a conibption about what you're doing and stuff. So yeah, tell the listeners, yeah, yeah, sure. So so I I base I was in I did my undergrad in in computer engineering. I was I was really into like computer science algorithms data structures. I felt that you know, I was very proficient at that stuff. And then for grad school, I decided that I want to go into neuroscience because this is the most exciting field right now. And the brain poses a very challenging problem to scientists. And it seems like, you know, I can use all of the skills that I learned at computer science to try to understand this very complex system that's, you know, mainly known for being a computational organ. And so, you know, I cave in kind of naively thinking that, okay, all this stuff that I learned about designing algorithms, data structures, you know, figuring out what algorithmic complexity this algorithm runs at, what's the memory complexity, all this kind of stuff. I thought that would be relevant to the study of the brain. And so I like just rather than you thought, oh, I'm going to find all these algorithms in the brain. I'm going to find the computational complexity and it'll map on to processes and stuff. Was it more direct or was it just relevant? You thought? I don't know. I don't remember exactly what I felt, but I feel like I felt that I had the right skill set. But very quickly, I was humbled to learn that actually not not not any of this is is useful. I mean, it might be useful in your like data analysis or like coding up some kind of behavioral experiment. But to understand the brain, it's not like classical computer science is it very relevant? And I guess the talking point that everyone would use is that brains are not designed, they're evolved, they're messy, they don't conform to like, you know, engineering standards of design. And so we're going to come back to that very point when we talk about sorry, then around to we're going to come back to that, of course. Yeah. Yeah, yeah, definitely. And so, so yeah, so basically the first few years of my grad school, I kind of learned that the brain is not a computer. It's just like a it's a messy wet organ and you're going to have to understand that the way it is and not try to impose your own idea of how computation should be on that on that organ. And then and and you know, like towards the end of grad school, I kind of felt that sense of disillusionment that you were talking about, about the whole field. Like what are we doing? Like it didn't feel like we're making any progress. We're just, I mean, I'm saying this in a very, I guess, not in very generous way, but let me just say in the most extreme way that that we're just collecting data. We're just we're just collecting more facts about the brain and not really making any insight. Well, that's the that's the old criticism of biology when it was called it, was it Ernst Meyer? Called it stamp collecting. Yeah, stamp collecting. I mean, and I and I agree that this is not a very generous weight of frame it. I'm just expressing the feeling that I had at the time because you know, not, I mean, people are doing amazing work. Of course, not everyone's stamp collecting. Good say. But you know, I really believe it. It's not as the same, but it's true. Just like the general, the general direction of the field seemed kind of aim aimless to me. And then and then that was, did your own work feel that way also? Because often people think that, but then next except my work, what I'm doing is is right on course to solve the thing that I need to solve. Yeah, I guess I mean, I like the work that I was doing. The issue was I had high hopes for like a certain direction. What I was trying to do. So I worked on rodent neuroscience in grad school with with Alana Witton at Princeton. And I started off thinking that I'm going to solve basically like some some very fundamental thing about how working memory functions. I was trying to like, I was thinking in terms of, okay, I'm going to optogenetically turn off all these cells and then get the rat to forget what it the short term memory that was in its head. And that kind of like that was a very ambitious goal that I had in mind. And I didn't get to that. I got to something that was valuable, you know, understanding how the strideum is involved in working memory. You know, I made a small contribution, I guess, to to that field. But it was it was more like, it wasn't that related to my own stuff, that disillusionment that I felt was more just about the whole field. Well, just I, are you familiar with the quote? I'm going to misattribute every single quote I try to quote here. But I think it's Mike Tyson or at least it's usually attributed, attributed to him that everyone has a plan until you get punched in the face. That's kind of like how experimental neuroscience works, right? Or a lot of experimental science, I guess. Yeah, yeah, yeah, yeah, external biology. Yeah, exactly. I hadn't heard of that, but yeah, sounds all right. But so, so you you were doing good science and Alana's lab's very good, you know, and you continue to do good experimental neuroscience research, but but eventually you felt that disillusionment with the field as a whole. Yes, until I suddenly got my hands on Randy Gallistel's book, Memory and the Computational Brain. The reason why it resonated with me was that it allowed me to unlearn what I had learned about computer science being irrelevant to the brain. And I'm talking about classical computer science like algorithms and data structures. Because the first nine chapters of the book are basically just honestly, I skipped the main text of those nine chapters because it was like stuff that I already knew from like, you know, studying computer science. I just skimmed through them and read the summaries at the end. But then the rest of the book was actually, you know, I was a grad student and you know, I was kind of surrounded by very smart people, people who are very knowledgeable that had a certain perspective about neuroscience. And then here comes a long Randy Gallistel, this professor in psychology with, you know, a very good reputation of being like a serious scientist, saying that actually, you know, it's okay to ignore this common wisdom that everyone is saying and treat the brain as a computer and use principles of computer science and theory of computation in your study of the brain. And that got me super excited because like, you know, that's kind of the reason I came into neuroscience. Yeah, wait, so I don't know if we said the name of the book. It's memory and the computational brain. So I'll link to it in the show notes, of course. But yeah, so I've had Randy on and I've had in a similar vein Dan Glansman on, and now you will be the third person to be kind of talking about this. It could be RNA or something subcellular, something molecular. But yeah, so he, so in that book, he talks about path navigation, how ants keep track of where they are and how some of the stories that we neuroscientists we don't think about ants too much, but neuroscience, the field of neuroscience has a story about how it works and you know, he goes into arguments. Why wouldn't work this way? Same with like bees, you know, so he goes through lots of examples, carefully saying, well, this would not work. And then of course some of the learning studies that he and others have been involved in. So that, but so that opened you up into feeling okay that it was okay to treat the brain like a computer again. But did that make you feel like the brain is a computer again? I particularly remember in the last chapters of the book, when they started speculating on where the solution might be and that's where they brought up the authors of the book, the Alastone King, they brought up the idea that it could be stored in molecules the same way that we have information, genetic information stored in DNA, maybe it's maybe it's that's how cognitive memories are stored or it could be something else, it could be like, you know, the same way that you could have specific changes to molecules like I don't know phosphorylation of some molecule and the way that that those phosphorylation rates are distributed across cells or something like that, there could be various ways that you could imagine memories being encoded. But, but you know, it kind of allowed me to let go of this synaptic hypothesis that's kind of the dogma of the field. Right, so let's just I just want to spell that out really quickly. So, so throughout the book, Galastol and King build the argument and Galastol and other works, that there are these problems for behaviors and memory and learning that we don't have solutions for in the spiking patterns of neurons, which has been sort of the hope and the assumption of neuroscience. Everything is spiking and everything is how the neurons communicate with action potentials and the patterns. However, he goes to great pains to show in multiple cases that there is not a good story and that it doesn't seem to be possible a good story on principles and just correct me if I'm wrong as I'm sort of spouting a self and memory. Yeah, that makes sense. I mean, and and the arguments were a lot of them were conceptual in that book. So, for example, there was an emphasis on the need for a read right memory. Right. And synapses aren't really a read right memory. Like, you can't go in and write a specific value into a synapse or read a specific value that was that stored in a synapse. And, you know, many other conceptual arguments that Galastol has made in his other writings. How do you encode a number? Like, what's the code? And a lot of people kind of rush those questions aside and I kind of understand their arguments, but I just don't agree with them anymore. Yeah. Any more. Okay. So, so you have this experience reading the book. You couldn't put it down. And that book specifically and most of Randy's work is on memory and learning and how those could be implemented at the subcellular level with, you know, hypothesized subcellular substrates like DNA, RNA proteins, etc. Fossilation, methylation, other various possible means of doing things. But then you kind of took a different course on it because is this a good time to talk about your interest in universal computation and touring equivalents, etc. Yeah. Yeah. This would be a great segue into that. So, most of of Randy's arguments come from an angle of understanding memory. And for memory, like the concept of memory isn't like not everyone agrees on what memory is and, you know, there could be semantic debates that just kind of pop up on the side when you're discussing what is the physical substrate behind memory. And there's another angle which you could take, which is just as rigorous, if not even more rigorous, which is computation. You can ask what is the scope of a, what is the computational scope of a system from like the lens of theory of computation. And when you ask that question, that kind of also leads you down towards molecules and RNA. And a lot of the the paradigms that we have, the models for computation and neuroscience, fall short of what's called universal computation. So actually, maybe it's better for me to just go straight into what what I'm talking about. What is universal computation? Yeah. And why we care? Yes, exactly. Yeah. So, so, so, okay, so in the, in the theory of computation, there are, there are various levels of computation power that a system can have. Okay. So one system can, you know, it might be able to compute a certain set of functions. Another system might be able to compute more functions, just the same set of functions as another system, but even more. So for example, finite state machines, they can compute things like, what's the remainder of this number when you divide it by seven? There's a single finite state machine that does that. And it will do that for any number. It doesn't matter how many digits you give it. It's always going to be correct. But then there's some problems that finite state machines can't solve. And like, I don't know of what's one example. Like, I don't know, is this string of parentheses balanced? That's a problem that there's no finite state machine that can solve that problem for any given string. However, there are other systems of computation, which could solve that. Basically, the point is that you can have a different computation systems that are able to solve different sets of functions. Okay. Now, what are the most important insights of the 20th century, my opinion, was the finding that with a very simple set of rules, you can achieve what's called universal computation. You can build a system that's capable of solving any solvable function, any computable function. And when I say capable of solving, it requires a description of the algorithm. So it's not like, okay, I have a universal computer and I can solve everything. No, you need to find the algorithm that solves certain problems. So when I say capable of solving, it means there is a description of a program for every computable function. It has the capacity for that description. Yes, that's right. Yes. And a really cool thing is that they had these competing models of computation back in the 1930s. General recursion theory and Lambda calculus and then Turing machine, a Turing's automatic machine, which we now call Turing machines, came along. And within a few years, they realized actually all of these systems, which were intended to be models that capture what it means to compute, they all are equivalent, meaning that you can simulate any one of these systems with another of these systems. And that kind of led to the idea that maybe we've arrived at something very profound. Maybe we found a limit to what's computable because there are functions that you can describe that are not computable. But like the functions that well, we'll get to it. We'll get to it. But like, one to one example is Kighton or Chiton's constant. Actually, I'm embarrassed to say I don't know how to pronounce his name, but let's say Kighton's constant. It's a number. It's a very well-defined number, but you can't compute it. But let's not get into what the things that are not computable. The point is that you can really easily reach that level of computation power, where for every computable function, you will have a description. That description can be the description of how a Turing machines operations work. It could be a description of how a Lambda calculus function works. But the point is for every computable function, you're going to have a finite length string that determines how the system operates through time. And that will lead to solving a certain function. This is kind of the theory of computation lens. Now, you can ask, we have these models of computation and neuroscience. Like, we have neural networks. How is a neural network a computation system? Well, for every function, you can have a description of a network that may be able to solve that function. The description of the network would be the set of neurons, the weights between these neurons and the activation functions that each neuron has, I can, I can like, in a string, I could describe a network. And this network would be solving a function. And then you can ask, okay, well, what are the set of functions that a neural networks can solve? And it's common wisdom that our models of computation achieve universality, but it's wrong. And I'll explain why. So, so, so, wait, our model, our neural models of computation? Well, our models of biological computation. Okay, okay. Okay. So, yeah, so back in the 1990s, there was a series of papers that showed that you could simulate a trigonomicine with neural networks. The problem was that the kinds of neural networks and dynamical systems to, the kinds of neural neural networks and dynamical systems that were shown to be able to simulate Turing machines, they are irrelevant to biology because they lack structural stability. They're even irrelevant to engineering. You couldn't even engineer these systems. Can you can you have certain? Yeah, yeah, yeah, I think you're about to kind of go into more detail on why. That's the case. Yes, yeah, so, so, the crux of the matter is structural stability. When you're describing a dynamical system, that the system includes a number of parameters. And then you can ask, what happens if I change these parameters by some infinitesimal small amount? Will it still resemble the same dynamics of the original dynamical system? In other words, is there in technical terms for those who are interested? Is there a homeomorphic neighborhood of dynamical systems to this system that you're describing? If there isn't one, then you're describing a singular point in in parameter space. Fragel. That'd be fragile. Yes, exactly. The smallest the smallest error in your parameters when you're trying to implement this system would result to something that's vastly different. Okay. And this comes from this is not my argument. This is Chris Moore's argument, which was actually the first person to show that dynamical systems can be used to simulate terrain machines. He argued that structural stability is a reasonable criterion for systems that either an engineer can build or you would be able to find a current nature. And then he also conjectured that no universal dynamical, no universal finite dimensional dynamical system would be structurally stable. Okay. So just to pause here, make sure I'm getting this. How do you square this with the idea of degeneracy in circuits in the brain, for example? You can use this exact same circuit to produce different rhythms, or you can use different parameters in the same circuit to produce the same rhythms in this case. So that seems unstable, right? It's robust. Not sure if that would be structurally unstable, because the thing is almost all of the models that people use, even in not just for not just in like studies of biological neural networks, but even in AI, almost all the models that people use, they are robust to a small enough amount of error in their parameters. Otherwise, they would just be irrelevant to deep learning. Like in deep learning, you're searching for a network that fits a certain function, and you're just moving in parameter space. If the target, if the solution is a single point with no clues nearby, then it's hopeless. You can't find that solution. Right. Yeah. Okay. I thought you were saying the opposite. So I misunderstood. I thought you're saying that biological systems are inherently stable. That is what you're saying. Biological systems. Yes. Yeah. Well, I would say our models of biological computation that we actually use, that we actually think might be relevant. They're all structurally stable. Okay. Fair enough. The model is stable. Yes. Yeah. We're always talking about, I think this model is how the brain computes. Right. And those models, they don't have this weird feature of structural instability. And it's kind of like, it's as if we're paying lip service to universal computation. If we just say, okay, look, RNNs are universally powerful. And then we never even talk about that kind of neural network that is universally powerful. We just like, there's a subset of RNNs that we actually study. And there's another set of RNNs which are universal. And don't as far as we know, those don't overlap. So for it to be universal, it has to compute that single point. Is that correct? Well, because you have to be exact when you're computing. I'm sorry, I'm so naive here. No, no, no, no, that's a good question. The way I would say it is, people have come up with a way to describe a single neural network for every computable function. However, each one of those networks is a single point in space. Oh, okay, I see. And just to be clear, are you talking about like the universal approximator theorem? No, no, no, no, that's very different. So I'm talking about a Seagullman and Santag. They had a neural network system that basically uses the neural network. I think it was conceived as the membrane potential of a single neuron as a unary stack. Like you can imagine there's a string of digits after the decimal point that represent the membrane potential of a single neuron. And if you treat that as a unary stack, you can compute with it. If you have like three stacks, you can compute with it. There are other ways to do it with a dynamical system models that aren't necessarily like neural networks. But they essentially treat digits after the decimal point of a number as a string of symbols. Strings of symbols are really important actually. That's kind of one of the arguments for RNA. They really allow you to express computational power in a computational system. And you can achieve it with like if you treat a number as a string of symbols. But I really don't think that's how the brain works. Okay, so let me just rephrase this then and see if I get it right this time in my own words. So there is a space of possible neural networks, parameter sets, architectures, activation functions, et cetera. Of that entire space, there are discrete points of the combinations of all of those different things that lead to universal computational abilities. And every other point that's not those discrete points are not universally computer computer. Yes, yeah, basically the point is, if you're talking about the subset of RNNs that are relevant to biology, we don't know if that covers all computable functions. Okay, okay. We don't know if it matters either. Right? So it matters to you. Yeah. Yeah. Yes, this is a big deal, right? Yeah. Yes. Yeah, no, that's a fair point. So so the thing is I find it really hard to accept that biology would not have stumbled upon universal computation because it's it's such an easy thing to accidentally stumble upon. When you're working in abstract systems, there are several examples of this like for example, cellular automaton's like linear cellular automaton's, wolf friends rule 110 accidentally stumbled upon universal computation. It wasn't intended to be a powerful computation system, but it was discovered to be weighing tiles, conways game of life. There's a lot of examples of people stumbling upon these like very complex unpredictable systems and later discovering that they're universal. And so it just it feels I don't know hard hard for me to believe that it biology can evolve something as complex as the eye that conforms to the principles of optics that uses a lens and an aperture. But somehow it just it doesn't care about the principles of computation and it can't can't achieve something that's so much easier to build than an eye. And and just from my intuition, it feels like and this is this is an intuitive, intuitive based argument and it might not be convincing to everyone, but I just feel like a universal computation system would have enormous selective advantages for organisms that are striving to survive and reproduce and solve complex problems. Yeah, that's that's that's why I think it's a meaningful and important question to ask where is life's universal computer? Where can we find it? You would mention to me that you think that this is so we've been talking basically about neuroscience and the models in neuroscience, but you think this is relevant to artificial intelligence as well. Yes, yeah, basically I the um so one of the things that that I've noticed right now in the interaction between AI and neuroscience, which is actually there's no interaction. There's no interaction. You've talked about this before on previous episodes I know. No, so there's there's this one interaction that I can I can confidently say exists. So I I can't tell you the number of times that I've spoken to someone in machine learning or just non neuroscience AI and I've explained how there's a problem that our current models of machine learning are learning functions in a space that's not touring equivalent and I can get into to that in a moment because that's also something that would seem contrary to common wisdom. But but yeah, I'm like I have a very similar critique to to current approaches and in machine learning and my argument is that hey, we're not taking universal computation seriously. And then the response that I've yet I can't tell you how many times that the response that I got was okay, well if the point is to create an intelligent system, aren't we intelligent and aren't we neural networks? So at the end of the day, if you're arguing as against neural networks, how are we intelligent? So so in in a sense those people that are working in AI and working on these neural net models, they're relying on the confidence of neuroscientists that this is it, this is it's a it's a neural network system that's doing this. Oh no, they're not. No, they're not. They're not relying on neuroscientists. They're just they're just building their models. They're they're building their models, but there is an assumption that there's no barrier to the computational ability of neural networks if if the target is an intelligent system because if you believe that but let me just because that has nothing I believe that from the common AI engineers perspective that has nothing to do with neuroscience. You you disagree? I don't know. I mean, I think I think well, I mean, at least in the in the discussions that I've had with people, I find them referring to the fact that we are intelligent and we are neural networks. Oh, okay. Fine. Yeah. And and and so so you know, there's there's there's gotta be a neural network solution to intelligence. That's true. And that that is the common assumption among neuroscientists as well. Neuroscientists. Yes, but but but I think I think the reason that but it's true also. Well, we have neural networks, but we are neural networks, but of course we have but we have a few other things as well. Yeah. What do you mean that it's true? We have a brain. It's true. We have neural. Yes, yes, but the question is the question is is is that enough for universal competition implemented through a neural network model versus some other kind of model that might be at the molecular level? And and so what what I was trying to get at was this was this um, mutual interdependence of neuroscience and AI, how how like AI researchers are relying on the confidence of neuroscientists that okay, you know, a competition is happening through neural networks. And and the other way around I feel that the neuroscientists see that the the most advanced cutting edge models for you know, AI look really like neural networks. And and you know maybe it's not exactly biologically plausible yet, but there's going to be some mapping at some point. That's that's kind of how both fields are relying on each other's confidences that that neural networks by themselves can solve intelligence. Okay, Ben, you think universal computation is required to solve intelligence, whatever the hell that means because what something you said earlier was we're human, we're intelligent. So therefore we think we can solve intelligence. I wanted to jump in and say yeah, we define what intelligence is. So it's not like intelligence is out there and we have some and we know what it is. We actually define it, right? So that's a semantic issue, but yeah, I mean, and then and then you could you could come up with like a new definition that's not grounded on us. And I don't really want to get into the semantic argument of what intelligent it, but what intelligence is, but my point is like universal computation doesn't care about the needs of an organism, for example, right? So every definition of intelligence of the million that are out there, there's something about learning and unpredictable environments, adapting to learn to do the thing that you need to do solving the problem, right? And these are all like problem solving things related to what you're what you need to do. But universal computation doesn't doesn't care about what you need to do. It's just a capacity to do anything, right? Well, well, yeah, I mean, the question here is is it's not it's not about okay, can I learn to universally compute? The question is when I'm learning, what is learning about? It's about picking a function in the space of all possible functions. Okay, it's about you have a bunch of examples and you want to find the function that solves these examples. Now, again, the question, the same question comes up, what's the space of learnable functions in your system? Is it the same space as all computable functions? Or are you just leaving out a ton of functions? And so for example, like I don't know if you're thinking about addition, let's say you have you have a bunch of input and output test cases. You could solve that benchmark for addition with a lookup table. Okay, and if you're learning algorithm, if you're learning method is restricted to lookup tables, you're going to find a lookup table that's going to solve that benchmark. But you're not going to solve addition. Okay, if you want to solve a addition, then I hope that the scope of functions that you're searching for that you're learning in includes programs. And you might be able to stumble upon the program for addition. And that program can solve things that are not in that benchmark. It can generalize. Now, I'm not saying, I just want to be clear, I'm not saying that current methods are lookup tables, but that was just an example to illustrate the point that the space of functions that you're learning in really matters. So shouldn't we be way better at math if we have universal computational abilities that is guiding our cognition? Sorry, it's a very naive dumb question. But I don't know. I mean, I don't know how to answer that. I mean, I guess we are good at math. Right? I mean, there are people who are very good at math. There are examples of people. Yeah. So yes, but they're not they're not running on magic, right? They I mean, there has to be some kind of way. Yeah, but yeah, okay, I understand that there are savants in many different areas. Maybe that's not the best example, but shouldn't we all be, right? Or is the brain or our neurons in our way? And if we could just get to the RNA computation, we'd all be the brains are slowing our universal computers down, you know? No, I mean, that's I don't I don't view it as like, okay, there's there's the neural network and then there's the RNA and these two things are like very different things. That's not how I how I would the neural network, which is won't listen to the RNA who's trying to tell it, you know. Yeah. Yeah. I mean, and and related to that, there's there's some if anyone wants a primer for for this like this small field of within neuroscience, I would really recommend Sam Gertian's paper. And the reason I brought that up right now is because it's it's an attempt to synthesize the view of of synaptic based memory and molecular mechanisms for memory. So in the second half of the paper, he lays out some some kind of model that that would synthesize these views. So it's, you know, how do you connect these the idea that neurons are talking to each other through synapses with this idea that maybe memories are stored molecularly. The first part of that paper is the primer that I'm talking about because it has it's it's some the the the best intro to this field that I know of. It covers a lot of the conceptual reasons and the empirical reasons why the synaptic story of memory doesn't really hold up. It also has a good a good summary of some something that happened back in the 1960s where there there was a a short period of time where a lot of people were working on what they called macromolecular and gram theories where they thought that that memories could be stored the same way that the same way that we have genetic information stored within molecules, maybe memories are stored within macromolecules and RNA was kind of one of the the the leading candidates for this. Let me just define in gram for those listening is basically a physical trace of memory in your brain however that's instantiated. So some people think the in gram is a a certain set of cells that are associated with the memories. Some people think the in gram is stored within the synapses and then a growing number of people perhaps think the in gram is laid out physically within these molecular structures macro or micro. Yeah and let me just clarify something that like a lot of the times when I bring up the idea that molecules could be storing memories one common response I hear is that of course molecules are storing memories everything is molecules synapses are also molecules like it's going to be molecular but the the real point is where where is that information stored and maybe maybe a better a better description of this would be an atomic theory of memories like this like like the same how are how are gene stored what's the what's the mode of genetic information encoding it's really about how atoms are arranged within molecules not how molecules are arranged within the cell right so you're taking a very reductionist approach I mean that was going to be my reaction to what you said about the common response was like well yeah of course and it's also an atoms and it's also in quarks and it's also but you can do that all the way down but we don't say that about genes anymore right now we say genes are stored as sequences of nucleotides I mean there could be there could be little I don't know tricks that organisms organisms use to also like you know carry information through transgenerations like you know there's epidysionics there's there's there's also different ways that you can have inheritance of information across organisms organisms but the main way that we conceive of genes being stored is in the sequences of nucleotides so that's it's about what the right level of emergence and emergent properties is the level to that carries the most causal information about what we're talking about man that was a mouthful sorry yeah I know that I mean that I'm yeah that makes sense there's a lot of parallels and I I talked to you about this when I met you a few months ago there's a lot of parallels between how this issue is being treated now the issue of mammary angrams how it's being treated now versus how it was treated how genetic information was treated before the discovery of DNA people used to think that it's messy it's like there's not going to be a clean story for it it's in proteins it's kind of like you know every cell has a different protein composition and proteins are rich in information because I mean they didn't they didn't use that word information but they're very rich and you know the the protein composition of a cell of a turtle is going to be different from a human cell and that's what leads to you know a human being formed versus a turtle being formed but then the central dogma came about the N.A. genes RNA to proteins and it turns out it is messy just in a different way well I wouldn't say yeah well we even actually I don't even know if it is messy it's still the messiness right yeah the messiness that we see right now still me might be a result of us not understanding the system correctly and the way we need to do that is through universal computation right is that what you're good well I think so I mean there's there's a whole there's a whole debate right now in well over the past 20 years there's been a debate over the non-protein coding a portion of the genome the job is it functional is it not yet I used to be called junk now nowadays nobody really calls it junk but like there's a there's you know once one end of the spectrum believes that it's not functional most of it and the other end of the spectrum thinks that it most of it actually may be functional and actually I when I stumbled upon this literature it was it was very exciting to read it's one of the most heated debates that I that I know of it that's that's like out there in papers that you can read what the functional like the junk versus non-junk so yes yeah yeah over the past 20 years I guess the main the main proponent of the idea that the the non-coding DNA is functional or one of the main proponents would would be John Matic and and and he was kind of if you look up his publications you can find the trace of that debate but but the idea is that hey okay um uh well there's there's a couple arguments here so the people who say that most of the most of the DNA is is non-functional they usually rely on things like conservation and if you look at if you if you use conservation as a criteria for what's functional or not you you you come up with like a a upper bound of let's say 20% of our genome would be function what do you mean conference it conservation sorry um it's it's uh what what what what uh portion of the genome is conserved across species oh that kind of conservation catch yeah so it stays the same stays the same is the same across species yes yes exactly yeah sorry I'm trying to you I'm trying to just make sure no no yeah that makes sense yeah and and and so um the other end of the debate they they they would argue that no conservation is not is not a good criteria there's many other there's many other criteria is that you you can use for for for hints for functionality um one of the one of the arguments that uh John Matic actually has brought up is that uh you see that the non protein coding portion of the genome uh the ratio of non-protein coding to protein coding increases uh as a function of organismal complexity uh so you know and in single cells um it's a it's a lower proportion and it and it just increases as you go into multi-cellarity and and um the the criticism towards this is that well um what is what is complexity exactly uh how do you how can you assign complexity to organisms uh and that's a fair criticism but what happens is if you sort animals if you sort species based on this criteria of what's the proportion of non-coding to coding it just looks intuitively like like it's an increasing increasing um a complexity do you know if you if you do the same thing with brain relative brain size it's the same it's it's not I think answer like above us or something uh relative yeah okay on the logarithmic scale of brain complexity size to body mass yeah um all right now all right we can I don't I'm like gonna look it up at the moment but yeah yeah might but I mean yeah I mean there's there's there's I get the point here is like it's like we're looking for like some sign that some some indicator that puts us um uh uh puts humans on the top of the chart and that's kind of a weird thing to do and it's a very human centric uh the earth has to be the center of the universe kind of approach um but but nevertheless there is this problem of okay what is all this what is all this non-protein coding DNA doing um and is it is it just transcriptional noise because a lot of it's transcribed like that's basically what happened 20 years ago is that we realized that um uh these portions of the DNA that don't encode for proteins are being transcribed um there's a lot of specificity within the cell like you can see that a lot of them are localized in very specific ways and we don't know what they're doing there's there's um you know you can find correlations with certain like traits and diseases and then and then you would see that those uh those uh those non-coding RNAs that are associated with a certain trait um are actually expressed in uh tissues that are relevant to that trait like if it's some neurological problem uh then it's also expressed a lot in neurons um there's a lot of little hints like that uh that say that okay there's a story that about about genetics that we don't understand getting back to your point about okay look genetics was actually messy uh the thing that wasn't messy was how proteins are encoded there's a very clean story to that there's uh you know um uh there's a codon for every amino acid there's a lookup table that this system uses and it does a very simple translation of RNA molecule strings to amino acid strings which become proteins um that there's it's a it's a you know a remarkably elegant and clean system to encode proteins um still the story of okay how does this actually encode for for an organism is messy but uh maybe that's because we just don't understand the system well enough um in humans uh less than two percent of our DNA ends up in messenger RNA and most actually half of messenger RNA usually on average and humans is uh untranslated so so ends up being less than a percentage point of our DNA encodes for proteins has sequences of of nucleotides that encode for sequences of amino acids uh now right now the story is okay well the rest is like there's a lot of regulation and it's all like it's it's um it's it's all about how how are these um uh proteins uh how is protein synthesis regulated uh across different cells um but you know it's it seems to me that like when i when when you take when you take a step back you see that there are these molecules within cells that resemble strings of symbols okay uh string of symbols that come from a very small alphabet of four letters um and they also fold up into these tree-like structures uh that you know uh kind of would be very useful for doing computational stuff um could it be the case that these molecules are involved in something more than just regulating the synthesis of proteins uh across cells maybe something else is going on maybe something uh uh uh some deeper explanation would actually make it make it make it make sense okay hold off on that because i want to because you're what i want to ask you how you even came to appreciate the combination of RNA and combining it with um combinatorial logic however yeah combinatorial logic yeah what am i saying am i it uh it's it's combinatorial not uh oh so there's two things there's combinatorial logic and there's combarena tore logic and combinatorial logic is the one that that i'm saying no you you you said you said you just pronounced it differently you said comb combinatorial combinatorial combinatorial yeah combinatorial yeah yeah geez all right combinatorial logic combinatorial I'm gonna now i'll just have to go back and edit myself the whole yeah sorry the combinatorial combinatorial combinatorial yes yes that's right that emphasis is on the first syllable yeah aclipore aclipore yeah exactly yeah okay this is staying in that way um oh no yeah of course because it's just me pronouncing things like an idiot um okay but what what i wanted to hang on to for just a minute before because i want to ask you about how you came to appreciate this or how you came to this idea is so the regulation story right what i've come to appreciate uh through works like alicia hoarero terrence deacon the auto poetic like you know verala montamora like the whole it is like in-life systems like if you have in a system the the contextual things the regulation things are way more important than the thing that we think is doing the thing right so you drop so so in a like a water maze the walls or let's say a river right you don't have a river without the um banks right the river affects the banks and banks affect the river um but when we talk about rivers it's like the river is the thing but the river is not the thing it's the banks and the river and they're affecting each other so there's top-down causation bottom-up causation um and so but the i so then i think so alicia hoarero hoarero wrote this book i think it's called context changes everything um and and she has argued for this strongly that these processes like um are all affecting you like you have to appreciate the context of whatever process is happening within which a process is happening just as much as what you consider the process so so then and unfortunately something like dna to rna to protein that manufacturing system if you want to call it that could be like a huge bureaucracy where we have like all this super unfortunate regulation that seems somehow necessary for like a giant democracy to get nothing done but maybe just like the very little that we can get done done i'm sorry that's a bad analogy but of course the other analogy i would make is people who work on cognitive architectures have come to appreciate that all right you have a working module working memory module you have a long-term memory module you have a executive module getting those things to work by themselves not that hard getting them to work in concert that's the hard that's the really hard part so it's how do you regulate how do you make these things act together and it turns out that the regulation part of it is a huge part of it so i just wanted to linger on that for a second to say well yeah i mean there does need to be a lot of regulation maybe it's not all regulation right well okay i mean now that you describe regulation like that i i think i would agree with all of what you said i mean but i was imagining that as computation right like there's there's there's there's decisions that need to be made of of how do i direct um um you know um approach into the themselves function yes exact approach into the membrane the self-function what what um uh you know where where does the cell actually go in space in the body plan uh um you know what kind of what kind of proteins do i need to um uh well so the the issue is there's there's there's this concept of gene regulatory networks okay yeah and and that's kind of i think what people mean when they say gene regulation uh if if what you mean is uh something more broad than i think i would agree with you um because gene regulatory networks at least the way that they're conceived of right now um are as powerful as finite dimensional dynamical systems they're just the same issue that comes up with neural networks comes up with gene regulatory networks where you have this gene is inhibiting this gene and this other gene is you know uh uh promoting this other gene and you know you have a big big network of uh gene interactions that define okay which which uh genes are expressed and which are not and it's very similar to the to to a neural network kind of model of competition if you think that's how it's being regulated i think that's um i would take an issue with that because of the computational capacity of the system that you're describing i think it needs to be it it's it's very likely that if you if if if biology can or has achieved universal computation at the molecular scale it will use it for uh development for for you know um implementing a body plan what about cognition? The cognition too yeah i mean the the thing is there's uh there's so many domains of life in which computation is not just useful but essential um what is your blog called life is computation? yeah that's that's the name of the blog life is computation uh let's not um not so so there's this we're in a very come everything is computation in neuroscience and i'm reacting generally over the past few years to that over the course of a lot of conversations i've had on this podcast like let's say it's true like let's say RNA has the has this universal computational capacity and man i don't want to just blower on here i want to get to the this this story but what if i mean if life is not computable and it's i don't believe it is because the universe is open and so there are there are only like solvable problems enclosed domain i'm not using rigorous mathematical terms here yeah i don't know if that's that's what people mean when they say non-computable in classical theory of computation that's your um the thing is okay so so you you just said there's an open domain like it's an open system it's not a closed system yeah and in fact uh that that's that's uh kind of the that's how you get universal computation a system that is able to recruit more dimensions to store its state uh that's the key ingredient for arriving at universal computation but if you think of a program yeah go ahead no i don't mean interrupt i'll come back to it but okay so if just to just to illustrate what i mean by that if you think of a program uh in that's running on a computer it occupies a certain amount of memory and it has the instructions that would enable it to expand in memory to recruit more memory if it needs to uh and it might run out of memory on your computer which you can conceive of as a closed physical device but that program isn't really you can't describe it as a closed system because it's it's progression and time requires it to interact with the environment and recruit more uh space to you know actually go on with a computation uh and in all of these like the the reason i say that that's the miss the the key ingredient is that and a lot a lot of these abstract systems that stumble upon universal computation accidentally they happen to have this ingredient like if you think of con ways game of life um you can think of like some pattern of on and off cells uh and it's you know uh it's it's always finite in size but it can expand in the surrounding space and if you're implementing it on a piece of paper you might run out of space on your piece of paper but you know that you fail to to to um stick to the rules of the system uh uh at some point and you need to add more paper to it you're describing a system that's essentially open so we have not that much time left and i want to because we haven't talked about combinatorial logic yet really and and its connection to RNA so so you have this problem so all right let me just sum up here so you go through this uh disillusionment and graduate school you stumble across gallastles book you start thinking about computation and you realize well maybe you return to the idea maybe i can think of the brain as a computing system or a computer um how did you come across the idea that RNA is a universal is universally has that has universal computational capacity um and then how did you how did you connect that with combinatorial logic yeah so so RNA was like kind of in the spotlight already just from like all these other people that are in the field that have have put it forth as a a um candidate for storing memories or for computation uh i mean it wasn't the spotlight for me uh like since grad school um that's because it's it's kind of stable enough that it could last like not a protein that would get degraded over a day or whatever um and for other reasons yeah i mean we've learned we've learned that it's stable recently i guess uh ten years ago um uh there uh the the nose i mean there's like there's a paper that came out earlier this year that shows that you can have RNA strands and the nucleus that that um last for for years for the lifetime of of of the animal um but um uh and i can i can send you that paper if you want to link that in the show notes or whatever um but um i mean a lot of people think the RNA is being short lived and transient um i think the reason why RNA well there there there are many reasons that it that it has come up but um i think just there's something very appealing about it being a string of symbols um but yeah i mean that that it it was basically in the spot i mean in the 1960s it was the like main candidate for these macram macromolecular theories they they would like uh you know um they would claim that we purified RNA from a a planarian and injected it to another one and it worked and that's uh this is related to uh your your interview that you had with David Glanzman uh he's kind of uh also landed on uh RNA methylation as as a as a very promising candidate for for a memory in epigenetic mechanism but anyway like but we'll hang on let me let me ask you so yeah was it also in the 60s when did the idea uh that RNA may have preceded DNA as like the original life molecule right so it used to be used and people used to think well DNA uh is the original molecule and then RNA came out of that and then you know to to produce proteins well i don't know if i don't know if there was a time where people thought DNA was the first molecule uh i know that i know that the idea that uh there was an RNA world that there was a world where um uh before DNA and proteins there was RNA that comes from uh the realization that RNA both has a coding encoding capacity uh in the sense that um you could have a reader like a like a ribosome actually read its content and translate it to something but it also has an enzymatic capacity uh so it can uh serve as an enzyme to chemical reactions um and so these two capacities uh people would argue are the main components molecular components in life and RNA has both of them although not as efficiently like proteins are way more catalytic and and and DNA is like way more better at storing information so then these two things evolve later and i think that's the main argument for the RNA world hypothesis i don't know actually how um serious people take it today you know uh what else you're like yeah you know what else it's so DNA involved for better storage uh proteins involved for for better enzymatic activity and and brains involved for better cognition yeah that that evolved later too but as long as as long as you're willing to concede that RNA had the computational capacity to begin with and then yeah yeah capacity is great that it's just like whether how whether it's implemented in so yeah yeah capacity i'm all about capacity sure yeah so so where where was i uh so i i interrupted you and asked about the RNA uh progenitor story with respect to DNA and in the like the RNA was the early storage thing but you were about to uh so you were talking about in the 1960s uh that was them the molecule that people thought could be used like as a symbolic string string of symbols right yeah i mean i mean a lot of people i guess based based on just the discovery that genes are actually encoded in molecules maybe that that wasn't was inspiration to the theory that cognitive memories might actually also be stored in molecules and so some people thought it was proteins uh in fact the earliest proposals were in the 1950 before the discovery of the double helical structure of DNA and those were based on proteins those were in a time where yeah a time when people thought proteins were more stable is that what you're going to say well well a time that people thought proteins were genes the genes were proteins actually okay um yeah yeah the the majority uh i mean at least according to this book uh the eighth of creation uh until the discovery of of the double helical structure of DNA most biologists uh thought that genes were proteins okay all right yeah um and but anyway the the point is that in that era uh in the 1960s uh there there like uh dozens of different uh um papers that that um are working on this idea or or doing like like different kinds of experiments on on this idea of of uh memories being encoded in molecules uh and they used different approaches not not all of it was like this uh crazy feeding planarians to other planarians the James James uh McConnell is known for um people for example showed that uh you have um changes in RNA composition with learning and uh you know they they would do experiments where like I don't know you put put a put a rat on a tight rope and then you would show that okay the the ratio of of um uh RNA nucleotides changes in this brain region um and and so yeah and but the the issue i mean that that that that that that field kind of died out uh in in the beginning of the 1970s um and i guess that one of the main critiques to that theory to to that to that um subfield uh was uh that how do you know that the changes that you're seeing are actually encoding memories um like you know people could you you could extract some kind of chemical from a learned uh learned animal and inject it to some other animal but that could just be like a hormone for fear or something but like how how do you know that that's actually encoding the content of the memories and uh they really didn't have the tools at the time to to to really study that question deeply okay so i remember now where we were we kind of where because i was asking you how you came to the linkage between commonatory logic and the RNA story and you started to talk about the 60s and how this is kind of an old story that went out of favor but you were you were so so then how did yeah go ahead yeah so so our RNA was kind of like already in the spotlight for me uh just because like there's many people uh that that have that have brought it up as a good candidate um and then uh since reading Randy Gellistel's book i had been thinking about okay how would a computation system that's really universal how would a really actually universal computation system look like at the molecular level um because you could imagine like okay it's cool RNA are strings of symbols are you gonna treat it like a touring machine tape hmm like with a touring machine that goes in and like you know edits this this sample and moves next i mean that that can't possibly exist in cells like you would you would see it like in uh the ribosome which does something very simple which is just translates nucle uh every triplet of nucleotides to an amino acid um that's huge um and it's visible in electron microscopy so um uh trying to say that something like a touring machine might exist uh isn't very plausible so so what what could a what could a computation system look like and this has been on my mind um ever since and i think i think the connection between commonatory logic and RNA happened when i when i realized uh well two things basically um one was like i just learned that RNA has a secondary structure uh meaning that it the same way that DNA strands can come in and fold uh and form these double helices like by by you bring in two strands of DNA and they they they connect to each other and form a double stranded uh uh helix um the same thing can happen within the same RNA molecule so an RNA molecule is a strand is a string of nucleotides uh and you can have one segment of that strand um pair base pair with another segment of that strand um and that can happen it has to sort of fold on itself and the pairs have to be the right pairs that would match and it can do that sorry i'm sorry i'm trying to just be crystal clear yes no it can do that um let's say it's like a hundred pairs long and if like the first and last four pairs are the ones that match it would fold in on itself and then you'd have like a big loop of all these pairs that weren't matching and then those four that just connected at the bottom like a little tiny lot of it. Yes but usually usually there's a lot of matches it's not just like the yeah yeah yes yeah exactly so so um you know you can uh like people do this that you can you can study a certain uh um RNA strand and study its secondary structure and usually the maps look really pretty there's they're they're they're very intricate there's uh many layers of of essentially parentheses people when people want to represent the secondary structure they use parentheses because if you think about it uh uh one part of the strand coming to another part of the strand is is matching a nucleotide to another nucleotide and and usually it turns out um uh it has a parenthesis structure sometimes you get these things that they they call pseudonauts which kind of uh deviate from a balanced parenthesis structure um but usually it's it's a balanced parenthesis structure um yeah so so is there sorry this is an aside but so when they're given i don't know a thousand base base nucleotide RNA sequence um they're going to be lots of places that could attach in so what is the possible uh number of secondary structures that a law you know a piece that long could form yeah that's a great question and the answer is a lot like like and and in fact we know a lot of RNA strands have um uh many different confirmations that they can take um uh and it's not it's not always static it's not always a single structure um yeah they they can so like a single are you saying like a single thing can fold up one way and then in a given solution relax and then fold up a different way yes yeah absolutely and in fact like that's how that's how ribosign's work like there's there there could be an aren there could be an element within an RNA strand that you know folds differently depending on temperature uh and so that can be used as a sensor for temperature um but uh but yeah okay so so that's that's like a very important feature of RNA strands um and then and then just out of curiosity like i was like i don't know actually i can't remember exactly how i stumbled stumbled upon combinatorial logic uh was it did you did you see lambda calculus first i mean did you probably knew about the impact i think i think yeah i think i saw lambda calculus first i don't even remember the the first time i was confronted with it but like i was trying to learn how it works and then and then um at some point it just clicked in like i think i think um i remember the moment that it clicked in oh what was that you gotta tell that story what was that uh it was it was there's nothing special but i was just like i was sitting in my desk and i was on a Wikipedia page i think a Wikipedia page for combinatorial logic and then i was like wait a minute this is that it's it's very similar to how we represent RNA strands there's these there's these uh parentheses that that you use for like representing a secondary structure and like you know you have a limited alphabet a of combinator's um usually it's like i don't know snk or bc k and w um and uh that's it with that limited alphabet you can express you can express any any um a computable function and the rules for running that function are very simple they're very local um wait how long was this moment probably a minute i don't remember where you had all these thoughts um well yeah i mean i guess they're just noticing noticing the parallels between the two and immediately afterwards i was sure that somebody had written about this uh this is so obvious once you once somebody i was sure somebody wrote had had like there was some there was there had to be some paper or something um and i kept on searching i couldn't find anything um which kind of made more and more excited wait okay but that was your way okay so i'm sorry i'm just i'm interested in this so you had this moment at at the computer and you realized it and then how soon so that must have been like oh my god and then uh and then immediately you're like oh well this must have been done that's kind of a downer on that on the moment right um i don't know yeah i guess i guess uh yeah i was i was preparing myself to be very disappointed that somebody else had brought this up yeah that's right you'd been in academia for a while and an experiment it's all neuroscience so yeah okay yeah it all makes sense yeah yeah sorry yeah i don't know um yeah and so so um and then and then uh we had this this like a journal club uh with with uh my advisor here um Gabby Maim and uh he's also like very interested in in in how RNA uh might be involved in computation and he has a lot of he shows a a lot of the same ideas with with this uh a growing group of people that that think molecules might be involved in computation more than than is appreciated and so um we had this this journal club with two other neuroscientists Abbas uh Rizvi and Jeremy Dittman you know we were like you know reading papers and and just thinking about how how molecules could be involved in memory and computation and that's kind of um where i had first uh tested the waters with with these ideas and and it was very it was very critical and kind of just just polishing the theory to like get get the feedback in this group it was it was it was really nice to have this group of like intimate four people like uh sitting around and then critiquing that you know this this part doesn't make sense or like the you know and and a lot of the things like the a lot of the perspectives that i that i had about like um that that i just recently described uh um for example like that the the ribosome is a very um um uh impressive molecule and it's and it's doing something pretty simple and it's large and you can see it uh that's something i specifically learned from from from uh Jeremy just that that understanding of that appreciation of the ribosome um and so so yeah so that that's kind of where i you know i um i had the opportunity to uh kind of flesh out the details of of of this idea so so i asked you about lambda calculus because i know that combinatorial logic and lambda calculus share a lot of similarities right but but you have mentioned i've heard you mentioned that combinatorial logic is a combinatorial logic is um precedes um you know touring machines and lambda calculus so what the hell what what was what was that story yes i love that you brought that up because because it's it's not appreciated that the first mathematical system the first abstract system that humans came up with uh that had a a universal competition capacity was combinatorial logic it was discovered by this mathematician named uh Moses Schoenfinkle uh and that's uh and and and that's kind of the only thing that i know he did uh i don't know if he had other any other contributions and then he was just forgotten and then haskell curry rediscovered it again um and then and then realized oh somebody else had worked on it and so he gave credit to Moses Schoenfinkle but i guess even it even this this uh this is and this emphasizes the point it was discovered twice independently before before we had any other system of computation by only two humans and if we have this universal computation shouldn't have happened much more that's i'm gonna keep coming back to stupid i don't know i mean i mean uh yeah maybe maybe maybe uh prehistoric times okay four units two prehistoric and two post yeah okay but so then then what's the connection why is why is it special um because the i think i think um the point i'm trying to make about it being the first one is that it's simple it's very simple it's like uh um and it's also very beautiful it starts off it's a it's a it's a very it's a functional programming language uh there are no there's nothing that's not a function everything is a function uh they call it combinators everything's a combinator uh the inputs of these combinators or other combinators there's no like primitive uh data types so it's functions of functions of functions that take in functions and spit out function exactly yes and and um every function takes a single function and spits out another function uh at the way that you can get a function that takes two inputs the way that you can like build a function that takes many inputs is to say okay like let's say i want a function that that does addition addition takes two inputs uh so what it does is what what what the way i can do that is i can say i'll take i'll define a function that takes a number and then spits out the function that adds that number to any new input that it gets uh and that's that's a technique called curing after haskell curie okay that's the second person who invented a commentary line there's no finishing um finishing culling is that the what's that no no no i guess i guess this technique was was actually uh uh exclusively curries idea um i'm not sure about that actually i i i i i i might be wrong about that but anyway um so so it's it's it's very simple and then it just also very nicely maps on to our name our na biology um because when you if you want to implement something like this at the molecular level the main challenge is parenthesis matching um regulation in other words um yeah i guess i mean if we're we're even a broadening the definition of regulation even more now okay it's like housekeeping right like i mean it's just counting parentheses is not doesn't sound it's not the sexy computational thing it's like i got to keep track of where i am in this nested series of yes yes yes yeah and and so so you could do that explicitly like like if i if i'm actually evaluating a commonatory logic term on paper that's probably what i would do i would i would like keep track of the of the depth of the parenthesis and just keep on going and then uh uh use use that technique to to determine what's a term what's a single term and then you know you could do stuff with that term um and then uh but but in RNA because of that intrinsic uh secondary structure that RNA have you don't need an explicit machine that goes in and does these matching this parenthesis matching because uh matched parentheses are are already proximal in space in physical space yeah so so what that allows you to do is you can implement every single one of these handful of application rules and commonatory logic with local operations with local strands of RNA with local with with local operations on a on a part of an on a on a part in our a part of an RNA some of which is paired with itself in a certain section in some of which in this in this open loop yes exactly so so um to i mean i guess most people will be listening to this so there's no like there's no illustration but i'll try to describe it in the in the most um uh illustrative way and for people for people who are watching i'll just put up a still of you giving a talk with like the hairpin loop structures right sure yeah yeah maybe maybe like the the rules or or like it doesn't oh also i don't have to be in it but like uh i don't have to be in the the picture just like uh just like um uh what what showing how one of the rules can be implemented one of the common tree logic rules can be implemented so so uh these application rules and the combinatorial logic um they're they're just very simple operations like swap two elements or like delete an element or add parentheses around two elements that come afterwards or something like that um and to implement that molecularly uh you only need some kind of enzyme uh that detects the motif encodes for that combinator first um like let's say i don't know we have three primitive combinators and that's a key point there's only you only need a handful of primitives so let's say you have three primitive commoners so you have three different codes for these different combinators so some enzyme should detect that hey we have a we have a motif here that encodes for one of these combinators and that enzyme also carries with it the rule for the application rule of that combinator but i want to say yeah we haven't figured out how this could be implemented i mean so these are uh these are completely hypothetical yes yes completely hypothetical but i would do want to stress that um that the point of this this model is that it doesn't require extraordinarily complex molecules and very different from what we already know to exist exist in cells so our arneast strands are frequently modified after their uh transcribed um so the the most uh commonly discussed modification is splicing like uh that there's there's something called the splice zone that goes in and uh select segments within this arneast strands and excises them and then attaches the two loose ends um and and that's a that's a way to delete certain parts within an arneast strand um so like these kinds of operations exist and we know that they're within the reach of evolution of of molecular biology um so and the point here is that i can imagine a system that's very simple uh that doesn't require like huge molecules that do complex operations uh and a system like this could have gone undetected over the many decades of molecular biology um uh but yeah i mean going going back to how it would work it it's it's basically um um every enzyme would execute cleavage and ligation operations so it would cleave a certain part of the strand and ligate the different part of the strand cut it and put it back together exactly cut it and put it back together um and the locations of the cuts and the and the connections are are fixed relative to the motif because by virtue of the secondary structure bringing the parentheses together now you can say okay i only need to cut it this position and connect these two parts without caring about how what's inside the parenthesis like i don't i don't care about like how many layers of nested parenthesis are inside this enzyme just goes in and mindlessly does this single operation um and so through through the system this hypothetical system to emphasize it's just the theory it's obviously going to be wrong it's details i mean it's cool it's so cool but you know it's well yeah um but but like it's like i want to acknowledge that it's obviously wrong it's going to be wrong in its details i had to come up with uh details that allow the system to work that i that i know that i just came up with um but the the point is that this is a proof of principle that you can imagine something like this happening at the molecular level implementing uh a computation system and at the same time you have like all this RNA and all this DNA that we don't know what what it's doing like we haven't attributed a function to most of the genome at least in humans um and and so it's just very i don't know it's a very um in tree game compelling compelling yes in tree yes it's a very compelling problem yeah oh it's compelling problem but it's a compelling hypothetical solution yes yeah well i i would call it i mean this it's compelling me at least towards a research direction towards towards you know at the end of the day uh i want to study things empirically i don't want to like you know i don't expect anyone myself included to believe this theory until you know we find evidence for you know we have to do do the science and so uh it's it's it's a it's a it's a research direction that that is it's it's that i'm arguing for not not like a specific model and the direction being let's figure out if RNAs are being edited in ways that can implement computation so you were disillusioned first of all you were disillusioned and now you're hopeful would you would you car yourself hopeful how would you describe your outlook yes i i i would describe it as passionate i would say i'm very passionate about this problem sure i feel like it's extremely exciting i think i think um it's just i mean sometimes i forget and then i i remind myself of all of the hints that are obviously pointing at RNA um and it's it's also um it's it's heartwarming i guess i for lack of a better word to know that there are also many other reputable scientists uh that take these ideas seriously and and that that circle is growing and i hope it grows i hope it doesn't end up being something like um that subfield in the 1960s and it really depends on us it really depends on us trying to make this argument that that this is a worthwhile research direction i don't want everyone to be working on i don't want you to like pour like 50 percent of all you know a research budget towards it but at least in in the in the spirit of diverse approaches i feel like um we should be able to maintain a you know a consistent research direction uh along the lines of molecular computation and memory using a little drip out of the firehose of funding um basically yeah so so those scientists those reputable scientists that you mentioned often get labeled kind of like often don't feel fully respected right in the field and often get labeled i don't know what's crazy but you know out of the mainstream out of the dogma yeah do you ever feel do you ever think am i crazy i don't know actually i i i like um i might regret saying this but like sometimes i i think do people look at me the same way that i look at roger penrose about like yes micro two bills and like that's more steward hammer off with them but penrose is on board with that but yeah yeah yeah or or or the way i look at that right so i look at the my two-beel thing and i'm like exactly yeah yeah um i don't know i mean do you feel crazy not not about how you think other people view you do you ever think like oh this uh am i crazy i don't think so i think i think like i think my the the um the arguments and the evidence that i'm uh resting on are very rigorous uh and and again i just i want to be clear like it's it's the threshold of evidence you need to believe a theory is much higher than the threshold of evidence you need to work on pursuing a theory and i think uh you know um i think it meets the ladder threshold so i've i've had lots of conversations with people right right now Alex Gomez Marine comes to mind um because he's studying things that are sort of outside the norms of what the scientific community especially in neuroscience would would consider okay to study right or get funding for something um and and you mentioned these people in the 1960s right this is not a new idea as your abuser he says there's no new ideas under the sun these are all like recycled things right um he says it in a different way and how brains um how brains do computations which he didn't mention RNA for that specific size of point but so so you mentioned these people from the 60s the idea of RNA kind of came up and then it died down but so what i want to ask you is like how do have you reflected on how you feel what this tells you about how science progresses because most people like stay they get into let's say experimental nervous right and drosophila and then that's their career is just studying this sort of space of problems fairly narrow but now you've now you have you've done that and you've learned about an alternative framework for univore univore mutation which is what one of your interests but and then you realized like well this was not new this kind of edden flowed already um how does this make you feel about like the history and progress of science in general um it it makes me realize that we're in a fragile place it makes me realize that it it may very well be that people look back at this era as um some sometime where some idea just kind of re-emerged um and it died out again now that's fine if this idea is wrong but but if but if you know a hundred years down the road they realized uh this was all correct but it like it kept on resurfacing and dying out um you know that that's a that's a that's a possibility um i i kind of want to prevent it i i i think i think there's no there's no like um there's no in a inevitability when it comes to these sociologically decided things and that's why i'm trying to um uh get people to if they don't want to work on it at least agree that it's worth putting some resources into um i just want to say that also the the the the the the era in the 1960s the ideas that were there were mostly around um a memory were mostly around molecules encoding memory um and i think i have like i think this is not the same thing really it's about computation uh and it's about actually bringing in the insights from the theory of computation um to these strings of symbols uh and it it's obviously related to memory you know um memory comes up in computer science all the time uh but it's a different it's a it's a different kind of perspective and also just all the all of the conceptual arguments um are much more um mature now like if you if you like you know read gals those work the arguments are much more solid and convincing than anything that anyone wrote uh in the 1960s and also just we just have better tools it's a different era it's a very different era and it's a different idea uh but it's very related all right sorry i have to ask this like so we've talked a lot about like molecules and biology and you have a computer science computer engineering background and then you got into experimental neuroscience and a lot of people who start off in neuroscience have this computational bent right but then a lot of people who are in cell biology for example don't have that that's where like the stamp collecting began right and bio and speciation things like that but as this made you how sorry i'm baiting you by the i'm trying not to bias your how has this altered your appreciation or lack of appreciation for the the micro molecular world right relative to like your kind of computational mindset and just leading you into the answer sorry but i but i i wanted your reflections on it yeah i guess i mean um i kind of maybe wish in retrospect that i had i had studied i had studied um uh you know molecular biology just because that seems like a very relevant field now for the problem i'm trying to i'm trying to work on yeah um and and just to say like when if um it's not that it's not that molecular biologists or geneticists are completely foreign to these ideas or find these idea uh computational ideas foreign like i mean um john maddick for example explicitly argues that um these non-coding RNAs might be might be implementing a digital computation device um and and they're they're people who you know are definitely like on on the on this like uh this these ideas resonate with with their ideas um i guess it's just it's just um right now neither in uh not in neuroscience not in molecular biology is anyone really trying to uh take universal computation seriously yeah you having fun i don't know if that was the that was the correct uh again uh maybe i misunderstood your question no well my question so i've i've come to appreciate i've had come to have a little more awe just how goddamn complex i right and and that like the world of the cell that's a whole world you know it's the brain brain is the most complex thing in the universe although terri synalsky pointed out to me that his wife said to him well actually it's uh two brains is more complex than one brain so two people talking right which is true yeah right but yeah but but the the stories of you know the story that you're working on if it turns out to have a validity in one form or another i mean just the the capacity the the astounding results of evolution that continue on right how oh it's just amazing to me and and so that wet biology part so when i got under neuroscience this all computation spikes that's how they're doing it information blah blah blah but then you look in the cell and's like man that is messy and hot and but it's doing just as awesome a job whatever the job that has you know i mean it's just amazing that anything works in bio mm-hmm well yeah i mean it's it's it's amazing until you understand how it works exactly like this is that what you're saying well well well then you've explained it like like until you explain it it's a mystery it's like how how the hell is this single cell creating a human fetus with all the intricate um you know uh body parts and like all it's it's it's a it's a it looks like a miracle um now if somebody writes a program that draws something on the screen i wouldn't call that a miracle because i know how programs work draw something i mean like some complex pattern that's like you know looks really cool and as as complex as a human fetus or something i don't know maybe that was a bad example well no no but well this gets back uh we're out of time another time another time perhaps okay yeah sure because i've taken you over i've gone over that's some good to see you again thanks for doing this uh it looks like you're having fun you haven't fun yes yes this was extremely fun no no not this kind of thing i'm just saying this i mean oh i mean in in your in in these research questions in in life yeah yeah hey am i fun am i fun that's yeah no i mean you are definitely fun Paul yeah thanks but i mean you know it seems like you're having fun that that's a great place to be it uh yes as long as i i know that i can survive in academia is it it would be the um you know uh the condition that i that i would add to that yeah there's there's uh yeah okay i wish there's that that question too yeah i wish you uh survival i will if we are on a boat again and you go overboard i will throw you what's the a life vest or something yes thank you i appreciate that i appreciate that okay thanks has him thank you so much Paul yeah thank you for your time this was this was really fun take care brain inspired is powered by the transmitter an online publication that aims to deliver useful information insights and tools to build bridges across neuroscience and advanced research visit the transmitter dot org to explore the latest neuroscience news and perspectives written by journalists and scientists if you value brain inspired support it through patreon to access full length episodes join our discord community and even influence why invite to the podcast go to brain inspired dot co to learn more the music you're hearing is little wing performed by Kyle thank you for your support see you next time you
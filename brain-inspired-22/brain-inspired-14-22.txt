 This actually gives me an occasion to mention one thing that always bothers me about trying to relate DBRL to biological systems, which is that there's no natural way of taking evolutionary learning into account. For me, the AI work gives us a language to talk about maybe particular axes of activity said across the dopamine neurons in the fly, but the question is how many axes are there? I suppose the fact that the training process is not completely biologically realistic might lead us to this rate, but on the other hand, I think having a model that works and encompasses many of the things we are looking for and expect is much better than alternative. Somehow across evolution, we have met to learn some internal homeocetic landscape, which is not representing the world, but it's representing some internal states that corresponds to the scarcity or presence of various dimensions of rewards in the world. Hopefully, by knowing those representations, I think we'll be in the position to come back as neuroscientists to AI and give them some output of what we know in terms of representation in the brain and maybe give something back to AI. This is Brain Inspired. Hi, I'm Paul Littlebrook's, welcome to Brain Inspired. In this episode, you'll hear the panel discussion that I moderated recently at the virtual dopamine conference. You heard Ali Mohebi and then Engelhard and we introduced dopamine, I suppose, and got their reflections on this panel that you're about to hear and had a pretty broad conversation otherwise about related topics. In this panel that I moderated, we have Matt Botfynik, Ida Momenesjad, Ashok Litwen Kumar, Ilana Witten and Armin Lack. It was co-moderated with me by Tim Kraus. We'll hear more about them in a moment. So the title of the discussion, the title of the panel, is What Can Artificial Intelligence Teach Us About How The Brain Uses Dopamine To Learn? And I'm just going to read you the abstract because it short and summarizes everything. Recent advances in artificial intelligence have yielded novel algorithms for reinforcement learning, which leveraged the power of deep learning together with reward prediction error signals. In order to achieve unprecedented performance in complex tasks, in the brain, reward prediction error signals are thought to be signaled by mid-brain dopamine neurons and support learning. Can these new advances in deep reinforcement learning help us understand the role that dopamine plays in learning? In this panel, experts in both theoretical and experimental dopamine research will discuss this question. So there you have it. As a reminder, I have an email list for the podcast. What I do in those emails is often before an episode, I will send out a relevant talk or abstract or some relevant information for the upcoming episode. That's what I did here. I sent out Matt Botmanick's talk that he gave that just preceded the discussion you're about to hear. If that's something that would be of interest to you, you can go to the website and sign up for the email list at braininspired.co, where you can also find information about the panelists that you're about to hear. So enjoy. I will just introduce our panel of speakers here before we kind of dive into the questions and discussion. In no particular order, we have Ashok Litwin Kumar from Columbia University, Aida Momanejad, who is currently at Microsoft, Armin Lack, at Oxford, and Ilana Witten, who runs her lab at Princeton. There's already multiple questions in the Q&A, and I guess I'll just remind everyone that if you want to submit a question, use the Q&A box down below. I suppose just to get started here, just from the perspective of your own work, do you see the deep reinforcement learning models as providing a way into how we currently think that that brain's actually learned, given the various species and computational models that you work on? And maybe I'll just point to someone, Armin, maybe I could start with you. Well, thank you so much. So I think in order to get to this question, you probably need to start thinking about what you're currently trying to do in the experimental side of systems neuroscience and understanding the dopamine. And to me, there are two things that different groups are trying to push forward in particular in case of dopamine. One of them is the idea of discovering the relation between new cognitive variables, which are not tested in the dopamine case, and the representation of them in the dopamine field. And I'm not sure that a lot of those examples of the kind of new variables that we were not discussing and how they can be tested and parameterized using RL. That's one avenue that I think in system neuroscience, we are trying to get to a design new experiment with new variables and trying to parameterize those in animal context. The second one is going to really cracking the circuit and trying to understand the exact connection between neurons, the input of those neurons, the neural and pseudo-pumming system, as well as the output. And on that front, I think a lot of the nice experimental work has been done. But when it comes to AI and it comes to reinforcement learning or deep reinforcement learning, it seems that we are still quite far from connecting the two together, like the bringing effectively mapping these computation onto the circuit. So if I want to summarize what I have been saying is that on the two sides that you're pushing in systems you're saying, some neuroscience cognitive variables and cracking the circuit, I think so far AI has been more useful on the side of those variables and parameterizing those variables. And it has been less useful on helping us to understand the the the circuit and the connection between neurons and how they work together to make those representations. I wonder if Ashok is the right person to bring in here because he works on partly on figuring out whether we can use circuitry and use connections to deduce function, which isn't just an infinitely difficult problem as you might. Yeah, I mean, I'm coming at this from mostly the Drusophila connectomics side and the kind of first statement one makes when looking at the connect those, everything connects to everything and there's a great deal of diversity and feedback among even all of these neurons that we think may be doing or mostly doing a particular function like learning to approach a boy. So I think for me the AI work gives us a language to talk about maybe particular axes of activity, say across the dopamine neurons in the fly, but question is how many axes are there, are there axes that are not encompassed by what we typically think about in AI and can we infer those from neuroscience data like the connectome or recordings? So I'm just pausing to see if anyone wants to jump in and so maybe just to make sure everyone gets to speak here in the beginning, Elana, do you want to add to the conversation? Yeah, sure. So I guess one thing that I would say is that the way I think the way you phrase the question is what does like the learning, like how does this model the learning process itself, these deep agents and I would say and I think Matt would probably agree that the training of the networks is pretty far from biologically realistic as currently stated, but as currently performed, but the, that doesn't mean it can't be a helpful model for the fully trained circuit, so provide constraints on hypotheses for neural data or formalized intuitions. So I think, I think at least that the, my impression is the current state of the AI training, the deep learning training, no one's taking the training of the network very seriously as a model of the biology, it's only the trained agent that's taken seriously. So I think that's an important distinction or clarification with the question and I do think that once you do have a trained agent, I mean I think Matt showed beautifully examples of how it can produce the state representations that can really clarify how an trained animal could potentially, a model for how a trained animal can do the task and provide a framework to either test formal hypotheses or reject them. And that leaves the other side, which I think thinking about more biologically realistic training protocols that might actually model the learning, the initial learning process itself, which and that also touched on seems like a big open area that seems really exciting and important for many biological questions that many of us are interested in. And I'd, I'm going to let you jump in here as well. Thanks. So I think it's fantastic that we have cellular representation here and we have Dorsophila and Roden sort of science. I work mostly in humans and I've previously worked with Matt. So I think something that may be important is the different types of architecture, especially for higher cognition. I think that deep RL methods are particularly helpful for comparing different architectures theorizing about them and comparing different theories in terms of their behavior and also in terms of the similarity of their internal representations with the kinds of similarity we would see in what we measure in humans and maybe monkeys or sometimes rodents in higher cognitive function. And I think that that's a particularly helpful part of deep RL. What I think might not be quite, and obviously this is a tradition that goes back to the 80s. So it's not necessarily new to deep RL, but I feel like it moved much further after a kind of a hiatus. But other than the higher cognitive function, I think that the reward maximization framework might have limitations because especially if one defines a singular reward. And the reason is that we have many different scales and dimensions of reward that are competing with one another and there needs to be some higher representation that kind of decides in different moments or manages their homeostasis with regard to long term and short-tongues. And I think that it could be a little sort of challenging to stick to just the reward maximization framework. So we might need a future kind of evolution of deep RL if we want to address these sorts of more complex long term, short term, multi-scale approaches. This might be a good time. Matt, you brought up the paper. Reward is enough or I don't remember the actual intriguing title is reward enough. Reward is everything. Reward is the end all. So I'm curious. You said you weren't advocating it. I mean, it is kind of an intriguing selling point. And I'm curious about everyone's take. I'm not sure who's read the paper and who's not. So I would, based on what I did just said, is reward enough or is that sort of a trivial question given that we can define reward as anything? And we have a lot of questions coming in. So after we discuss this a little bit, we'll have some questions from the audience as well. Can I kick that off? Please. Yeah. So before Dave and Rich and Dwayna and St. Timber put that paper on archive, there was actually a lot of internal debate at DeepMind about this argument that they were making. And one of my colleagues, Vlad Nee, who was the lead author on the Atari paper, actually, his rebuttal to the argument was basically two words, architecture matters. And so this gives me an excuse to circle back to the points that Armin was making about architecture. I think this circuit, like cracking the circuit, that's a place where it really is important that most deep RL research is not computational neuroscience. There are just aspects of kind of economics that are just not going to come up from AI work. But I think on the other hand, there are a lot of insights that we gain from DeepRL about how things play out assuming different architectural structures. I mean, convolutional neural networks are a great example of that. Of course, they were inspired by neuroscience and they're heavily used in analyses of the biological nervous system. Coming back to the question of whether reward is enough, this is also an excuse for me to emphasize the point I made at the end of my spiel, which is that deep RL does not necessarily mean end to end learning. So a lot of deep RL systems now use self-supervised representation learning. So just trying to predict what's going to come next. That's not a reward-driven learning process. But it yields representations that are useful for reward maximization. So this connects with the comment that Ida was just making. And it also is relevant to what Alana was saying about training regimes. If you have a system that is trying to predict what comes next, that's a very different form of that's a very different way of using than learning from RL alone. And the results that can give you much greater sample efficiency, for example. One complaint that neuroscientists often have about DeepRL is that it's highly sample inefficient. It doesn't have to be if you configure things correctly. So maybe we'll jump in with a couple audience questions right now. This is really interesting discussion. And we have one question that's asking, do you think that AI can lead neuroscientists in the wrong direction to match the model instead? So misleading how the brain might compute by trying to match the models you're using? I'm happy to feel that one if I just start a certain resident. Maybe start a soft net. Short answer? Yes. I mean, at the minute you adopt a DeepRL model as in neuroscience work, you're treating it as a hypothesis. And hypotheses are almost always wrong. Whether you can be misled, that to me seems just like a question of good experimental design. There's no fundamental difference between a DeepRL model as a computationally implemented hypothesis and any other hypothesis in neuroscience. So I think you just need to be careful to design good experiments. But I think maybe what the question is getting at is this airplane versus bird thing. There are a lot of aspects of DeepRL systems that are just wildly unbiologic. And can we be distracted by that? Absolutely. Yeah, definitely. Ilana, maybe you have something perspective on this because given your comment about the differences in training and needing and the environment and the way it's structured, the differences between training a reinforcement learning algorithm, you know, agent. So of course we have, you know, very quote unquote, realistic mazes and games. But that is different than the structure of the world. And I don't know if you have thoughts about how that might affect the learning process and the training process, how it could lead us astray in that sense. That's a good question. I mean, I actually, I suppose the fact that the training process has is not completely biologically realistic might lead us astray. But on the other hand, I think having a model that works and simulates many of the encompasses many of the things we are looking for and expect is much better than alternative, which is not having a model. So I wouldn't, I wouldn't, I guess I wouldn't use the term lead us astray. I still think I think would be interesting. And I definitely think the self supervised learning aspect that map brought up might be a really key and important part of it. But I think it would be very interesting to have like, you know, initial training, be as biologically realistic as possible to model that part as well and see if that makes different predictions about the end function of the network. That seems like an interesting and important research direction. But I, I wouldn't have gone so far to say that like having the initial training being not particularly biologically realistic means that it's not a useful model at the end. But it's an, I mean, of course, that could be the case. That sounds like an important research direction. Okay. I think I was also, I haven't read the reward as enough paper myself. So I just was curious what that paper was really saying. Were they really arguing against the importance of self supervised learning and sensory sensory prediction or what were they actually saying? Because that seems like a very strong argument to make that there's no like learning about the structure of the world without reward because even if ultimately the goal is to get reward, obviously it seems very useful to know about the structure of the world and learn it. So what were they actually saying? Defend your colleagues, sir. I haven't read the paper, but I've talked to the authors an awful lot. So I think the argument is that optimizing a reward on a reward function through experience with the world is guaranteed to be enough to give you the representations that you need in order to maximize reward. But it doesn't guarantee and which is something that's neglected in their argument, at least as they made it verbally inside D-Mine is, is sample efficiency, right? Yes, reward, like RL is enough to get any intelligent behavior given infinite experience, infinite data. But it doesn't explain how humans and other animals, for example, are able to learn in such a sample-efficient way. Of course, that's an argument. Like that's a point that cognitive scientists like Josh Tenombom have been making for years with great legitimacy. And that's just not something that comes up in that paper. There are ways of getting there. The nature neuroscience paper that I briefly summarized in my spiel is really about meta-learning. And if you look at our paper, it's all about how pre-training on a wide variety of tasks can lead to a system that can then learn in a very sample-efficient way. But this actually gives me an occasion to mention one thing that always bothers me about trying to relate deep RL to biological systems, which is that there's no natural way of taking evolutionary learning into account. So when the Laundit has experiments with animals, not only did they have the benefit of whatever experience they've had in their lifetime, but their brains were designed by eons of evolutionary learning, which consumes data. It's not sample-efficient, but it wires in certain architectural structures into the brain. And I don't know any way to capture that side of the story in deep RL. There are people who play with evolutionary algorithms that design architectures and so forth, but it's very hard to get the analogy right. I just want to echo that I think the conflation of evolutionary development and kind of within lifetime learning is one of the most difficult things when we're trying to make analogies between these agents and biological systems. Yeah. Aida, you have a raised hand there. Just wanted to maybe ask Matt this or anyone else. Matt, do you think it would be fair to think that there are different cultures of reinforcement learning that kind of correspond to the history of cognitive science where some are closer to behaviorism, some are more akin to the cognitive turn after told men, which I guess you and me would be a part of it. And some might be even closer to inactivism and talking about empowerment and interaction, sort of learning, which is much more used in robotics where there is a body that needs to interact with an environment. And these are the three sort of cultures that sort of progress through cognitive science. And it seems to me that perhaps reward is not enough, is akin to the behaviorist culture of RL. I wonder what you think of that. Yeah, I mean, it's just like any other discipline, any other human culture. There are definitely different subcultures. And in particular, there have been people who've been very focused on questions of sample efficiency and other people who just don't care, who figure we'll get the data we need. We're not, you know, I mean, it's important to emphasize most AI researchers are not trying to model human behavior or brain functions. So, you know, the fact that AlphaGo takes a huge number of games to learn what it learns relative to what a human consumes is just not, it doesn't matter to some of the people who do this kind of research. It's not the problem that they're addressing. Whereas for other researchers, often ones who come from a cognitive science background, yes, that difference is irritating, not because DPRL is necessarily being applied as a model of human learning, but just because we have a notion of what intelligence is and part of that is learning in a sample efficient way. Like that's just what intelligence, that's how intelligence manifests. Another important cultural division is between people who are very focused on single task learning. Like I just want to get as good at chess as I can versus people who are more interested in multitask learning, you know, I want to learn six Atari games and then I want the seventh to be acquired more rapidly than it would otherwise be acquired. They just people have different, they set up, they define the problem in different ways, which leads to different emphases absolutely. This might be a good time to ask a couple listener questions as well. I mean, I just care about what you're just talking about. So, the top of the question right now is in AI, you have known problems like reward hacking. Do you think those are unique AI problems or is there a biological parallel? Any animal researcher has seen reward hacking in the lab? Well, given, okay. So, so the answer is yes, I suppose. I wonder if that'll be satisfying to I should let other people talk. If no one has anything to say, there's another, there's an actual dopamine question and this is a dopamine conference. So, maybe we can move on while people think about that question more. Dopamine, what might be the co-release of GABA or glutamate with dopamine both at the levels of axon and vesicles imply for deep reinforcement learning? This is kind of a specific question, but it also speaks to the larger issue of the relative narrowness and specificity that computational approaches can take. So, I'll just, does anyone have a response to that co-release question? I actually do inspired by Matt's paper, his 2018 Huang et al. The Huang et al paper. The Meta reinforcement learning paper, right? Yeah, the one that you described, which is that, I mean, one argument that you made was that maybe dopamine is after, and like, maybe initial, it's involved in initial learning to train the weights, but then after initial learning, dopamine might be just an input to the system that just like a reward that the network uses as a reward input. And to me, that makes much more sense as a potential function for the glutamate co-release. I don't know if you specifically said that in your, I don't think he's, so then your paper, that's sort of how I entered it. Oh, we didn't say that. That's a great idea. Yeah, but it makes more sense to me because like, you know, dopamine does modulate pluses. It just doesn't feel right for just like an input that like drives the neurons, but there is this co-release, so it could be like a kind of computational function of that co-release. So, that's how I read that paper. I never thought of that. That's fascinating. Thank you. There's also the question of whether we have dopamine solved, right? So, a couple of the panelists here, Armin and Ilana specifically, I know, have talked about all the different co-factors outside of just reward prediction error that dopamine is involved in. Armin, maybe you can talk about this. Do you feel like that we have a good grasp on what dopamine is doing within the reinforcement learning approach, or do we really need to expand include a thousand different computational factors as well? So, and yeah, that's a really good question. And it's very hard to answer. I think at some level, we have some good grasp of what the function, but one of the core function is, but that of course doesn't exclude other functions. And I think we are in the beginning of, of even exploring those other functions. And the tools that we have now would actually allow us to do so. I think it's only a matter of years that it's only some years that you're actually measuring dopamine with the specificity that we that we like. And that's really allowing us to now think about the variables beyond reward prediction error, which has been hard to really add to examine because most of these type of variables might turn out to be subtle in terms of the behavioral effect that they have, and therefore the neural representation that they have. And which then this subtle nature of this type of variable beyond reward prediction error could have been made very hard to actually discover in the limited type of recording that we have been doing previously in one neuron or in direct measurement of dopamine. I think with the tools that we have, and we are actually in the position to explore and go beyond what is known in the dopamine system. And some of these questions are, which are at the moment, a lot of group line addressing are really going towards the idea of characterizing the responses in relation to very basic variables, like the action, or the motivation, or the representation of a stimulus. So kind of very much external input or a very kind of visible, observable type of behavior variable. And some of the others are really internal variables. There are those that we can argue, argue only recover using models using RL models or similar models. And I think in that front, we are really in the beginning of understanding not only the dopamine case, but also in the rest of the brain of how those representations are happening. And hopefully, by knowing those representations, I think we will be in the position to come back from as neuroscientists to AI and give them some output of what we know in terms of representation in the brain. And maybe give something back to AI. I want to jump in here with another question that is somewhat related. We're going to summarize this longer one where it's asking essentially if you're starting with your models with the assumption that dopamine is encoding RPE. But we also know that dopamine is involved in things like vigor, motivation, responsibility, etc. How can we then test in our model that dopamine is just coding RPE? And also when we know that the RPE itself is estimated from, it really depends on the value function you're using or your reward function. So is that assumption really hurting us? Should we go about this a different way? I can kick things off if that's all right. I think deep RL models have exactly the same character as classical RL models that, of course, have been used widely in psychology and neuroscience. If you're modeling the function of dopamine strictly in classical RPE terms, but the dopamine neurons that you're looking at don't actually, they're not actually doing that, then you're going to falsify your model. That's just the way it is even with a deep RL system. I mean, the distributional dopamine project that I described is a great example of that. The dopamine neurons are definitely doing stuff that a classical RL model cannot explain. There's just variance in the data that's not explained, but that is explained by quite a different kind of deep RL model. There's nothing, there are no new principles involved in computational modeling here. What does get complicated and the question flag this is that because there's representation learning going on, things can happen in the model that depend very much on the way that you assume the perceptual data is structured or what you assume about the reward functions. I'm thinking about Anne's talk yesterday showing that the dopamine system cares about whatever outcome was just instructed to a human participant as the goal. You have to know that humans, I don't know, are sensitive to social reward. They want to comply with instructions or something. That's just part of our reward function as Anne's results show. If you're not assuming the right reward function, again, you're going to get a model that doesn't explain your data. Fortunately, it's a model and you can disconfirm it and you'll know you're wrong. That's so it's normal science in that regard. Ashok, you've done work in the mushroom body of Drosophila, the olfactory sensory processing area. I'm wondering, thinking about the distributional reinforcement learning that Matt talked about, there might just be this trend toward more and more detail. Matt talked about it in terms of the amount of prediction, the amount of optimism and pessimism, each different little circuit might have in the dopamine reward circuits. I'm wondering if we need to incorporate a thousand more details or how that maps on, how you think that a deep reinforcement learning model might need to be adjusted architecturally and or in any other way to map on to something species specific like Drosophila in the mushroom body as an example. In the mushroom body, there are other axes of heterogeneity across different compartments, each of which learns with its own particular dopamine signal. These include compartments that are involved in short-term memory versus long-term memory, about the novelty versus reward or versus learning. Even subtypes of reward should something else. I really like the distributional story because it's introducing but axis of variation here. But it flies, it seems like there are at least three or four others axes of variation. I need to think about systems that are each learning a slightly different thing than somehow combining each of these different predictions into a copier action. And we have a connectum which we are beginning to correlate with these behavioral and functional heterogeneity. So I think there's a lot of work to be done. Does structure matter? So in an AI agent, structure is less important and you just kind of throw the world at a big model right and it can learn those associations. But in something like the mushroom body, the architecture is very specific. Does it, do you need specific structure in an AI model to derive experimental predictions in something that is much more architecturally specific? And this is for everyone, not just Ashok, but... Yeah, I'll just start. I think one thing that's interesting about the mushroom body is at least that's in fact, your representation is one of our unstructured and the flybrain of past, this kind of random heterogeneous connectivity. At the same time, there are these separate systems for visual input or for thermosensory input and these reflect some kind of, you know, evolutionary optimization of exactly what kinds of information should be combined when in order to do what a fly should do. So I think the answer is, yeah, we do need to think about these architectural optimizations that we're going to map onto behavior in particular. Rodent experimental modelists, do you guys agree, Armin, Elana? Well, if I understand the current question, it also kind of relates to the point that was brought up about evolutionary learning versus... versus... Elteville, the learning versus task learning, which are, of course, very, very hard issues, which again makes me less excited about the whole initial learning question of the first place in the context of the least deep networks. But I mean, I think it's a really important point. And I do think adding more kind of anatomical constraints to models is clearly the direction the field should go in all of neuroscience, I think, as the connectomes and becomes available in more species that is going to be really exciting to understand what or the constraints it gives. But there's also just, I think, this bigger deep challenge about the different levels of learning and how we can seriously think about them is pretty challenging us as has already been discussed. So, there's a question here that I think Ida would call me the best to answer. It's for Ida's view that the reward maximization has limitation with the homeostatic RL. Would there be any RL model with an agent's internal set point or internal state dependent modulation of value rather than context from the environment? And along those lines, how could the multi-dimensionality of value map or internal drive like Thirst versus Hunger be computed in the AI agent? So, excellent questions, something I am actually working on. And I just want to also acknowledge paper, but two papers by Karamati and Goodkin, where they use the RL framework to address homeostatic learning. And they indeed just sort of have this idea of an internal landscape of your homeostatic state. And they assume a simple surface for it. But then some internal sense of where you are compared to that ideal point in your distance would translate into drive towards action. So, they have a wonderful theory there. However, it's still within the framework of RL. So, it hasn't completely moved towards using this idea of homeostatic learning as opposed to reinforcement learning to manage it. So, there the idea is to minimize the distance, for instance, between this internal actual state. I'm going back to the wonderful point that Ashok was mentioning with regards to different dimensions of reward. Here, they consider two, but it's generalizable to multiple, which they consider, for instance, sugar and temperature. And then they simulate some results from rodent data. It was first the nerds paper and then an E-life paper. And then there was another paper I recommend people to look at it. But there are other ways to consider how rewards evolve or how these internal landscape evolves over time, for instance. And there, for instance, we are using this idea of metal learning, applying it to homeostatic metal learning. So, that's one way to go forward. Somehow across evolution, we have metal learned some internal landscape, homeostatic landscape, which is not representing the world. But it's representing some internal states that corresponds to the scarcity or presence of various dimensions of rewards in the world. And so I think there's wonderful work to be done there. If I remember the entire question completely, I would love to hear also from from Matt and from others what they think about these competing dimensions of reward that I think to some extent Ashok mentioned. And in some situations, for instance, if I'm dying, if my sugar is in a particular state, I need to predict how long I have to replenish it without dying so that I can go to sleep. If I go to sleep, and I'm going to sleep for five hours, but then my sugar is going to get depleted, I'm going to die in four hours, I shouldn't go to sleep. So, even if I'm very sleepy, so there are many situations where animals need to decide with these kind of competing situations by predicting their own homeostasis within a particular time, given they take one or the other action. It seems super complicated. And I want to hear from everyone, what kind of architectures, what kind of circuits they think allows for this. And I guess in their softwa, it might be easier because you have all the connective and you know all the dimensions that it might be harder for mammals. But I do wonder what everyone thinks about how that is sort of managed. Obviously dopamine has been around way early in the evolution before animals, like yet way earlier than the species we are sort of analyzing. So I wonder also what would be the role of dopamine in this? Which seems to be maybe a little less dimension specific. It might be, I don't know what you guys think actually. There are different dopamine for different dimensions of reward. If there is a way that it kind of credit assigns to those dimensions or access, as Ashok was saying, to solve the problem, what is the computation that decides between them? How does it forecast into the future, my homie of status, state? I wonder what you all think. Let me kick it off just with a high level comment about deep RL. People approaching intelligence from an AI perspective tend to have a bias toward generality. Like there's this term AGI, artificial general intelligence. People want learning systems, including deep RL systems that have the least commitment to some particular problem domain and our general purpose in that way. It's tricky though because in machine learning there is this no free lunch principle, which says that, in a sense it says the opposite. It says intelligence is sort of proportional to the degree to which you commit to a particular range of environments. And of course, animals do commit. Fruit flies are good at being fruit flies. Their brains are designed to solve the problems that fruit flies face. There's nothing general about the need to keep your glucose level up when you have a certain metabolic rate. That's a very, very specific problem. A brain that's optimized to deal with that or to solve, say, foraging problems is not a general purpose intelligence. And this along with the very related evolutionary learning point that came up earlier is a major disconnect between between studying biological systems and the kind of approach that people take in AI. And this is I think even true of humans. Humans are like, we seem much more general purpose than say fruit flies, but we're good at dealing with problems that have a particular kind of generic structure, which is a kind of compositionality. And we're stuck with certain representational formats. If you permuted our rods and cones, we'd be blind. We really are brains work under a certain set of assumptions. And I think that's important to keep in view. And it's relevant to the kinds of issues you're raising, Ida. Anyone else want to jump in and comment? I think the question from Ida, very, very interesting. It has been very hard to actually translate those experiments, the homostatic experiment into type of recording that we do in the dopamine system, given the long time scale at that type of behavior. When we think about those homostatic situations, they are all developed over hours and days. Well, I think these are really, interesting questions to address. Now that we have the tools for addressing those questions, to move towards the kind of the internal state of the animal and bring that into the studies of learning and decision making. And the role of dopamine on this, I think it's a still a very open question. So some years ago, we did experiment in which we were changing the properties of reward from one type of the juice to the other type of the juice, from our magnitudes to the other magnitude or the probabilistic versus safe juice. And in the recordings that we were doing at dopamine at that time, we got to the conclusion that it doesn't really matter for dopamine, what exactly properties those rewards have, as long as animal is liking them, as long as you can see the reveal preference of the animal, then the dopamine neurons are showing up. But that doesn't really exclude the possibility of having subset of these neurons doing a specific computation for a specific reward and paying attention to the properties of those rewards considering the internal and homostatic state of the animal. Yeah, and also, well, at least my thinking, and I think consistent with what Armin just said is I was very struck by a paper that came up maybe in the last couple years from the dystrophy lab where they made the animal thirsty optogenetically or just, I guess, or stated them one or the other and they got like entire brain change representation. So I think the state representation is everywhere. So you don't need it in the dopamine system in any way, shape or form, not to say it's not there. It couldn't be there, but it has, there hasn't been super compelling evidence so far that it is there as far as I'm aware and certainly it's everywhere else. So those two pieces together, yeah, make me lean to that view that at least the type of reward is in dopamine neurons aren't super special as with the type of reward, but the state representation, internal state representation is part of the state representation everywhere. So it's very easy to do internal state dependent reinforcement learning with that like, you know, rest of the circuit for dopamine to work on. So Matt, in your talks, you often talk about how you guys have been working on deep reinforcement learning system and then you thought, huh, maybe the brain does this way. Where could it do it do it this way in the brain? There's a question from the audience that I'm going to kind of morph, I guess. The question is about whether a deep reinforcement learning system could come up with solutions that are just completely unrecognizable in brains that that could either change the knowledge, change the nature of the way that we think about how brains do things or we just may not have a touch point with. Or is it, are the deep reinforcement learning models constrained enough as they are since they're made of artificial neural networks and a reinforcement learning system that they will be within the space of possible solutions that they will be able to map any solution they come up with onto some algorithmic level or computational level and or implementation level in the brain. I mean, I think I think it's definitely possible for artificial neural networks to arrive at solutions that will be very unbrained like. But the problem is it's very hard to know when you're looking at that. I mean, there are some, the cases where the cases where things are very unbrained like are obvious in some regards because they have to do with the way that the data is handled by the system. So for example, many people in today's session will be familiar with recent language modeling breakthroughs in AI. And basically what what contemporary language models do is they process a text by just sort of ingesting all of the words in parallel. So they, what they basically do is they spatialize time. They don't really deal with the word by word. At least some of them don't deal with the one word at a time sort of causal causal sequence. And that's just obviously not brain like. So you know right from the setup that you're dealing with something that doesn't really address the way that the brain handles language. On the other hand, there are a lot of cases where something cool is going on in an artificial neural network and you're just not whether it's brain like or not. And those are the cases where I think these systems offer themselves as interesting like testable hypotheses. So this also happens in large language models. I'm sort of fascinated by these models because transformers have an architectural bias. They have a particular structure that encourages compositional computation and allows deep learning systems to generalize in ways that they didn't before. And I look at that as a cognitive scientist and I think, oh, we've been wanting neural networks to do compositional computation for decades. And now we have a way of doing it. That's amazing. Maybe that has something to do with the way the brain does computation. It does compositional computation because we know it does. But I don't know looking at these transformers whether the solution in those models is really the same in any, even in the abstract way as the solution that the brain is finding. And that's just that's where research comes in. Like you just have to ask and find out. I do want to ask as we are closing in on time a little bit, but there's and I don't know exactly how to ask this. I want to ask each of the panelists in your daily work as you're making models as you're thinking about the architectures and specific needs of the species that you're working with. Is there anything lingering in your minds that you think deep reinforcement learning really doesn't address this particular thing? And if so, do you have insight on what you would need from a deep reinforcement learning approach? How it would need to change to address that lingering thing in your mind? I guess deep reinforcement learning has solved it all perhaps. Reward is enough. I can start. So I think there's two things that I think at well three, I would say three things that I would like to see more in DL. I think that DPRL or I think that at least those are the directions that I'm working on. So maybe I'm biased. But one thing is something that Matt already mentioned and others already mentioned, which is architecture. And I do think comparing different architectures and that using that as a means either of comparing developing and comparing different architectures for cognition and biological agents or whether it is for solving different tasks. It's a big part of it. And it's not even in deep learning for AI purposes without a care for biological agents. Architecture matters a lot actually. Whether it's for sample efficiency, whether it's for what kind of problems are possible to solve, whether it's for can you remember how much can you remember given your memory and sample size of the events you will experience in the world are not a match. There is so much to be talked about about architecture and algorithm even in the absence of the relevance to biological creatures but even more so for us who are cognitive neuroscientists. The second thing I would say is actually something that we got accepted and it's going to be out at ICML which was like my first paper at Microsoft Research which we call interaction and grounded learning. In much of the situations biological agents do not move around the world where things have a reward tag. So and there is no programmer God who provides a reward function in the world. We really have to learn an interaction with the world and it's really noisy. And so we took a first step, a first theoretical step and a sort of a basic demonstration that we hope to follow up on for interaction grounded learning. And I think that doing more work on situations where rewards are not labeled or provided can the new generations of reinforcement learning can handle that. It's not an entirely different field to think about reinforcement learning without rewards. So to speak where the agent in interaction with the world needs to ground or define its own rewards somehow. And the third thing that I would say is again something that I learned from Matt which is another paper that we got at ICML which is about human-like navigation. If you compare state of the art deep learning algorithms that can solve navigation or pass a benchmark for navigation in an Xbox game that we used. Not all of them look human-like. So we had humans in a touring test look at the behavior of the avatar in the game when it was played by different architectures all of which passed the benchmark and not all of them looked human-like. And some looked more human-like and we needed to add more architectural components where it to even behave more human-like, let alone have a human-like representation just the next step. So I guess the third part that I would say is that in many applications of the RL which are going to interface with humans we want them to behave human-like even if people don't care about cognitive neuroscience. I mean I do care very deeply but even for those who only want the applications they should care about it's not just passing a benchmark but behaving the same way humans do that includes having errors or having trajectories that are more human-like. And then the next step is what for instance Matt was mentioning as well which is solving it in a sample efficient way with sample efficient representations like humans. So I would say these three things would be what I would hope deep reinforcement learning would have in the future. One thing that I will add that I think hasn't come up that I've wondered about is for is that as far as I know when these deep reinforcement learning networks are fit they aren't fit. I mean so they're not fit they aren't fit to data they're not fit to the neural activity or to the behavior they're just trained to do a task and that's different from like for supervised learning and for simple reinforcement learning you know those are actually fit to data and neural activity which makes the link many ways easier and more compelling so it does seem like thinking about ways to fill that hole seems like a major hole to really provide constrained models that's literally to constrain the models based on data rather than just like produce a model and see if it matches the data if that yeah I love that point Alana I just want to mention that there there is work fitting behavior so behavioral cloning is a major area in in AI and it's aimed at finding policies that generate some target behavior like let's say you have some ocean capture data and you want an agent that does that in fact the soccer the soccer video that I showed was tuned with RL you know scoring points being the reward function but the basic skills like past like kicking the ball and running were trained through behavioral cloning and you can apply this readily in neuroscience so I showed the little model rat running around that rat was trained with RL but you can also train a rat you can sit train that same artificial rat using motion capture from a real rat and then you have a controller like a recurrent neural network that you can say well okay what's going on inside this recurrent neural network does the unit activities resemble what we see say in striatum and in fact we're collaborating with Ben Seylvetsky on a project exactly of that kind while I have the floor I'll just respond to the original original question one thing that deep RL systems aren't good at that at least humans are is continual learning like learning in an open-ended way like collecting knowledge gradually and adding new knowledge to a growing store of knowledge without overriding earlier learning many people will be familiar with the with the problem of catastrophic interference it's like not a solve problem so that kind of the there's a term lifelong learning that some people are starting to use an AI more and more like that's becoming a goal and it's we don't know how to do it um I just I'll shut up in a second but I just want to say something radical which is the question was what do deep RL systems what can they not do that brains can do I would flip the script and say like what I'm really proposing is that brains are deep learning they are deep RL systems brains are deep RL systems they're natural deep RL systems uh and they work differently in some ways from artificial engineered deep RL systems um and so the question can be rephrased in those terms but I'd like to to make my own radical statement along those lines I suppose this is a natural ending with with Matt screaming brains are deep RL systems so I feel like we're just getting started but um this was fun so thanks to the panelists Armin, Ilana, Ashok, Aida and Matt Brain inspired is a production of me and you I don't do advertisements you can support the show through patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science go to braininspired.co and find the red patreon button there to get in touch with me email Paul at braininspired.co the music you hear is by the new year find them at the new year.net thank you for your support see you next time
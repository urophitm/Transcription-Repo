 Maybe this interdisciplinary process, whatever we're calling NRAI can help dislodge some of these stale debates and arguments and help us get to a new conceptualization of what's really going on in brains. There are some people that think neuro-AI is about using AI to understand neuroscience, and then there are people that think neuro-AI is about using insights and principles from neuroscience to improve AI. We're really more interested in the convergence and the reciprocal aspects of neuro-AI. You can like ignite it essentially, and then the connections are such that the nonlinearities all line up, and you get self-reinforcing, self-supporting, self-sustaining activity. That's the base, that's the base computational unit. And I turn out to be the only student that had the right answer. Is that because you used a for loop and just did it like the careful, slow way? Yes! This is Brain Inspired, powered by the transmitter. Well, well, if you're a long time listener, you may recognize that I added back in, Chopin. That is at the demand of one of my guests today, Joe Monaco. Joe berated me, you could say, for removing it a long time ago from the introduction. So he took it upon himself to play it, record it, and send me the recording. And so it's back. I hope you're happy, Joe. Joe and Grace Huang, the other voice you just heard, will introduce themselves in a moment. But I will introduce them now as co-organizers of a recent workshop that I participated in, the 2024 Brain NeuroAI Workshop. You may have heard of the Brain Initiative, but in case not, it is a huge funding effort across many agencies, one of which is the National Institutes of Health, or the NIH, where this recent workshop was held. The Brain Initiative began in 2013 under the Obama Administration with the goal to support developing technologies and implementing those technologies to help understand the human brain, so that we can cure brain-based diseases. So the Brain Initiative just became a little over a decade old now, with many successes under its belt, like the recent whole Brain Connect Dones you may have heard of, and discovering the vast array of cell types, and many others. I'm not going to list them here, but I'll point to a reference for you to learn more. So now the question is how to move forward. And one area they're curious about that perhaps has a lot of potential to support their mission. Is the recent convergence of neuroscience and AI, or what has been recently coined as neuroAI for better or worse, as we discuss. So the workshop was designed to explore how neuroAI might contribute moving forward, and to hear from neuroAI folks, people doing the neuroAI research, to hear how they envision the field moving forward. You'll hear more about that in a moment. That's one reason I invited Grayson Joe on. Another reason is because they co-wrote a position paper a while back, that is, among other things, it's an impressive synthesis of lots of concepts in the cognitive sciences and neurosciences, and history. So we talk about that, but it also proposes a specific level of abstraction and scale in brain processes that may serve as what Joe calls a base layer for computation. So the paper is called neurodynamical computing at the information boundaries of intelligent systems. All right, so you'll learn more about that in this episode as well. Okay, I don't want to yammer on here, so let's get you to Grayson Joe. There are lots of show notes in this one, to workshop related stuff, and to many of the papers that Joe and Gray's reference. Those are at braininspired.co slash podcast slash 200. Right, I forgot. It's the 200th episode. That is awesome and amazing. And what a fitting way to bring in 200, talking about a neuro AI workshop, among other things. Patreon supporters, you are the best. Let's have a live chat before the Christmas holidays, if you're up for it. So I'll be in touch about that. Go to braininspired.co to learn how to support the show on Patreon and join in on fun stuff like that and get full episodes all the time. All right, here are Grayson Joe. Yeah, we're starting. We're starting. Okay. You guys just blew my mind. So this is the beginning. I didn't, I've been interacting with both of you. I had no idea that you were a married couple because my interactions with you, you know, were very professional because I just came back from this neuro AI brain initiative workshop and then 30 seconds into us speaking to each other. Gray said you do know that we're one of you said you do know that we're a couple, right? And nope, those, no, I did not. But I do know. So first of all, hi Joe, hi, Grayson, thanks for being on the podcast. Thanks for having us. So in this is a little bit different than the way I normally do things, but could you just like very briefly state your name and occupation or not your name, but your occupation. Grace, we'll start with you. Grace Huang, I am a program director at the NIH at the National Institute of Neurological Disorders and Stroke and I support the Brain Initiative full time. And Joe. I am Joe Monaco. I am a scientific program manager and I am a contractor for the office of the Brain Director at the NIH. So we are housed under NINDS. And so I'm there with Grace, but I work with the Brain Director and I work with all of our internal brain teams. Okay. And I just worked with both of you among many other people who worked with both of you because you both put in an absurd amount of work to organize this recent Brain Initiative NeuroAI workshop. But now I don't know where to start because your partnership has gone back a long time. And Grace, I had recently learned that it was an interesting way that you guys met originally and came to form an intellectual partnership. So maybe we could just start there. But we met originally in 2004 at Brandeis University when we both were taking computational theoretical neuroscience class under Larry Abbott back when he was there. That's kind of a famous class. I feel like a lot of people matriculated through that class and have fun memories of it. Indeed. Joe and I started our first collaboration. This was when I had just gone into computational neuroscience and was learning how to use MATLAB for the very first time and how to do a homework problem. And I was the only student that had a different answer than everybody else because I had not figured out how to unroll my loops. So I was literally writing all these loops. And not the wrong with a for loop, although it's better to not have them if possible. Yeah. That's right. Well, that plays into the story. Because I was the only person who wrote a for loop, I had a different answer and I thought I had the wrong answer. Even the TA had a different answer than me. So I sat in the computational annex for days trying to find the bug in my code. And then at one or two in the morning before the homework was due, comes in a student very quickly writes his code and is about to leave with his correct answer. And I asked him, can you just look at my code? And he looked at it and he said, there is nothing wrong with your code. And an hour later, he found a bug in his code that every other student who are super programmers made. And I turned out to be the only student that had the right answer. Is that because you used a for loop and just did it like the careful slow way? Yes. Yes. Because all of the other computer nerds in the class, they know about vectorization and they do it. And Matt Lab is very slow when you write manual for loops. And so everyone got the same wrong answer. And they thought it was the right answer. But looking through the racist code, it's like she sequenced the order of operations absolutely correctly. And then I was able to figure out when you vectorize it, it actually changes the order. And we were doing classical conditioning. So the order in which you update the parameters of the synapses really mattered. And you guys were already married at that time, right? No. No, no, no. So you were just saying that you that was when we met. Yeah, yeah. We married. We've been married for 10 years. So it took us a long time to get married in 2014. We had to get our PhDs first. Backing up even further, Grace, I understand that the only reason you went into computational neuroscience was because you developed mouse allergies. Is that correct? Yes. Yeah. I initially went to Brandeis University to study biophysics and structural biology. I was building a single photon molecule microscope trying to study the translocation of HIV protein. And that required that I work with protein chemicals and rotate it through a mouse lab. And I had really bad allergies, whereas in hives and going and had an epipen. And it was not a sustainable lifestyle. So in my 30 year graduate school, I changed labs into a computational memory lab where I joined Michael Kahana back when he was at Brandeis University. I mean, the interesting thing about that is I've only physically been near Joe a couple times now, but I think I developed an allergy. No, I don't know how to take that. No, okay. So then how did you guys, because what you were just, what you just co-organized was a neuro AI workshop. So then how did you guys end up coming together in that state? Well, so I'm out of the two of us. I'm the one who was a neuroscientist from the beginning. He was a computational neuroscientist in a theorist. I did my PhD on grid cells and play cells and modeling how they might be related through remapping. It's very important computational transformation and hippocampal studies. But I kind of brought that through to expanding outwards to think more about the complexity of behavior throughout my postdocs. So I joined Jim Kinerim's lab at Johns Hopkins where he had a wealth of experimental data of freely moving, but on track rats, basically navigating in a clockwise circle a number of times. So you can very closely track the behavior. Where's the position of the head and body and then you can track the emergence of place-field activity over time. And so that was kind of the basis of like, beginning into complex neurobehavioral analyses. And thinking about from an organismal perspective, what's actually going on here? Do we have all these just internal computational representations? How does that translate into what this interesting little animal is doing on a moment-to-moment basis? And so that's kind of where I started thinking more deeply about complex temporal dynamics and behavior. And eventually, Grace was kind of off doing other things in other fields. She's kind of a public lot of science and technology. And then yeah. I basically, after I graduated with my PhD, I went to the MITRE Corporation and I was developing optical biosensors to detect pathogens from Excel breath. And I did some work in government at IARPA as a CEDA contractor and also at DARPA CEDA contractor. And it wasn't until 2015 when I went back into quote-unquote academia, I joined Johns Hopkins University of Plight Physics Lab as a program manager to run their applied neuroscience program, initially I was an assistant program manager. And that was the first time since 2005 that I started paying attention to neuroscience again. And it was so exciting because neuroscience had just accelerated. And so I started going to the side of our neuroscience meetings. I wasn't following Joe's research at all. And it wasn't until the 2017 Society for Neuroscience post-recession that I learned about Joe's discovery of phaser cells. This came out of his research with actually Ketchon Zangen, Todd Blair. And I was so intrigued by phaser cells because unlike hippocampal placels that maps phasem… They're mapped asymmetrically to the traversal of a place field. And so very famously in hippocampus, the Pramelomarounds there, the place cells will start firing at a later phase of the theta rhythm. And then as the animal moves through the place field, each spike will become earlier and earlier within the theta cycle across subsequent theta cycles. And so if you plot the distance through the field against the theta phase of spikes, you get this kind of nearly monotonic decrease in the phase, this advancement. So that's called phase procession. And that's a very robust field. It's thought to be related to sequence learning and very important things that like C-3 is doing in hippocampus. That's the highly recurrent sub-region. But with Todd Blair, I had collaboration where we were looking at the one synapse downstream into the sub-cortex. We were looking at lateral septum. And so the septal bodies are very interesting people don't record from them very often. But we were looking at what other phase codes might there be is a theta rhythmic brainer. And so we found, well, I went looking for a different kind of code, one that wasn't locked to a particular trajectory, but one that was locked to space. So I was looking purely for spatial information in the timing code relative to theta oscillations and found it. And so that's kind of, I coined the name phaser cells. There's a couple other models out there called phaser. And this blew me away because you were able to directly map phase to place. And I was working at the polyphysics labs in the intelligence system center surrounded by roboticists. So I immediately thought, well, if we could use this internal phase code to do self-localization and mapping, wouldn't that be cool? And on that very same day, there was this other paper that came out. It was titled Sormulators. And these are sormulators that can sink in swarm using an internal phase variable, which came out of Kevin O'Keefe and Stephen Stroghatslam. And so the two things got me thinking, wow, maybe there's a there there for using Jo's discovery of phaser cells and controlling multi-agent robotics. So that was how our collaboration reinitiated back in 2017. And then AI became pervasive. And so we kind of now just begrudgingly call it neuro AI because people tend to know what that means. There are some people that think neuro AI is about using AI to understand neuroscience. And then there are people that think neuro AI is about using insights and principles from neuroscience to improve AI. And AI could be hardware, software, or a combination of the two. So we kind of left it open ended because we didn't want to, you know, we told people for the purpose of the neuro AI workshop, we're really more interested in the convergence in the reciprocal aspects of neuro AI and not in the feed forward using AI for any science approach. Oh, I wouldn't say be grudgingly. I would say this is where the opportunity is, right? So you know, in this interview, we both have to be clear about when we're speaking from the perspective of the brain initiative when we're speaking from our own scientific perspective. From the perspective of the brain initiative, there's a lot of interesting opportunity here. And so, you know, in my opening remarks, I showed a figure from Brad Heiming's review paper of like the four or five main threads of this kind of how AI has evolved from different ways of computing through learning with data. And the question is, how does all this come together? And what do more brain like forms of that of this type of computing by learning with data? What does that look like going forward? And there's been a few major inspirations from neuroscience from the brain over the decades since good old fashioned AI in the 50s where you had these symbolic approaches, new and Simon coming out of cognitive science. And then, you know, we're all kind of familiar with the history of the back and forth there, the AI winters, connectionism, rows in the 80s, you know, with the advent of neural networks and back propagation for updating weights. And, you know, it's only in the last 15 years to, you know, the scalability come into play with a hardware that enabled, you know, the amazing advances that we've seen in the last 15 years in what we now call, you know, AI computing or AI technology. And so the convergence right here is really right. And I think we should not be, you know, arguing about definitions necessarily because, you know, people in cognitive science and neuroscience and artificial intelligence can are very good at arguing about terminology and definitions. We can bring in the consciousness researchers if you really want to go to leave the philosophers out. Because they're the best at arguing about semantics, right? Right. Well, we do need people worrying about this, but I think it's at this early stage in this kind of like this exciting conversion period, there's a lot of decades of thought and research going into all these different threads and they've all hit kind of fairly related roadblocks, it seems. You know, cognitive science didn't become that fully encompassing, you know, research program that started in the 60s and 70s, that Miller kind of anticipated, you know, neuroscience has kind of gotten wrapped around certain ideas, attractor dynamics and, you know, population geometry. And we're trying to figure out if this is the right way to go or not, how to incorporate large scale data. But maybe coming together, we can solve all these problems simultaneously. I guess I want to defend my personal position a little bit about being, uh, begrudgingly with word AI because this is, this would be the second time that the word neuro AI was put in a program or in a, in a work, in a concept that I'm working on. When I was at the National Science Foundation, uh, back from 2020 to 2023, I created a program, a topic that was part of the emerging frontiers and research and innovation program called Brain Inspired Dynamics for Engineering Energy Efficient Circuits and AI. The original name of that program was not, did not have AI in it, but because AI was so hot, my former NSF leadership said, AI's got to be in the title. And so that was kind of where, where I was coming from, Joe, is, you know, we have to use the word AI because it's gotten so involved these days. Well, I think it's useful to use the words that people are using. Right. And, and it's a, it's a new term. I think it's not fully well defined and that's kind of what this workshop was about. Let's bring together, um, so Grace and I have been going to neuro AI conferences and workshops, a lot of them over the last year as I know you have as well, Paul. And, and this kind of, you see a lot of themes emerging. And so, brain is interested. Well, let's explore what potential roles look like. Is there a piece of this where brain can help where it, you know, it fits within Brain submission to go forward? And so we want broad input from the community, um, for the workshop, we wanted broad input from the community. And I think we got it about, you know, helping us identify what those opportunities are, um, and to be considered further. Yeah, that what I wonder is, so I hear when you have a new hot term like neuro AI, it makes me think of, you know, if, if I'm not sure what hats you want to put on, if you respond to this, but, um, speaking for scientifically or, or, you know, from NIH, but, you know, you have to put the word mechanistic in all of your papers now because that's, you know, mechanisms became the hot thing and computational neuroscience is dominated by mechanisms. And I've heard that what the sentiment I'm about to express expressed among grad students just in the past few weeks, you know, of like everything now is neuro AI. And if you want funding for anything, you just call it neuro AI. Um, and, and therefore you don't really have to worry about how what you're doing fits into neuro AI. And if, if it's ill defined or not defined at all, then, then, so here's the worry is that like, all right, so then there's going to be this surge of grants and everyone's going to use the hot term neuro AI. And that's going to bolster their chances of getting funded. And I'm not sure if there's a solution to that because that's just the name of the game, but I don't know if you have a response to that. It's always something like that. You can't control, you know, the community, you can't control people. Essentially, you have to communicate in a way where, um, you know, people say need to be clear about what they're doing when they're applying for funding. And, and we have, you know, obviously the NIH and NSF and other funding agencies have, have reviewed scientific review processes in place to kind of discern, um, whose, maybe following hype and over, over claiming or overusing words, um, versus where the real advances are. And that's obviously not a perfect system. Right. Well, of course, of course. And I'll say, for my experience when I was at the NSF, for the braid topic, we had very strict Christmas selections, solicitation specific criteria that would filter out people who are just using the buzzwords to try to get in. And, and I think, um, having strong review sections would, would avoid these kinds of problems. I, I do want to say that there is a true inflection point here in that the technologies that's been enabled by the Brain Initiative is allowing us to observe, uh, circuits in animals across many different spatial and temporal scale. And so there is a real opportunity. And I think just putting the word neuro AI on any project is going to be easy to filter out. Perhaps so. Yeah. I mean, I, I think that that little white bulb went off in my head when we, when Grace used the term begrudgingly because I can, that, that sense that I get from, you know, these handful of people who have, it's sort of an eye roll like, oh, that must be neuro AI, you know, like, uh, because, because that's just a hot term or something. So I'm wondering, man, is this, I already feel like the backlash of this emerging field. So I originally wanted to call this workshop the Brain, Neuro AI and theory workshop because I wanted to bring Neuro AI, I wanted that convergence point to be focused on advances in theory, theoretical frameworks and theory driven models. Um, cause I think there's so much there. And I think that's where a lot of the obstacles in all, so scientific opinion of my, uh, personally speaking, um, that's where I think a lot of the obstacles have been. I mean, there are a few unifying theoretical frameworks of neuroscience and then you've, you know, discussed them on this podcast over the years with a lot of people who have, you know, have been, uh, have made major contributions to those theories. Um, but I don't see, you know, I don't see that CUNY and kind of process of, of the field, you know, testing confirmatory hypotheses, falsifying, uh, different theories and, and, and making progress. And so I think maybe this interdisciplinary process, whatever we're calling Neuro AI can, can help like dislodge some of the, these, these stale debates and arguments and help us get to a new conceptualization of what's really going on in brains, um, and which fun, by the way, are fundamentally embodied and inherently integrated as biological systems, which is different from AI. Yeah. And I also think being a little loose with our definition is okay because, you know, we were able to bring in the Neuro-Morphic community. Yeah. It was a huge, I mean, maybe let's talk about just the, the reason that the workshop existed and just how you managed to, how you decided how to frame it, how to organize it. And because the Neuro-Morphics was a larger part of the workshop than I had anticipated it might be if one just said we're going to have a Neuro AI workshop and that's all I had to go on. So for me, you know, coming from the NSF to the NIH, I was shocked that there was very few investment in Neuro-Morphics at the NIH. If you go into the NIH reporter, you took, typing the word Neuro-Morphic, you'll probably get 60 returns and under 20 million investments since the 19 late 80s whenever NIH reporter was, uh, started compiling information. Is that, is that because Neuro-Morphic is inherently slow? Is that the reason? It has been as slow as we heard from the last two days, but I think the other part of it is just that the Neuro-Morphic engineers and the Neuro-Tech and Biomedical Engineers go to different meetings. They don't really talk to each other. And so for me, it was really important to bring, to close the loop between Neuro-Morphic and Neuro-Science so that we can better benefit brain health and not just brain, but health in general. And that, that was very critical for me and I actually hosted, co-hosted a workshop with my colleagues at the NSF with co-funding from NIB and NINDS in late October and Baltimore. It was a workshop called Neuro-Morphic Principles and Biomedicine and Healthcare. And it was important to capture the health focus for Neuro-Morphic and Neuro-Tech at this workshop. And that was why the second day was designed to be more like a tutorial. The first session of the second day was intended to teach everybody what Neuro-Morphic means, both large-scale digital computing style of Neuro-Morphic as well as the small-scale mix signal analog computing and Neuro-Morphic sensing, which we had Jacque-Mohonde-Vary as well as RELFITian Cummings who are, you know, who really I think help teach the audience the differences between the different kinds of Neuro-Morphic technologies and how they may or may not be useful in healthcare. And then the second session was to really bring it home and have clinicians talk about the value of Neuro-Morphic technology. Yeah, just as an interjection, I mean, I've done some conversing with people like I just said, you know, reflecting about the workshop and one of the things I said to another academic was I was surprised that there was, you know, so much Neuro-Morphics in the workshop. And this person said, what's Neuro-Morphic? And I was like, whoa, and there are neuroscientists, you know. So. It's surprisingly not very well known. And that's one of the exciting opportunities and why we wanted to have good, you know, healthy representation from Neuro-Morphic approaches. Wait, because we should, Joe, maybe just say what Neuro-Morphic is because I realized there's thousands of neuroscientists, I guess, who don't know what Neuro-Morphic is. Or, or, that's introduced it. Sorry, okay. So, this question was asked at the, by an audience member at the workshop, you know, well, what is the definition of Neuro-Morphic? And nobody wanted to take that on. Well, I had managed folks not to get into debates about definitions. Yeah, yeah. But I just wanted to like, broadly, what are we talking about? But I'm saying so close, so close. And I bow-hand, who is also a leader in Neuro-Morphic computing, he put forward, it's computing that scales. It's scalable computing. So in order to have fundamentally scalable computing, you need to be more brain-like. You need to have, you know, memory on compute. And so the closer you get to the brain, which is very, you know, fundamentally memory on computing system, then you have the brain-like computing system. And you break or bend some of the scaling laws that make it difficult to scale up, you know, conventional CPUs, GPUs, on CMOS processes. But Neuro-Morphic, I would say, it kind of comes to the Feynman quote that we see everywhere, right? And there were two good Feynman quotes that came out of the workshop. But the one that you see everywhere is, you know, I follow the paraphrasing, you know, what I cannot build, I cannot understand, essentially. And so the Neuro-Morphic engineering community is folks who have been trying to build it. I mean, they've been, you know, from the synapse level to the cellular level to the, yeah. The original lineage, Carver Mead lineage of Neuro-Morphic engineers were people that were trying to emulate these channels on a chip, and creating spikes and characteristics that are comparable to what you would measure off of a cat pyramidal cell. That was the original Mishama Haba in 1991 paper that I think first popularized Neuro-Morphic. And that's what Ralph referred to as Old School Neuro-Morphic. But since then, there's been all sorts of development, which is why it's so hard to define Neuro-Morphic. Yeah. It's kind of, it's a word that, you know, it means different things to different people. And it could even mean principles. You want to, yeah. And as Kai said, you want to have your device that operates with the statistics of their signals from their brain, right? So some people even think of Neuro-Morphic as principles. But it could be physical principles as well. Yeah, physical and material principles as well. Guiding principles. Whether or not you actually use a Neuro-Morphic hardware is, they don't, it's okay. So there are people in the Neuro-Morphic community that think of Neuro-Morphic as hardware. And there are others that think of it as design principles. So it's hard to define Neuro-Morphic. But for me, it's brain-like. It's usually operates on spikes, but not always. Most importantly, it's energy efficient, six orders of magnitude energy efficiency. And it's adaptive to the user. It's a system that can evolve with the user. And those are the four things I think that really stood out and was discussed at the panel. And people win as far as saying because it's adaptive, it's less hackable. Because you don't actually need a computer to run it. The on-chip device is self-containing. Well, I'd say it's important to distinguish not all Neuro-Morphic hardware is learning hardware. A lot of the test-fed systems that have been developed. Intel had a program called Loehe. There were two generations of these Loehe chips that were basically, it's an academic research partnership program. You kind of applied and you can get some of these chips to devise models for and test and run them. And those were, you know, you could have a large number of spiking neurons, essentially, like you get in a great-enfire neurons. And you could just run that physically on these chips. But it was very difficult to get, you know, to implement like spike-based learning rules, like STDP on those chips. So there's other kinds of chips which are more amenable to implementing different types of learning rules. But that's something the field is still working on and trying to figure out. And it's one of the big questions going forward. How do you have these adaptive in a safe way? That's a really good point. There are some chips that are designed for inference only and others that are designed for learning. So I totally see how neuromorphics, and it sounds like the whole thing was focused on neuromorphics, which is not the case at all. But this is just a way into talking more broadly about the workshop. I totally understand why neuromorphics then would be something to put a large focus on. But then the question is like, okay, so what Joe was talking about earlier about needing to bring in theory and get all these people under the same roof to sort of bring together these otherwise kind of disparate ideas to work on the problems. One could wonder, well, what is that? What is what you guys were just describing? What does that have to do with anything related to theory? Right? So did we make progress on that? Or is that an ongoing challenge? It's an ongoing challenge. I mean, so the workshop was to bring, like Ray said earlier, this has been kind of a, there hasn't been enough conversation between these fields. And so that was a very intentional on our part to bring together these different communities to start having that conversation, even within neuromorphic engineering, the folks who want to use large scale neuromorphic computing systems to basically run large models of the brain. And so they need large scale brain data to build those kinds of models, connectomes and cell types and that kind of thing. And so you can use that as a test evaluation system, modeling a simulation system to confirm or falsify theories about how certain aspects of these networks work. But then there's the small scale side of it, energy efficient analog or big signal devices that can be distributed to the edge to to brain like neural like intelligent computing in a wider array of applications. And that's more towards the translational end. But I think it covers the full spectrum. And I want to go back to the theory. I don't think we talked a lot about it. So Brad Eimony actually came to the NIH and gave a Wednesday after lecture series talk on October 23rd, where he talked about how large scale neuromorphic computing could inform new theories, how you can make observations at the scale that you just don't see in smaller circuits. It's much more difficult and particular to the types of models that you're building, how to scale them up. And so I think there's opportunities here to break some of those scale barriers within computational neuroscience and theory driven modeling. And take it to the scale where the things that we care about happening in brains can actually be studied in a principled way. I said, so okay, we've kind of been focusing on neuromorphics and then we delved into theory. But I didn't say in the beginning, the backdrop of this is that the brain initiative is 10 years old now. So part of the driving force, and correct me if I'm wrong, with putting this works up together, was to figure out where the future roadmap should lead or what avenues are exploreable and should be explored moving forward. Is that a fair assessment? Well, we wouldn't use the word should. Because we want to get all right, exactly. We want to get the shoulds from the community. What are the different, all you have the neuromorphics? We've got the people doing metrics and benchmarks. We've got the people thinking about natural intelligence capabilities. How do they all come together? What are the main priorities and opportunities they see? And we want insight about that. And you've got some shoulds, at least during my last session there, there are some shoulds from the audience, which was fun. So good. Right. So from the brain and it's just perspective, we want to see, okay, get all these pieces together in this jumbled puzzle. And then figure out which of those pieces makes sense for the brain initiative, potentially to contribute to lend the type of work that we do in the brain initiative. Another thing I would say as a co-organizer is, we invited a lot of other funding agencies to this meeting, to be part of the workshop. Because a lot of these problems aren't necessarily brain's problems. Right. So coming up with the next super efficient computing system is a great idea. And it's great for humanity. It's great for solving the carbon footprint. But that probably lies in a different agency's mission space. However, the knowledge the brain initiative collects and continues to generate is useful to that longer term mission. And I was really hoping that would be clear from the funders panel. I don't know if we hit the mark there, but this is really a collaborative effort where lots of agencies are interested for different reasons. Yeah. I want to kind of complete the circle on just what the workshop was about and how you decided what kinds of topics to bring in. So when I think of neural AI, I think of like the truth. It's not traditional. It's new, but it's not new. But it's like kind of testing, using some artificially artificial intelligent models, neural network models as proxies for brain processes. And then asking whether you get something brain-like representationally out of those systems, like the sort of early work with convolutional neural networks and modeling the visual object recognition system on those networks. There's been a lot since then there's been a lot of recurrent network work like that. And then someone like Andreas Tollius, who works on what are called foundation models, was represented. So that sort of side of it was also represented. But maybe you guys could speak to what you wanted, what are the kinds of things that you wanted to bring into the work. Our personal scientific opinion and perspective at that time, which came out of the symposium that Grayson Microorganized for the 2020 Brain Investigators meeting. And that was a great panel. We had Conrad Corning and Zach Ficco and Nathaniel Da, Conrad Caraghan, and I'm forgetting Brad Fiber on that kind of, we call it dynamical systems neuroscience and machine learning. That's what we were bringing together, but it's prototype for Nero AI, thinking about his ideas through. But the perspective that Grayson I wrote coming out of that was that we can't. A lot of the problems seem to be from this purely computational perspective. And that's the perspective that's kind of grounded in this almost traditional brain as computer metaphor that has pervaded all of these fields. It's a pervaded cognitive science, AI, and neuroscience. Neuroscience, neuroscientists talk constantly about neural encoding, decoding representations, talk about representations without as these computational constructs. And that once you have representation, and it has some, to the experimenter, some explainable relationship with what we think is going on in the animal. We put it in the task, so it needs to go left at this point after seeing this queue and we see correlations to the right kinds of things. And it's like, okay, we found the computational representation. That is the explanation we're done here. I think we're done here part of that has been the barrier because you're not actually done there because there's still behavior that needs to be in the loop. It's, you know, behaviors this moment to moment, dynamical coupling between brain and body, between body and environment. And so that's why we wanted to expand outward and bring in ideas from embodied cognition, you know, the 40 literature. And you're not to sign onto that to say, hey, there's something here about these massive distributed feedback loops through the environment that are a key part of what's going on in cognition and animals. So that's kind of where we took this. And day one was designed to be all computational intelligence, whereas day two was more of the embodied noromorphic translation. So that was hard. Even during day one, I was impressed with how many people were reflecting on the importance of embodiment. I mean, it came up a lot. That was not planned. It was a surprise. I might have, I may have planned that. Did you plant that seed? Is that what? Well, for instance, in the same issue in which our paper came out, there was another paper called Deep Intelligence by Ali Menai, who's a electrical engineering professor at University of Cincinnati. And so I was aware of that work. And he's done computational neuroscience for a long time now, since I started in the field in hippocampus. So I was aware of him, but he had a very evolutionary perspective on it as well. He has a very holistic perspective on biology. You know, biological organisms are inherently integrated. They're integrated through evolution, philogenetically. They're integrated through development, onto genetically. They're integrated through learning and aging and experience. And you have to keep coming back to that, because that was his perspective that he was taking in the paper and in the talk that we invited him to give, comparing natural intelligence with AI. And there's so many important distinctions that you can make. But I think that's one of the key ones. We can't call it just occurred to me that holistic neuroscience would be a great term, except that it would be associated with holistic medicine, I think, which the word holistic has some positive and negative kind of. Right. It gets into the impulse for reductionism and the kind of counter movement of looking at downward causation and emergence. Well, I just meant the science of holistic medicine is sometimes questionable. So to be a holistic neuroscience, someone might see that and think, oh, it's woo-woo or something. But on the whole, that's pretty good phrase. I think it's woo-woo if you do ignore the internal computational representations. You can't ignore it. That's why across two days we had the focus on, yeah, so a personal opinion is calling it kind of the mainstream neuro-AI. Let's figure out how to map these task-constrained AI models to what we see in the ventral visual stream. Like you said, a lot coming out of that, people are looking at the personal stream. People are looking at motor system and other areas. Cardative maps. Cardative maps. You name it. Well, kind of maps are maybe the clearest example of a actual high-level cognitive encoding. At least I, that's my personal opinion as a hippocampal researcher. Yeah, well, hippocampal's chauvinism. Just applying sort of neuro-AI models to account for cognitive functions. Cognitive maps has been a big. Absolutely. No, I think it's an important, I mean, it ties into the dimensionality reduction, the task-based, low-dimensional manifolds. Yeah, we're hurting hundreds of thousands or millions of neurons now. There's no way to visualize that. If you just throw everything into a UMAP, you get some interesting colored splotches on your screen, but it doesn't tell you how to interpret what's happening. Oh my gosh, you're speaking to what I'm, I make some really pretty naturalistic behavior. Neural UMAP graphs right now, and gosh, they're pretty, but they're not the solution. They're not, I'm not done. Right. When this ties into the whole interpretability, explainability, and mechanism discussion, how do we get what the important factors are that are driving that high-dimensional neural activity? Oh, wait, so now you're, you're just jumping the gun and going right into, I mean, maybe that's the way that we should do it, is talk about some of the topics in the paper and then bounce back and forth, but I don't want to also not come back at least to, well, actually, let's hold off on going, I know that they're all related. I've never said that you need both. Okay, so then I'm not sure what we've missed about the workshop, but I wanted to get your kind of general reflections on how it went. Yeah, I mean, so from the point of view of the workshop, we had, you know, we were both incredibly thrilled and pleased at the discussions that we had and thank you Paul for stepping in as I discussed it on the first day and for helping with the wrap-up on day two. Yeah, we've been reviewing the recordings and it's, when you're in the middle of it, you don't get to really listen and experience it, but it's really great conversations and discussions and questions that we had. At one point in my first discussion panel thing, I got to yell at Terry Sanowski. Well, and I thought like, oh, why am I saying something negative about Tudari Sanowski about what he said? As I was saying, and I was having this moment like, oh, you're not in a position to even talk about this with him, but you know, he was, it was fun. It was fun. Because he's like a hero, an intellectual kind of hero, too many people, including myself. And so you're in that situation, and this is maybe very meta, so I apologize, but you know, you're talking to your heroes sometimes, and you realize either are they a colleague or they a hero? Like, you know, and it's just kind of a, that's surreal, but an interesting feeling. Well, this is why we brought everyone together. We want the leaders of the field who have been around and just driving things forward for as long as anyone can remember. And his colleagues had one of the Nobel Prize, just a few weeks before the workshop. And if you talk to people around that and you go back and read those papers from 84 or 86, Terri is a co-author and all. I was wondering how he felt about that. I'm not sure if it's the right place to discuss that, but I imagine a lot of people have wondered like, does he feel like he was missing from that? Well, the day the Nobel Prize was announced, the tie-right neuromorphic community overtly wondered why Kerry was left off. Yeah, no, I saw a few things like that. That was shared upon all of us who attended Tell You Right this year. So it was good to be able to acknowledge Terri's contribution at the workshop through multiple talks. And I think you were fine, Paul. You were great. Oh, no, yeah. But he corrected me, which was wonderful because he correctly corrected me. And then I was like, all right, I'm not going to get into it back and forth with this first. I have to say every time I felt something didn't go well, I went back and listen to the recording. And I was like, oh, it went way better than I thought. So I think the workshop was really a great success. And I learned things that I didn't expect to learn. Well, I came away thinking that it felt like a win, a great success. And so I'm not sure if you guys want to elaborate more on how you reflect more on how you felt about how it went and maybe even what could have been what may have been missing of that will happen next time or how this how reflecting on what happened here affects how you think about moving forward. I feel like the ethical the neuro ethical conversation was a really important one because neuro AI is going to bring about a lot of new challenges. And that Karen Marble funders talk was really insightful. And I think if we were to have another one of these workshops in the future, I felt like we didn't give her a chance to ask her question because we ran out of time. So it may be a little more actually to be honest, I don't think you could do neuro AI ethics justice in a one hour session. So there ought to be more conversations about ethics and as well as regulatory questions. We can't speak directly about the future, but clearly brain put on this workshop is interested in the space and we agree. We think it was a great success as far as our goals of hearing from everyone. Paul, do you think there was a scientific community that was not represented at the workshop that should have been? Well, geez putting on the spot here. I mean in some sense, so I was rereading your position paper and maybe this is a segue into that because yes, there's a lot of embodiment and I'm trying to reflect now myself because as I'm reading through this paper here, the title of your position piece, neurodynamical computing at the information boundaries of intelligent systems. And so I'm reading through this thing again and it's so rich and dense and makes the case for embodiment and the importance of environment, body, brain, continuous cycle interactions. And of course, and then I'm like reading and I'm like, oh my god, I click on every reference and I'm like, half the time I'm like, oh, that's good. I've read that. And then the other half, I'm like, I got to add it to my reading list, you know, and it's like, it's, but so in some sense, and the irony is my original job was to synthesize the workshop. And I didn't, to be honest, I didn't really know how I had an idea of how I was going to do it, but I didn't have like a set plan. And I ended up being like a more of a moderated discussion, which was great. And then a lot of interaction from the audience as well. But the reason why I said synthesize and I think Joe might have mentioned that term earlier is because your position piece synthesizes a ton of stuff with the goal of using so much historical perspective and what may be missing these days in AI to synthesize what you call a base layer of computation. You're going to correct me on this. I don't have the exact quote, but a base layer of neural computation. I think it's what we called it. Yeah. Okay. Well, I know it's called a base layer, but yeah. And so, so you asked me what I thought might have been missing. It might have been that sort of bringing together of the historical contexts and why these things are important. And then an interesting thing happened that two people, Blake Richards and Zach Pitco at Zach at the very end said, a great goal for us would be to record the connection strength of every synapse. And that's such a reductionistic approach that is in line with modern reductionist neuroscience. And it kind of flies in the face of what you guys argue for in this position piece a little bit. And I thought it was odd that there is still kind of this like reductionist assumption underlying all these things to measure more. And here's the level that we need to measure at and modern dynamical systems theory, manifolds, looking at larger populations and that lower dimensional structure is somewhat antithetical to that story. So if anything, I thought maybe like that the whole even manifold and more talk about like levels and going across levels, what's the right level of abstraction? Why it's the right level of abstraction? So if anything, and that's more on the theoretical side, if anything, I thought there could have been more of that, I think. Man, that was long winded. I apologize. More of a comment than a question. No, I totally agree. So here I have to be very careful to differentiate my perspective opinion on the field. As I was at Grace and I wrote this paper a couple of years ago now. And this did kind of come out of a wide range of frustrations, which is why I went deep historically and I brought in ideas from philosophy of mind, philosophy of computation. Let's think about what is computation? What does computation mean in the brain? What does information even mean in the brain? Because one of these other things that pervades all these fields, and I do give like a historical capsule at the top of the paper, cognitive science, neuroscience, AI, is this kind of, they have the same conception of what information is. And it's Shannon information, which as we know from the 48 paper and the later one, where it's about communication, where you have a transmitter and a receiver, and you have a shared alphabet. And that may not be the right metaphor framework for understanding information in the brain, especially how brains construct semantically meaningful structures and processes and dynamics. Well, Shannon was even aware of this conflation of information with meaning. And there's a very short, I could put it in the show notes, he actually wrote up a very short piece saying like, look, people, this is, this might not apply to your field, because everyone was applying Shannon information to their fields. So he was warning against his own work being applied to broadly and misconstrued. Yeah, I think I've seen that. And it kind of reminds me, but also Tony Zador in the opening keynote brought up the concept of the hardware lottery. So in a sense, Shannon's information theory is kind of a theory lottery. It provided everyone a readily accessible tool for like, oh yeah, information, this is a really important concept. How do we measure it? You know, and like, you know, grab hold of it. It's like, okay, well, here you go. You just, you just run these sums and averages over this kind of distribution. And it's a purely a statistical process now. It's purely syntactic. And, you know, I think, I think it was, it was Shannon maybe that piece, who said, you know, this is purely syntactic, like this does not get at, you know, the semantics of what this actually means. And if we think about, so this paper, neurodynamical computing, and information boundaries, it's because there's different types of information and they are transformed across the boundary of an organism. And so you have, you know, the, but now you're not using information in the Shannon sense. You're using it in the right. Right. So what is different about information and a biological cognitive organism? It's that, that organisms construct a boundary. It's, you know, it's our skin, but it's also, you know, our xteroceptive senses. And we have ways of taking in information. You have the whole universe of sensory input coming in, but then you also have the internally generated universe of goals and drives. Right. So what organisms, you know, are in constant conversation with the environment, we depend on the environment for energy. That's why forging is such a fundamental problem for animals, you know, animals fundamentally what defines them is that they move and they move over to forage and find food and energy and then so they can do shelter. They can find more food so they can move more. So they can find more. Yeah. Well, it turns out, ecologically, there's, there's a lot of niches that are that open up if you can move through the environment. Right. Otherwise, you're a coral or you're a sea squirt, you know, attached to a rock somewhere and you're a filter fear. People will take umbrage to the idea that those don't move by the way. Some people will, you know, even plants. But yes, I get the process. There are societal animals. Yeah. So sorry. I did not mean to offend the SSL organism community. But movement is fundamental to all of this. And so it's that that dynamical coupling at those informational boundaries that allow the the goals to basically stream against the incoming sensory inputs along that kind of like hierarchy of both perception, you know, ascending, the hierarchy of of drives and movement and behavior control descending. And so that was our perspective that you can think of, you know, it related to like, you know, the predictive processing framework like Carl Friston's unifying theory, where he sees the brain as a distributed, you know, internally generated feedback model and and you're canceling out, you know, prediction errors as they ascend with the top down expectancies and then there's trade-offs that are governed by his conception of of free energy. But, you know, that unifying theoretical framework has, you know, has had trouble gaining traction about making, you know, direct predictions about what people should be doing in neuroscience. What type of experiment should you design to figure out, oh, that's, you know, this particular function, you know, operates this way within the predictive processing framework. In a way that's distinguishable from some from some other framework. And so I'm just trying to step back and not put a name on things, but fundamentally organisms construct meaning through kind of managing basically the entropy at this interface. And so that maybe that's prediction error, maybe it's something some other quantity, but it's you need to manage entropy fundamentally. That's what you're doing, you know, thermodynamically, we're far from equilibrium. Like that's the whole game. And that's the control theory aspect of it. Is that where that comes in as well? So, so what I was going to say originally is, you know, the injecting meaning back into neurosciences, you know, not the default. And the default to neurosciences is reductionist, brain is a computer, and then we we can go from sensation back to, and we don't have, we can disregard goals and meaning and purpose. That's kind of been the default position of most neurosciences for a long time, although in the paper, and in elsewhere, it's pointed out like that early cyberneticist movement, you know, was more about control. And so, so I guess that's why I'm asking like, is that where the control theory aspect comes in? So that ties into, so in this paper, which again was our, you know, our perspective, we found the most, I guess, some Pannico framework out there to be what's called perceptual control theory. And so, this is an alternative branch of cybernetics essentially from the 50s and 60s that was kind of brought home or initiated by Bill Powers in the 60s and 70s. Yeah, but the whole, so like Henry Yan, whom you sat in the paper and who, you know, he's been on this podcast like many of the reference references that in your paper, you influenced us too, Paul. Well, just by having maybe by having people, yeah, through the past. Thanks. That's awesome. It's a, yeah, it's awesome to see when it's awesome to see so many references in a paper is like, oh, that person has been on the podcast. That's a person's book. So I just mentioned Henry Yan, and he was saying one of the problems with like early cyberneticist research and control theory in general actually is that the reference signal of a machine is external to the machine, whereas we have internal reference signals that we're trying to control to match for those reference signals. And that's a fundamental difference. And that's what neuroscience is missing. And so, I don't even remember, remember my question, but that is what you speak to in the paper as well. Right. Yeah. So Henry's written a couple of book chapters with this perspective and they're, you know, they're kind of bomb throwing chapters in a way. And it's, I think it's helpful to have strong opinions out there. Because it really makes you think, okay, this kind of sounds interesting. It's provocative. But where does it go wrong? And so that's, or does it go wrong? Yeah. So that's kind of reading, reading that and reading, you know, in my, I'm not a motor systems person, but you know, I have passing familiarity with with the motor control paradigms, those theories, you know, coming out of the 90s and 2000s, you know, optimal feedback control conceptions of motor commands, you know, the theories models about, you know, referring to copy and corollary discharge systems. And these all, you know, all the traditional motor control systems or frameworks are based on, you know, building up more and more detailed and refined internal models, basically forward models, you know, to predict, you know, the consequences of action and movement. And then using that to evaluate, you know, different commands and behaviors. And putting that in this much larger, much more complicated control loop and PCT or perceptual control theory is appealing because in a way, it kind of reverses it. It says, no, it only matters is that you're just, you're only making comparisons at each level in this, in this hierarchy. Because you're making direct perceptual comparisons. And so if you have a direct perceptual goal at the highest level, then those reference points come down or compared to the ascending perceptual input. And then that, you know, descending reference to the next level gives you what you need. And then so eventually it filters out through your muscle actuators and your movements in the world. But it's not eventual because it's all simultaneous. It's all simultaneous. Yeah, everything, it's a staged flow of control signals, right? Sensory and control signals across a hierarchy. I'm individual in terms of time where one signal flow starts, it takes time to propagate. Not that there's a central organizer that says go and then, you know, from nothingness because, yes, part of what you push were also is, you know, this consideration that perception and action cycles are continuous flows. Right. And so that phrasing kind of ties into this conception of everything as being linear input output. So if you have a cycle, it's like, okay, first you're at this step and then you're at the next step. So you're at sensation, then you're at cognition, then you're at motor commands, and then you're at behavior, and then that changes, you know, the, you know, the pose, the orientation of the organism with respect to the environment, which changes the sensory inputs, and now you're back at the beginning of the cycle. Right. And so if you have a very complex computational forward model in that control loop, then you have to imagine that the delay of computation is now a delay in your control loop. And from a control theory perspective, from a control engineering perspective, the more delays you have, the weaker your control is. Yet one of the, I think the most, you know, dispositive or fascinating, you know, properties of animal behavior is that it's really good. It's really resilient. Animals can accomplish their goals, you know. Not always, but they're really good at it. Yeah. Right. But compared to the variability of the observed behavior, so, you know, the, the robustness of accomplishing goals far outstrips, you know, the actual movement, which seems like way to say, how is any of this possible? How is it possible for, you know, for a rat to, you know, to make its way through this very complicated borough with basically no light and only a small set of, you know, sparse cues. But like, it can navigate that borough like really well. And so it's, it comes down to, well, maybe don't have all this complex computation going on in the loop. And so this is a conversation that I've tried to have, you know, have over the years with people, you know, working in motor systems and, and, and they think, you know, either I have the wrong idea or Henry has the wrong idea or actually all of their theories already encompass this idea. Don't worry about it. And so it's, it's, but it seems to me this is an open, you know, for my personal scientific opinion. This is an open question of, you know, forgrescentverse models, you know, ascending like prediction error comparisons versus perceptual reference point comparisons up and down the hierarchy. And that's some of the discussion that I wanted to open up at the workshop. So from brain perspective, you know, I'm not going to impose my, my views on this, but I, I see that that's an important conversation. And I think that will open up a lot of potential opportunities for driving theory forward. And data is a part of it. So, you know, the reductionism is, is, you know, obviously, molecularly characterized cell type atlases of whole brains, very fine grain, connoctomic datasets, the flywire dataset, that was just released, which was, which was brain supported on a number of grants. And there's more to come. We launched the brain connects program last year. And we'll start seeing data from, from those projects in the next few years. Lots of exciting stuff to come, but that's obviously from a very reductive, you know, approach to neuroscience, break things down and just so we can see everything that's there. But I think that does need to be in the loop with this more holistic way of thinking. And so I think that's where, you know, there was a lot of talk about digital twins, multi-scale biophysical modeling. And then thinking about different ways of putting this in the loop with, with, you know, behavioral neuroscience in different ways of understanding. That was definitely a surprise for me at the Narrow AI workshop. That there was so many talks about digital twins. Oh, even in sessions three and four, it seems like the community is really ready and really want digital twinning in their rate respective research areas. What's a digital twin? And why do we want one? Well, that's another definition question. Well, you don't have to define it, but roughly. There actually is a definition that the National Academy of Science, Engineering and Medicine put out. I have an infronomy. I'll read it to you because a digital twin is a set of virtual information constructs that mimics the structure, context, and behavior of a natural, engineered, or social system, or a system of systems is dynamically updated with data from its physical twin, has a predictive capability and forms decision that realize value. The bidirectional interaction between the virtual and the physical is central to the digital twin. That is like the, that's it. That's the official definition. And I think people have their own definitions. Yeah, I'm sorry. I asked for the definition just kidding. And they were using their own definition, which is a subset of this official definition. And this actually was a question that came in in the email. So, so I, you know, I was surprised to hear so much digital twin talk. And I think that's potentially a new exciting area that could, you know, that the neural AI workshop participants can continue to engage in. I think there's an important continuum there as well. So we heard some of that discussion in the first session of the workshop between neural foundation models on one side and digital twins on the other. So these are both large scale ways of using large scale neural and behavioral data. But they have different goals. So neural foundation model is kind of like, you know, the foundation models in AI where you want to have, you know, a base model from which you can, you know, generalize to downstream tasks and, and, and, and, you know, the application specific domains or to answer particular questions. And so digital twins, you know, kind of parsing the definition that Grace just gave it is really more focused on using lots and lots of data to make very, you know, clear predictions about a particular individual system. And so it's kind of individualized. It's or in the health context that can be personalized. And you can test hypotheses about the natural system using the digital twin. Right. And the idea is it evolves with the system you're studying. So if you have a digital twin of, you know, let's say a mouse. And then the mouse is in a predictor task. You can be running the digital twin model in silico in parallel with the actual experiments. And now you've got, you know, your, your, the title of Patrick Manot's talk, you know, closing the loop with virtual neuroscience. So closed loop neuroscience, we've got in, in silico ghost or simulation, essentially, of the actual animal in the experiment. And then you can do very fine grained real time predictions modulation. And, and, and the idea is that that should be a very powerful approach. Yeah. I mean, you can imagine all sorts of things like tracking through the lifetime changes. And, you know, through development and the lifetime. And, you know, in, not necessarily humans, because that's really longitudinal. But you can do it on a faster time scale with something like, well, it actually came out during chimilers presentation that he as a, as a functional neural surgeon would, would like to have digital twin in the future. Yeah. But I just mean that, that particular idea of like tracking over the lifetime. But, but if he had that in his surgical suite, then he could test things very quickly and then decide whether or not to implement some surgical technique. Exactly. Yeah. It was, it was a, that was really, you know, a conversation I wasn't expecting from session four. And it, you know, but it was very insightful. Those are the fun things that workshops when something like that surprises you. Yeah. And, and also, you know, the combination of Chris Rosal and Ky Miller on the same discussion, sort of both both are very clinically savvy and taking opposing views for, for neuromorphic was, was exciting session four. And when, when Joe brought up the concept of a hardware lottery in the concepts of Shannon information theory, it kind of reminded me how neural texts certainly also could suffer from the hardware lottery, given how hard it is to get devices approved. And so we're often just stuck with what's approved and not what's necessarily the best. I'm aware of our time. And I want to make sure that we talk about the, so, so what's in one of the interesting and maybe surprising things because there's the paper does so much is that you end up arguing that for a base layer of neural computation, right, that like we talked about before. So what, what, and you don't have to define it, but what is a base layer roughly of neural computation? And then, you know, why do we, why do we need one? Why do we need to determine what the base layer is? Well, okay, well, what I call the base, this is not a, like a term that I coined. It's just, I refer to it as the base layer in this paper. But that kind of came out of thinking, you know, I was reading some philosophy, philosophy of computation. What, you know, what, what are the different types of computation? How do you do computation and physical systems and dynamical systems, right? Is a system of ODE's differential equations and you just evolve them forward with, you know, Rungecutta or, you know, whatever your algorithm is, is can that do computation, can, continuous dynamics compute? So there's a lot of really interesting questions around computation and, and brains are a particular kind of physical, dynamical, material, chemical system. So I think just, you know, again, falling into the default concept of, you know, the, the neurosentric framework for understanding brains, you know, can, is maybe leading us astray, obviously, in about throughout biology, cells are super important. So neurons are super important to brains, but also their answer not the only cells in brains, as we know, is, is also a glia astrocytes, you know, oligodendrocytes as microglia, which have, you know, important roles in structural plasticity. Looking beyond, like, digital computing has gotten this into thinking about computational systems as, okay, there has to be a transistor. It was like some unitary element, that's the lowest level thing. And so if you think about a, a, a silicon disc with, with chips that have been, you know, that have been burned into a, a, a, photo with a graphic process, all it is is, is, is, is, is, is a material carving and silicon and other types of materials. And so that's the base layer. Like, you know, the, the, the, the, the, the, the, the, the, the, the, the, the, transistors on, on the, in the CPUs, W's, and all the work computers and phones right now. That's the base layer of computation for, for digital computing and on conventional, you know, CMOS processes. So is, our brains just like that? So our cells, like, transistors, is that it? No, cells are binary event action potential generators, right? I think macollochin pits were, were right. And nothing has changed. And so we should still consider them that, right? No. Oh, I mean, so macollochin pits, I mean, that was fundamentally wrong, right? It's not a binary signal. It's a, it's an event. A spike is, is an all or none event. And, you know, and so some people will say, Oh, okay. So it's a spike timing. It's like, we just need a highly precise, you know, what's the timestamp on, on that spike and, or versus that spike? And then that'll tell us everything. Well, no, because you don't need an absolute timestamp. You need to know what the role of that one spike is in this dynamical system, because that spike is propagating to downstriven neurons. And, you know, at some point you hit recurrent connections and it feeds back. And then you go up a layer to the next, you know, a higher level of cortex or whatnot. Everything is causal dynamical, interconnected. This is, yeah, so you can't just say, you know, it's one or zero. Obviously, that inspired von Neumann. And it's, and it's an amazing insight because that's why we have digital computing technology now. But, but it's not how brains work. And it's all for Freeman pointed out. It has we also heard from Yota. There's all this dendritic computing that's happening at the dendrites, right? And so there's just, I think, so much more richness that we are now aware of that wasn't available to my colleague and pit. That's, that's true. But now, Grace, you just went down a level, uh, physically from the point neuron to dendritic computing, which would make a punyota very happy. But then, but, but you guys want to go up a level and talk about the role of oscillations in this dynamical coupling. And, and I also found myself wondering, I don't know if to how engineers think about oscillation. The other thing is traveling waves that we really didn't talk much about at this workshop. That's true. Which I'm surprised Terry didn't bring up because he likes to talk about traveling waves. Yes. And he actually sees oscillations and traveling waves to be one in the same for me parts. How are they? How would they not be? I mean, an oscillation has to has some spatial oscillations repeat. You can have a travel a wave that travels that's that's not being generated by an oscillating generation process. Okay. Yeah. Yeah. That's fine. You can separate them. And, but that's like having one wave in the ocean, which isn't an odd. Once one wave leaves, the other way of has to, I don't know, they are intertwined in general, but I could see you could just do a one right space and time are coupled in the brain, right? So fast oscillation. I mean, so there's a mathematical formalism called hierarchy theory, which is basically if you have, you know, oscillations at a fast frequency, you can only maintain coherence over a small amount of space. If you have oscillations out of slow frequency, you expand the region of space over which you can maintain coherence with that clock, with that slower oscillation. And, you know, at least mammalian brains have a really well-preserved set of, of, you know, neural oscillations in different parts of the brain at different times that, you know, interplay with each other in different ways, at, you know, base frequencies, which are at these really interesting, interesting, commensurate fractions of, between each other. So it's almost like we need, you know, nature needed to find, you know, half a dozen different frequency bands that didn't interfere with each other, or minimally interfere with each other. Because then you can have a theta and you can have a gamma and you can nest seven of those gamma cycles in one theta cycle. And then that becomes an interesting packet, right, of coordination. So it's not the spike timing. It doesn't matter that, oh, neuronephired at time t zero within theta cycle, you know, whatever x. It's not that absolute index of time that matters. It's the fact that, oh, you got this, you know, this packet of activity that's carved out by this, you know, the sequence of gamma oscillations, gamma cycles within this theta cycle, and that theta cycles within this, you know, larger set of, you know, slower rhythms. And, you know, it's hard to ignore, you know, this kind of, these, these, these laws almost, so, you know, this, this relationship between space and time, we have these conservative oscillations. And they do govern, you know, the timing of spikes, the activity of neurons. And so there's this feedback, you know, loop that kind of goes up a level to a collective, you know, collective behavior, like an oscillation, and then through epaptic effects or through just, you know, other, modulations, they, they entrain and, and, and feedback into, into causal mechanisms at the cellular level. So then I want to ask what the base layer is, like what the proposal is for the base layer, but maybe even before that, I could sort of list off the three requirements that you posit for a base layer to be a useful computing layer. And by the way, this is a, I should also say that the proposal is a non-reductionist mechanistic account of neural computation, which is an interesting thing self. Okay, so the requirements that you state for a base layer, and I'm reading directly from the paper here, is that one that they encompass a macro scale hierarchical control structure, so over which it implements comparator, error, and output functions. So that's the control theory aspect, part of it, to, that, to adaptively control access to internal and external information flows generated by physical embodiment and situated embedding in a causal environment. So that's the sort of almost ecological psychology interaction between these continuous flows. And then three support discrete neurodynamical states and adaptive high-dimensional state transitions across time scales of neural circuit feedback. And then you list some specific kinds of time scales. And I suppose that links into the reason why oscillations are important. These nested structures of oscillations, the spiking information carried within those oscillations and how they're interacting across different time scales and structures and flows and flows. All right, mouthful. But wow, that's an ambitious framework. It's super ambitious. It is like super ambitious. This is like a 30 year brain initiative kind of that's the other thing is reading this paper. It's like, this is like a whole textbook or a whole like four special issues in some journal, like condensed into one thing. I mean, yeah, it's initial. Well, I should make clear if this is not, this document has nothing to do with the brain initiative. Yeah, it's not. Fuse, perspective, priorities, plans, or any of that. But you come out like thinking, oh my god, like where do you begin? Like how do you start? And where's the, you know, so much to do? Well, I mean, so, okay. I wrote this a couple of years ago and I'm sure I even remember the three criteria that you just listed. I'm looking at the paragraph now. But this all came out of, you know, again, kind of a frustration and just kind of wondering, well, what if, you know, the whole neurosentric paradigm is wrong? Yeah, you know, Raffa used to have a review or perspective paper from a number of years ago, you know, saying the, the network centric paradigm is what, right. He went from the, the neuron doctrine saying that's old, didn't work. Now we're in a population doctrine era. He was advocating for, which is kind of where the field is right now. Well, like, okay, it's all population dynamics and then a fold. And so, and John Hafield just got the Nobel Prize. And so, yeah, there has been movement in that direction. You know, as we've been, you know, fundamentally, there's a lot of technological determinism here, like the better our tools get. And, you know, brain initiative was certainly behind a lot of that. The more neurologic and you record a better fidelity, and more throughput, the more you can see. And so that's, you know, we're getting beyond single neurons because we can record millions of them now. But we need to understand what's happening still. And so now the focus is on low dimensional representations, right? But what's a low-dimensional representation? Essentially, it's an attractor. It's a, it's a small subset of a high-dimensional space that's been carved out. And you can say, okay, effectively, you know, these, this million neurons I'm looking at, you know, the, the, the state of the system is somewhere on this like, two, three, four dimensional maybe four, you know, but, you know, a low-dimensional system. And you can basically understand it. Like, if you can map the axes, the dimensions of this low-dimensional manifold onto task requirements and constraints, then it's like, oh, that's explainable. Like, I know what's happening in this neural system. But then there's questions about, well, okay, the whole brain isn't a single tracker. It's not one giant hot field network. I don't know. Some people might disagree, but okay, go ahead. But I'm saying, well, I think there's a tractor-like dynamics everywhere, but it's complex and heterogeneous modular to a degree, and governed, transiently, dynamically, by lots of things at different levels of organization, including things like oscillations, and things like traveling waves. So, you have coherent organization at time, coherent organization at space, a lot of its quasi-hierarchically organized. Because the flexibility, you know, people will think about natural intelligence and agility, its flexibility. Your animals are optimizing multiple objectives simultaneously. How do you do that? Well, you do that by flicking on and off different sub-networks at different scales adaptively in the right way. It's like, I need to keep trying to do three things at once. I want to get food in an hour. I'm trying to wrap up the current sentence I'm speaking. You've got these multiple goals in mind. How do you do that? You need to activate different attractors in different ways in a complementary pattern to achieve the goals of the organism. So, that's where the spatial temporal organization comes in. But it's government, kind of like this heterarchy, maybe, of attractors or quasi-attractors. So, that's where I went to in this paper is thinking about something, I regret that terminology, but I called them tokens or causal tokens. I was just trying to think of like, how do we think of an attractor not as this, like, oh my gosh, with this huge, you know, task manifold. You know, the animal goes into amaze. The town of fold needs to come up and then it needs to go along the manifold. Yeah, absolutely. And then all the activity protects on to that manifold. You can figure it all out. And then it's like, you know, if there's a go-no go, then it's like, okay, then the selection factor rotates through it and then boom, like the behavior happens. And you know, kind of like Dave Sassillo's or Eric and Neo going back to that 2013 paper, which I think is a great idea. And so, something there, but I'm, you know, the interpretation of what that means. What does the selection vector versus the, versus the command vector or whatever the other space was there? How do we think about communication subspaces? How do they come on and off, you know, adaptively and serve us up goals? So, my idea was, you know, causal tokens are kind of like these little quasi-attractors, and they can exist at different scales in quasi-attractor because like, you know, you don't get stuck there. The system doesn't get stuck there. You always need to, you need destabilization. You know, if you go fall into an equilibrium state, you can get out of your debt. But, you know, as we know, cognition keeps moving. It's always, always moving. So, you're always finding little attractors and then being bumped out of them. You know, there's a competitive process maybe, you know, it's, but it's, it's just trying to think of like, what is the base unit of computation if that's what's happening? So, what is the base unit of computation that you're advocating for, the base layer? Well, I kind of went back to Heb, Heb in Carl Lashley, and I should say, this was, this was a great review and perspective, put together by Drew Marr and Lin Nadel from a few years ago, it's cited in the paper where they really make the, they reconceptualize what Heb was talking about. You know, Heb and his student, Carl Lashley really thought deeply for many decades about what it meant for the networks of neurons to be connected to each other. What are they doing? Looking at persistent activity as well, they're self-reinforcing patterns of activity. So, basically, that's, that's the causal base layer that I was interested in. If you have a super neuronal group, you know, a group of multiple neurons, and the thing is, there's like a loop of referberant activity going through those synapses within that interconnected loop at some level, and it's self-stabilizing, like you, you can like ignite it this essentially, and then the connections are such that you, the non-lady, the non-lady already is all lined up, and you get self-reinforcing self-supporting, self-sustaining activity. That's the basis. That's the base computational unit that I was speculatively, you know, putting forward in this paper. And then the nice thing about that is you can sprout loops off of that, right? There's always another connection. You can always reconsolidate those connections in a different way. That's, you know, maybe structural plasticity is like, you know, maybe there's a side loop that's sub-threshold, but then one thing happens experientially for the animal, and then that connection gets twisted up a little bit, and now the non-liniarities line up in a different way, and the loop expands. And so now you're at a higher scale causal token or whatever I was calling it. This quasi-attractor now means something that gets us incorporated this new correlation from the environment. And so maybe that's a control signal or control parameter that was updated within the hierarchy or something like that, but it's a self-sustaining bit of activity governed by all the spatial temporal structure that we were talking about with oscillations. It's fun to see you light up like that when you're describing it. You look excited and sounded excited talking about it. I think you reactivated my neurodynamical state when I was writing the paper. Yeah. Yeah. So then all right, we have just a few more minutes grace to do. It looked like you might have wanted to jump in there also or no? Well, I just wonder how any of this could be measured experimentally. Oh, good God. We don't have another two hours or or is where you planting. Was that a plant question? It's a quick question for Joe because the idea of measuring every synapse came up yesterday. I mean, oh, well, you could do that. You could do that. That's very straightforward assuming you have the right technology, right? But yeah, that's a great question. Well, you brought up earlier Paul as well. And so this does tie in directly, right? So I'm not saying my idea here that I just went through is absolutely right. I mean, it's just kind of where I went is like this seems like the most likely useful framework for thinking about it. But if this is true, then the actual particular value, the precise synaptic weight of any given synapse is almost immaterial. Okay. That's why you're bringing that up. Yeah. Because I questioned that too. I actually pushed back in that discussion. What did I say? Something about if you just measuring something doesn't give you the theoretical some blah, blah, blah. I can't remember what I said. But but then I got kind of pushed back for saying that. And I was like, oh, I didn't realize that what I was saying was even controversial. But okay, yeah. So thanks for bringing that up again. Right. Yeah. So in the paper, we say that, you know, if this hypothesis were true of, yeah, this, this kind of like what matters is self-sustaining little clumps of neurons that can expand out work and adaptively, then this completely is hypothetical to what we see in AI models based on artificial neural networks where everything we care about is, you know, when you distribute an AI model or a transformer LLM, it's a binary blob full of very precise weights. And then the whole game is to see how far you can quantize those weights down and still preserve the functionality. And so you can put these things on phones and, you know, and not in home computers and all of that. So everything that matters is the weights. It's all in the weights and nothing else. The biases too. But if this hypothesis is right, then that doesn't matter. And we don't understand the brain. We don't actually need to go around measuring every synapse because they're wildly fluctuating anyway. It's highly volatile, right? So people would argue against that, but because I've received pushback saying that exact same thing that actually they're largely quite stable. And I guess we won't know until we measure every single synaptic. Well, you have to deconfound the effect of if we do have like the self-sustaining, you know, quasi-attractors, then you're going to have synaptic loops which self-sustaining and do maintain like strong correlations over time that persist about relative synaptic weights. You would expect that. But it's not the weights that matter. It's only lining up the right set of non-linearities so that that group fires in the way that it does and interacts with other tokens or other quasi-attractors. Sounds like dynamics. That's what's largely. The dynamics was switching, right? Yeah. And that was not. And that was not hierarchical and hierarchical fashion. Or just hierarchical. I think it encompasses all of it. Guys, I have to go here in a minute. I really, again, this is one of those papers that I'm going to revisit and then feel guilty that I'm not reading every reference. And that's that stack that grows ever so larger of what we're supposed to be reading all the time. But it is just so rich. And I'm glad to point to point people to it. Well, congrats again on running a great workshop. And I think I think a successful workshop. And I really hope that you guys get some rest. Get some rest. I know you have to sort of take in everything now and then reflect. But maybe that's hopefully a little bit more relaxing a process. And then you can take a little vacation. Yes. And thank you so much, Paul, for trying to the various pre-coronation meetings. I mean, I was so impressed at how hard everyone worked. Yeah. We got multiple abstracts, multiple versions of presentations. It was amazing that we had everybody share their files on the on the NIH box. And you could see how people were changing their presentations and response to each other. And it was, it was just thank you so much to you and everyone else who really made this a great workshop. That's great. Thanks for having us. Yeah. Thank you. Brain inspired is powered by the transmitter, an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advanced research. Visit the transmitter.org to explore the latest neuroscience news and perspectives written by journalists and scientists. If you value Brain inspired, support it through Patreon to access full-length episodes, join our Discord community, and even influence why invite to the podcast. Go to Braininspired.co to learn more. The music you're hearing is little wing, performed by Kyle Dunovan. Thank you for your support. See you next time.
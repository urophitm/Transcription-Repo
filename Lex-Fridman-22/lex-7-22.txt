The following is a conversation with Melanie Mitchell. She's the professor of computer science at Portland State University and an external professor at Santa Fe Institute. She has worked on and written about artificial intelligence from fascinating perspectives, including adaptive complex systems, genetic algorithms, and the copycat cognitive architecture, which places the process of analogy making at the core of human cognition. From her doctoral work with her advisors, Douglas Huffstatter, and John Holland, to today, she has contributed a lot of important ideas to the field of AI, including her recent book, Simply Called Artificial Intelligence, A Guide for Thinking Humans. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it five stars on Apple Podcast, support it on Patreon, or simply connect with me on Twitter. Alex Friedman spelled F-R-I-D-M-A-N. I recently started doing ads at the end of the introduction. I'll do one or two minutes after introducing the episode and never any ads in the middle that can break the flow of the conversation. I hope that works for you and doesn't hurt the listening experience. I provide timestamps for the start of the conversation, but it helps if you listen to the ad and support this podcast by trying out the product or service being advertised. This show is presented by CashApp, the number one finance app in the App Store. I personally use CashApp to send money to friends, but you can also use it to buy, sell, and deposit Bitcoin in just seconds. CashApp also has a new investing feature. You can buy fractions of a stock, say $1 or worth, no matter what the stock price is. Broker services are provided by CashApp Investing, a subsidiary of Square, and a member of SIPC. I'm excited to be working with CashApp to support one of my favorite organizations called First, best known for their first robotics and Lego competitions. They educate and inspire hundreds of thousands of students in over 110 countries and of a perfect rating on Charity Navigator, which means that donated money is used to maximum effectiveness. When you get CashApp from the App Store or Google Play and use code Lex Podcast, you'll get $10 and CashApp will also donate $10 to First, which again is an organization that I've personally seen inspire, girls and boys, to dream of engineering the better world. And now here's my conversation with Melanie Mitchell. The name of your new book is Artificial Intelligence, subtitle, a guide for thinking humans. The name of this podcast is Artificial Intelligence. So let me take a step back and ask the old Shakespeare question about roses. What do you think of the term, artificial intelligence for our big and complicated and interesting field? I'm not crazy about the term. I think it has a few problems because it means so many different things to different people. And intelligence is one of those words that isn't very clearly defined either. There's so many different kinds of intelligence, degrees of intelligence, approaches to intelligence. John McCarthy was the one who came up with the term, artificial intelligence and from what I read, he called it that to differentiate it from cybernetics, which was another related movement at the time. And he later regretted calling it artificial intelligence. Herbert Simon was pushing for calling it complex information and processing, which got mixed. But probably is equally vague, I guess. Is it the intelligence or the artificial in terms of words that is most problematic, you would you say? Yeah, I think it's a little of both. But it has some good size because I personally was attracted to the field because I was interested in phenomenon of intelligence. And if it was called complex information processing, maybe I'd be doing something wholly different now. What do you think of, I've heard the term used cognitive systems, for example, so using cognitive? Yeah, I mean, cognitive has certain associations with it and people like to separate things like cognition and perception, which I don't actually think are separate. But often people talk about cognition as being different from sort of other aspects of intelligence, it's sort of higher level. So to you cognition is this broad, beautiful mess of things that encompasses the whole thing. Memory, perception. I think it's hard to draw lines like that. When I was coming out of grad school in 1990, which is when I graduated, that was during one of the AI winters. And I was advised to not put AI, artificial intelligence on my CV, but instead call it intelligent systems. So that was kind of a euphemism, I guess. What about the stick briefly on terms and words? The idea of artificial general intelligence or like Gianlacune prefers human level intelligence. Sort of starting to talk about ideas that achieve higher and higher levels of intelligence and somehow artificial intelligence seems to be a term used more for the narrow, very specific applications of AI and sort of the, there's what set of terms appeal to you to describe the thing that perhaps was strive to create. People have been struggling with this for the whole history of the field and defining exactly what it is that we're talking about. You know, John Surrell had this distinction between strong AI and weak AI. And weak AI could be general AI, but his idea was strong AI was the view that a machine is actually thinking that as opposed to simulating thinking or carrying out processes that we would call intelligent. At a high level, if you look at the founding of the field of McCarthy and Surrell and so on, are we closer to having a better sense of that line between narrow, weak AI and strong AI? Yes, I think we're closer to having a better idea of what that line is. Early on, for example, a lot of people thought that playing chess would be, you couldn't play chess if you didn't have sort of general human level intelligence. And of course, once computers were able to play chess better than humans, that revised that view. And people said, OK, well, maybe now we have to revise what we think of intelligence as. And so that's kind of been a theme throughout the history of the field is that once a machine can do some task, we then have to look back and say, oh, well, that changes my understanding of what intelligence is because I don't think that machine is intelligent. At least that's not what I want to call intelligence. Do you think that line moves forever? Or will we eventually really feel as a civilization like we cross the line if it's possible? It's hard to predict. But I don't see any reason why we couldn't, in principle, create something that we would consider intelligent. I don't know how we will know for sure. Maybe our own view of what intelligence is will be refined more and more until we finally figure out what we mean when we talk about it. But I think eventually we will create machines in a sense that have intelligence. They may not be the kinds of machines we have now. And one of the things that that's going to produce is making us sort of understand our own machine like qualities that we, in a sense, are mechanical in the sense that cells are kind of mechanical. They have algorithms they process information by. And somehow out of this mass of cells, we get this emergent property that we call intelligence. But underlying it is really just cellular processing and lots and lots and lots of it. Do you think we'll be able to, do you think it's possible to create intelligence without understanding our own mind? You said, in that process, we'll understand more and more. But do you think it's possible to create without really fully understanding from a mechanistic perspective, from a functional perspective, how our mysterious mind works? If I had to bet on it, I would say, no, we do have to understand our own minds, at least to some significant extent. But I think that's a really big open question. I've been very surprised at how far kind of brute force approaches, based on, say, big data and huge networks can take us. I wouldn't have expected that. And they have nothing to do with the way our minds work. So that's been surprising to me, so it could be wrong. To explore the psychological and the philosophical, do you think we're okay as a species with something that's more intelligent than us? Do you think perhaps the reason we're pushing that line further and further is we're afraid of acknowledging that there's something stronger, better, smarter than us humans? Well, I'm not sure we can define intelligence that way, because smarter than is with respect to what, what, you know, computers are already smarter than us in some areas. They can multiply much better than we can. They can figure out driving routes to take, much faster and better than we can. They have a lot more information to draw on. They know about traffic conditions and all that stuff. So for any given particular task, sometimes computers are much better than we are. And we're totally happy with that, right? I'm totally happy with that. I don't bother me at all. I guess the question is, you know, which things about our intelligence would we feel very sad or upset that machines had been able to recreate? So in the book, I talk about my former PhD advisor, Douglas Hofstetter, who encountered a music generation program. And that was really the line for him that if a machine could create beautiful music, that would be terrifying for him, because that is something he feels is really at the core of what it is to be human, creating beautiful music, art, literature. I, you know, I don't think... He doesn't like the fact that machines can recognize spoken language really well. Like he doesn't, he personally doesn't like using speech recognition. But I don't think it bothers him to his core, because it's like, okay, that's not at the core of humanity. But it may be different for every person, what really they feel would use, serve their humanity. And I think maybe it's a generational thing also. Maybe our children or our children's children will be adapted, they'll adapt to these new devices that can do all these tasks and end. Say, yes, this thing is smarter than me in all these areas, but that's great, because it helps me. Looking at the broad history of our species, why do you think so many humans have dreamed of creating artificial life and artificial intelligence throughout the history of our civilization? So not just this century or the 20th century, but really many, throughout many centuries, that preceded it. That's a really good question. And I have wondered about that, because I myself, you know, was driven by curiosity about my own thought processes and thought it would be fantastic to be able to get a computer to mimic some of my thought processes. I'm not sure why we're so driven. I think we want to understand ourselves better, and we also want machines to do things for us, but I don't know, there's something more to it, because it's so deep in the kind of mythology or the ethos of our species. And I don't think other species have this drive. So I don't know. If you were to sort of psychoanalyze yourself in your own interest in AI, are you, what excites you about creating intelligence? Are you saying understanding our own selves? Yeah, I think that's what drives me, particularly. I'm really interested in human intelligence, but I'm also interested in the sort of the phenomenon of intelligence more generally. And I don't think humans are the only thing with intelligence, you know, or even animals, that I think intelligence is a concept that encompasses a lot of complex systems. And if you think of things like insect colonies or cellular processes or the immune system or all kinds of different biological, or even societal processes, have, as an emergent property, some aspects of what we would call intelligence. You know, they have memory, they do in process information, they have goals, they accomplish their goals, et cetera. And to me, that the question of what is this thing we're talking about here, was really fascinating to me and exploring it using computers seemed to be a good way to approach the question. So do you think kind of intelligence, do you think of our universe as a kind of hierarchy of complex systems and intelligence as just the property of any, you can look at any level and every level has some aspect of intelligence. So we're just like one little speck of intelligence. So we're just like one little speck in that giant hierarchy of complex systems. I don't know if I would say any system like that has intelligence. But I guess what I want to, I don't have a good enough definition of intelligence to say that. So let me do some of multiple choice, I guess. So you said ant colonies. So our ant colonies intelligent are the bacteria in our body intelligent and then go into the physics world, molecules and the behavior at the quantum level of electrons and so on. Are those kinds of systems do they possess intelligence? Like where is the line that feels compelling to you? I don't know. I mean, I think intelligence is a continuum and I think that the ability to in some sense have intention to have a goal, have a some kind of self-awareness is part of it. So I'm not sure if, you know, it's hard to know where to draw that line. I think that's kind of a mystery. But I wouldn't say that, say that, you know, this, the planet's orbiting the sun or is an intelligent system. I mean, I would find that that may be not the right term to describe that. And this is, you know, there's all this debate in the field of like, what's the right way to define intelligence? What's the right way to model intelligence? Should we think about computation? Should we think about dynamics? And should we think about, you know, free energy and all of that stuff? And I think that it's a fantastic time to be in the field because there's so many questions and so much we don't understand. There's so much work to do. So are we, are we the most special kind of intelligence in this kind of, you said there's a bunch of different elements and characteristics of intelligence systems and colleagues? Are, is human intelligence the thing in our brain? Is that the most interesting kind of intelligence in this continuum? Well, it's interesting to us because it is us. I mean, interesting to me. Yes. And because I'm part of, you know, human. But to understanding the fundamentals of intelligence but I'm getting it, do we, studying the human, is sort of, if everything we've talked about, will you talk about in your book, what, just the AI field, this notion, yes, it's hard to define, but it's usually talking about something that's very akin to human intelligence. Yeah, to me, it is the most interesting because it's the most complex, I think. It's the most self-aware. It's the only system, at least, that I know of, that reflects on its own intelligence. And you talk about the history of AI and us, in terms of creating artificial intelligence, being terrible at predicting the future, with AI, with tech in general. So why do you think we're so bad at predicting the future? Are we hopelessly bad? So no matter what, or there's this decade, or the next few decades, every time we make a prediction, there's just no way of doing it well, or as the field matures will be better and better at it. I believe as the field matures, we will be better. And I think the reason that we've had so much trouble is that we have so little understanding of our own intelligence. So there's the famous story about Marvin Minsky assigning computer vision as a summer project to his undergrad students. And I believe that's actually true story. Yeah, no, there's a write-up on it, because everyone should read. I think it's like a proposal that describes everything that should be done in that project in Scolaris, because it, I mean, you could explain it, but for my recollection, it describes basically all the fundamental problems of computer vision, many of which that still haven't been solved. Yeah, and I don't know how far they really expected to get, but I think that, and they're really, you know, Marvin Minsky is a super smart guy and very sophisticated thinker. But I think that no one really understands or understood still doesn't understand how complicated, how complex the things that we do are because they're so invisible to us, you know, to us vision being able to look out at the world and describe what we see, that's just immediate. It feels like it's no work at all, so it didn't seem like it would be that hard, but there's so much going on unconsciously, sort of invisible to us, that I think we overestimate how easy it will be to get computers to do it. And so, sort of, for me to ask an unfair question, you've done research, you've thought about many different branches of AI through this book, widespread looking at where AI has been, where it is today. If you were to make a prediction, how many years from now, would we as a society create something that you would say achieved human level intelligence, or super human level intelligence? That is an unfair question. A prediction that will most likely be wrong, so but it's just your notion because... Okay, I'll say more than 100 years. More than 100 years. And I quoted somebody in my book who said that human level intelligence is 100 Nobel Prizes away, which I like, because it's a nice way to sort of, it's a nice unit for prediction. And it's like that many fantastic discoveries have to be made. And of course, there's no Nobel Prize in AI, not yet at least. If we look at that 100 years, your sense is really the journey to intelligence has to go through something more complicated as again to our own cognitive systems, understanding them, being able to create them in the artificial systems as opposed to sort of taking the machine learning approaches of today and really scaling them and scaling them exponentially with both compute and hardware and data. That would be my guess. I think that in the sort of going along in the Nero AI, that these current approaches will get better, I think there's some fundamental limits to how far they're going to get. I might be wrong, but that's what I think. And there's some fundamental weaknesses that they have, that I talk about in the book, that just comes from this approach of supervised learning requirements. Requiring sort of feed forward networks and so on. It's just, I don't think it's a sustainable approach to understanding the world. Yeah, I'm personally torn on it. So, I've, everything you read about in the book, and sort of we're talking about now, I agree with you, but I'm more and more depending on the day. First of all, I'm deeply surprised by the success of machine learning and deep learning in general. From the very beginning, when I was, it's really been my main focus of work. I'm just surprised how far it gets. And I'm also think we're really early on in these efforts of these Nero AI. So, I think there will be a lot of surprises of how far it gets. I think will be extremely impressed. Like, am I senses everything I've seen so far, and we'll talk about autonomous driving and so on, I think we can get really far. But I also have a sense that we will discover, just like you said, is that even though we'll get really far, in order to create something like our own intelligence, it's actually much farther than we realize. Right. I think these methods are a lot more powerful than people give them credit for, actually. So, of course, there's the media hype. But I think there's a lot of researchers in the community, especially, like, not undergrads. Right. But like, people who've been in AI, they're skeptical about how far you're playing and get. And I'm more and more thinking that it can actually get farther than we'll realize. It's certainly possible. One thing that surprised me when I was writing the book is how far apart different people are in the field are. They're opinion of how far the field has come and what is accomplished and what's going to happen next. What's your sense of the different who are the different people, groups, mindsets, thoughts in the community about where AI is today? Yeah, they're all over the place. So, there's kind of the singularity, transhumanism group. I don't know exactly how to characterize that approach, which is. As well. Yeah. The sort of exponential, exponential progress. We're on the sort of almost at the hugely accelerating part of the exponential. And by in the next 30 years, we're going to see super intelligent AI and all that. And we'll be able to upload our brains and that. So, there's that kind of extreme view that most, I think most people who work in AI don't have, they disagree with that. But there are people who are maybe don't, you know, singularity people, but they do think that the current approach of deep learning is going to scale. And it's going to kind of go all the way basically and take us to true AI or human level AI or whatever you want to call it. And there's quite a few of them. And a lot of them. Like a lot of the people I met who work at big tech companies in AI groups kind of have this view that we're really not that far. Just to linger on that point sort of if I can take as an example, like Jan LeCoon, I don't know if you know about his work and so a few points on this. I do. He believes that there's a bunch of breakthroughs like fundamental like no well prizes. There's needed still. Right. But I think he thinks those breakthroughs will be built on top of deep learning. Right. And then there's some people who think we need to kind of put deep learning to the side a little bit as just one module that's helpful in the big. Or cognitive framework. Right. So, so, so I think. So what I understand Jan LeCoon is. Rightly saying supervised learning is not sustainable. We have to figure out how to do unsupervised learning. That that's going to be the key. And. You know, I think that's probably true. I think unsupervised learning is going to be harder than people. I mean, the way that we humans do it. Then there's the opposing view, you know, there's the the Gary Marcus kind of hybrid view or where deep learning is one part, but we need to bring back kind of these symbolic approaches. And combine them. Of course, no one knows how to do that very well. Which is the more important part. Right. To emphasize and how do they yeah, how do they fit together? What's what's the foundation? What's the thing that's on top? What's the cake? What's the icing? Right. Yeah. Then there's people pushing different different things. There's the people, the causality people who say, you know, deep learning as it's formulated today completely lacks any notion of causality. And that's. And that's what it's like. Dooms it. And therefore we have to somehow give it some kind of notion of causality. There's a lot of push from the more cognitive science crowd saying. We have to look at developmental learning. We have to look at how babies learn. We have to look at intuitive physics. All these things we know about physics and somebody kind of equipped. We also have to teach machines intuitive metaphysics, which means like objects exist. There causality exists. You know, these things that maybe were born with. I don't know that they don't have the machines don't have any of that. You know, they look at a group of pixels and they maybe they get. They're not just a lot of millions of millions examples, but they. They can't necessarily learn that there are objects in the world. So there's just a lot of pieces of the puzzle that people are promoting. And with different opinions of like how, how important they are and how close we are to. You know, we'll put them all together to create general intelligence. What do you take away from it? Who is the most impressive? Is it the cognitive folks, the Gary Marcus camp, the young camp. It's unsupervised and they're self-supervised. There's the supervised. And then there's the engineers who are actually building systems. Sort of the Andre Carpati and Tesla building actual. You know, it's not philosophy. It's real like systems that operate in the real world. What, yeah, what do you take away from all this beautiful. I don't know if you know these, these different views are not necessarily mutually exclusive. And I think people like Yan LeCoon agrees with the developmental psychology, causality, intuitive physics, etc. But he still thinks that it's learning like end to end learning is the way to go. We'll take us perhaps all the way. I mean, there's no sort of innate stuff that has to get built in. This is, you know, it's because it's a hard problem. I personally, you know, I'm very sympathetic to the cognitive science side, because that's kind of where I came into the field. I've become more and more sort of an embodiment adherent saying that, you know, without having a body, it's going to be very hard to learn what we need to learn about the world. That's definitely something I'd love to talk about in a little bit. To step into the cognitive world, then if you don't mind, because you've done so many interesting things, if you look to Copycat, taking a couple of decades step back, you'd Douglas Hofstetter and others have created and developed Copycat more than 30 years ago. That's painful to hear. What is it? What is Copycat? It's a program that makes analogies in an idealized domain, idealized world of letter strings. So as you say, 30 years ago, wow. So I started working on it when I started grad school in 1984. Wow. It dates me. And it's based on Doug Hofstetter's ideas about that analogy is really a core aspect of thinking. I remember he has a really nice quote in the book by by himself and Emmanuel Sandor called Surfaces and Essences. I don't know if you've seen that book, but it's about analogy. He says, without concepts, there can be no thought and without analogies, there can be no concepts. So the view is that analogy is not just this kind of reasoning technique where we go, you know, shoe is to foot as glove is to what? You know, these kinds of things that we have on IQ tests or whatever. But that it's much deeper. It's much more pervasive in everything we do in every our language. Our thinking, our perception. So we so he had a view that was a very active perception idea. So the idea was that instead of having kind of what a passive network in which you have input that's being processed through these feed forward layers. And then there's an output at the end that perception is really a dynamic process. You know, where like our eyes are moving around and they're getting information. And that information is feeding back to what we look at next influences what we look at next and how we look at it. And so copycat was trying to do that kind of simulate that kind of idea where you have these agents. It was kind of an agent based system and you have these agents that are picking things to look at. And deciding whether they were interesting or not, whether they should be looked at more. And that would influence other agents. How do they interact? So they interacted through this global kind of what we call the workspace. So it's actually inspired by the old blackboard systems where you would have agents that post information on a blackboard, a common blackboard. This is like very old fashioned a us. Is that we're talking about like in physical space is this a computer program? It's a computer program. So agents posting concepts on a blackboard. Yeah, we called it a workspace. And it it it the workspace is a data structure. The agents are little pieces of code that you can think of them as detect little detectors or little filters that say I'm going to pick this place to look and I'm going to look for certain thing. And it's just the thing I I think is important is it there. So it's almost like, you know, the convolution in a way except a little bit more general and saying and then highlighting it on the on the work in the workspace. What what what what's it once it's in the workspace. How does the things that are highlighted relate to each other like what's there's so there's different kinds of agents that can build connections between different things. So just to give you a concrete example what copycat did was it it made analogies between strings of letters. So here's an example ABC changes to ABD. What does IJK change to and the program had some prior knowledge about the alphabet knew the sequence of the alphabet. It had a concept of letter of successor of letter it had concepts of sameness. So it has some innate things programmed in. But then it could do things like say discover that ABC is a group of letters in succession. And then some an agent can mark that. So the idea that there could be a sequence of letters is that a new concept that's formed or that's a concept that's innate. So if can you form new concepts or all concepts in eight. So in this program all the concepts of the program were innate. So because we weren't I mean obviously that limits it quite quite a bit. But what we were trying to do is say suppose you have some innate concepts. How do you flexibly apply them to new situations. And how do you make analogies? Let's step back for seconds. I really like that quote that you said without concepts there could be no thought and without analogies that could be no concepts. In a Santa Fe presentation you said that it should be one of the mantras of AI. Yes. And that you all see yourself said how to form and fluidly use concept is the most important open problem in AI. Yes. How to form and fluidly use concepts is the most important open problem in AI. So let's what is a concept and what is an analogy? A concept is in some sense a fundamental unit of thought. So say we have a concept of a dog. Okay. And a concept is embedded in a whole space of concepts so that there's certain concepts that are closer to it or farther away from it. Are these concepts, are they really like fundamental like we mentioned in Nate, almost like ex-yamatic, like very basic and then there's other stuff built on top of it. Or just include everything is are they're complicated. You can certainly have formed new concepts. Right. I guess that's the question. Yeah. Can you form new concepts that are. Combined complex combinations of other concepts. Absolutely. And that's kind of what we we do in learning. And then what's the role of analogies in that. So analogy is when you recognize that one situation is essentially the same as another situation. And essentially is kind of the keyword there because it's not the same. So if I say. Last week I did a podcast interview in actually like three days ago in Washington DC. And that situation was very similar to this situation, although it wasn't exactly the same, you know, it was a different person sitting across from me. We had different kinds of microphones. The questions were different. The building was different. There's all kinds of different things, but really it was analogous. Or I can say. So so doing a podcast interview that's kind of a concept. It's a new concept. You know, I never had that concept before. Essentially. I mean, and I can make an analogy with it. Like being interviewed for a news article in a newspaper. And I can say, well, you kind of play the same role that the newspaper reporter played. It's not exactly the same. Because maybe they actually emailed me some written questions rather than talking. And the writing, the written questions played the, you know, are analogous to your spoken questions. And you know, there's just all kinds of. And somehow probably connects to conversations you have over Thanksgiving dinner. Which is general conversations. You can there's like a thread. You can probably take. That just stretches out in all aspects of life that connected this podcast. Sure conversations between humans. Sure. And if I go and tell a friend of mine about this podcast interview. My friend might say, oh, the same thing happened to me. You know, let's say, you know, you ask me some really hard question. And I have trouble answering it. My friend could say, the same thing happened to me. But it was like, it wasn't a podcast interview. It wasn't. It was a completely different situation. And yet, my friend is seen essentially the same thing. You know, we say that very fluidly. The same thing happened to me. Essentially the same thing. But we don't even say that. Right. Yeah. And the view that kind of went into, say, a coffee cat, that whole thing is that. That that that act of saying the same thing happened to me is making an analogy. And in some sense, that's what's underlies all of our concepts. Why do you think analogy making that you're describing is so fundamental to cognition? Like, it seems like it's the main element action of what we think of as cognition. Yeah. So it can be argued that all of this generalization we do of concepts. And recognizing concepts in different situations is done by analogy. That's every time I'm recognizing that say you're a person. That's by analogy, because I have this concept of what person is and I'm applying it to you. And every time I recognize a new situation, like one of the things I talked about in the book was the concept of walking a dog. That's actually making an analogy because all of that, you know, the details are very different. So, so so, so reasoning could be reduced on to censor analogy making. So all the things we think of as like, yeah, like you said, perception. So what's perception is taking raw sensory input and it somehow integrating into our, our understanding of the world, updating the understanding. And all of that has just this giant mess of analogies that are being made. I think so, yeah. If you just linger on it a little bit, like what do you think it takes to engineer a process like that for us in our artificial systems? We need to understand better, I think, how, how we do it, how humans do it. And it comes down to internal models, I think, you know, people talk a lot about mental models that concepts are mental models that I can, in my head, I can do a simulation of a situation like walking a dog. And that, there, there's some work in psychology that promotes this idea that all of concepts are really mental simulations that whenever you encounter a concept or situation in the world or you read about it or whatever, you do some kind of mental simulation that allows you to predict what's going to happen to develop expectations of what's going to happen. So that's the kind of structure I think we need is that kind of mental model that, and in our brains, somehow these mental models are very much interconnected. Again, so a lot of stuff we're talking about is essentially open problems, right? So if I ask a question, I don't mean that you would know the answer, I'm just hypothesizing, but how big do you think is the network graph data structure of concepts that's in our head? Like if we're trying to build that ourselves, like it's, we take it, there's one of the things we take for granted, we think, I mean, that's why we take common sense for granted, we think common sense is trivial. But how big of a thing of concepts is that underlies what we think of as common sense, for example. Yeah, I don't know, and I'm not, I don't even know what units to measure it in. You say how big is it? That's beautifully put, right? But you know, we have, you know, it's really hard to know, we have what, 100 billion neurons or something, I don't know. And they're connected via trillions of synapses. And there's all this chemical processing going on. There's just a lot of capacity for stuff. And there are information encoded in different ways in the brain. It's encoded in chemical interactions as encoded in electric, like firing and firing rates. And nobody really knows how it's encoded, but it just seems like there's a huge amount of capacity. So I think it's huge, it's just enormous. And it's amazing how much stuff we know. Yeah. But we know, and not just know, like facts, but it's all integrated into this thing that we can make analogies with. Yes. And there's a lot of dreams from expert systems of building giant knowledge bases. Do you see a hope for these kinds of approaches of building, of converting Wikipedia into something that could be used in analogy making? Sure. And I think people have made some progress along those lines. I mean, people have been working on this for a long time. And this I think is the problem of common sense, like people have been trying to get these common sense networks here at MIT. There's this concept net project. But the problem is that as I said, most of the knowledge that we have is invisible to us. It's not in Wikipedia. It's very basic things about, you know, intuitive physics, intuitive psychology, intuitive metaphysics, all that stuff. If you were to create a website that's described intuitive physics, intuitive psychology, would it be bigger or smaller than Wikipedia? What do you think? I guess describe to whom. Sorry, but that's no, it's really good. I exactly right. Yeah. That's a hard question because, you know, how do you represent that knowledge is the question, right? I can certainly write down F equals MA and all Newton's laws and a lot of physics can be deduced from that. But that's probably not the best representation of that knowledge for for doing the kinds of reasoning we want a machine to do. So, so I don't know it's impossible to say now. And people, you know, the projects like there's a famous, the famous psych project, right? That Douglas Lennart did that was trying to still going. I think it's still going. And if the idea was to try and encode all of common sense knowledge, including all this invisible knowledge in some kind of logical representation. And it just never I think could do any of the things that he was hoping it could do because that's just the wrong approach. Of course, that's what they always say, you know, and then the history books will say, well, the psych project finally found a breakthrough in 2058 or something. You know, we're so much progress has been made in just a few decades that it could be who knows what the next breakthroughs will be. It could be. It's certainly a compelling notion what the psych project stands for. I think Lennart was one of the earliest people to say common sense is what we need. That's what we need. All this like expert system stuff that is not going to get you to AI. You need common sense. And he basically gave up his whole academic career to go pursue that. I totally admire that, but I think that the approach itself will not in 2020 or 20 or 40 or whatever. What do you think is wrong with approach? What kind of approach would might be successful? Well, again, nobody knows the answer. I knew that. You know, one of my talks, one of the people in the audience is a public lecture, one of the people in the audience said, what AI companies are you investing in? Investment advice. I'm a college professor for one thing. So I don't have a lot of extra funds to invest, but also like no one knows what's going to work in AI. Right. That's the problem. Let me ask another impossible question in case you have a sense in terms of data structures that will store this kind of information. I think they've been invented yet, both in hardware and software. Or is something else needs to be? Are we totally, you know, I think something else has to be invented. I that's my guess is the breakthroughs that's most promising. Would that be in hardware or software? Do you think we can get far with the current computers? Or do we need to do something that you're saying? So if a certain kind of term computation is going to be sufficient, probably I would guess it will. I don't see any reason why we need anything else. But so in that sense, we have invented the hardware we need, but we just need to make it faster and bigger. And we need to figure out the right algorithms and the right sort of architecture. So if you're wondering, that's a very mathematical notion when we have to build intelligence, it's not an engineering notion where you throw all that stuff. Well, I guess it is a question that people have brought up this question, you know, and when you asked about like is our current hardware. Will our current hardware work? Well, to turn computation says that like our current hardware. It is in principle, a Turing machine, right? So all we have to do is make it faster and bigger. But there have been people like Roger Penrose, if you might remember that he said Turing machines cannot produce intelligence because intelligence requires continuous valued numbers. And that's sort of my reading of his argument and quantum mechanics and what else, whatever, you know, but I don't see any evidence for that that we need new computation paradigms. But I don't know if we're, you know, I don't think we're going to be able to scale up our current approaches to programming these computers. What is your hope for approaches like copycat or other cognitive architectures? I've talked to the greater of SOAR. For example, I've used that to arm myself. I don't know if you're familiar with it. Yeah, I am. What do you think is, what's your hope of approaches like that in helping develop systems of greater and greater intelligence in the coming decades? Well, that's what I'm working on now is trying to take some of those ideas and extending it. So I think there are some really promising approaches that are going on now that have to do with more active, generative models. So this is the idea of this simulation in your head of a concept when you, if you want to, when you're perceiving a new situation, you have some simulations in your head. Those are generative models are generating your expectations, they're generating predictions. So that's part of a perception. You have a mental model that generates a prediction, then you compare it with, yeah, and then the difference. You also, that that generative model is telling you where to look and what to look at and what to pay attention to. And it, I think it affects your perception. It's not that just you compare it with your perception. It, it becomes your perception in a way. It, it, it, it, it's kind of a mixture of, of the bottom up information coming from the world and your top down model being opposed on the world is what becomes your perception. So your hope is something like that can improve perception systems and that they can understand things better. Yes. Yes. Yes. What's the, what's the step, what's the analogy making step there? Well, there, the, the idea is that you have this pretty complicated conceptual space. You know, you can talk about a semantic network or something like that with these different kinds of concept models in your brain. And that are connected. So, so let's, let's take the example of walking a dog. So we're talking about that. Okay. Let's see, I see someone out in the street walking a cat. Some people walk their cats, I guess. Yes. Seems like a bad idea, but. Yeah. So my model, my, you know, there's connections between my model of a dog and model of a cat. And I can immediately see the analogy of that those are analogous situations. But I can also see the differences and that tells me what to expect. So, also, you know, I have a new situation. So another example with the walking the dog thing is sometimes people, I see people riding their bikes with the leash holding the leash and the dogs running alongside. Okay. So I know that the, I recognize that as kind of a dog walking situation, even though the person's not walking, right. And the dogs not walking. Because I, I have the these models that say, okay, riding a bike is sort of similar to walking or it's connected. It's a means of transportation. Because they have their dog there. I assume they're not going to work, but they're going out for exercise. And, you know, these analogies help me to figure out kind of what's going on. What's likely. But sort of these analogies are very human interpretable. So that's that kind of space. And then you look at something like the current deep learning approaches that kind of help you to take raw sensory information and to sort of automatically build up hierarchies of, of what you can even call them concepts. They're just not human interpretable concepts. What's your, what's the link here? Do you hope it's sort of the hybrid system question. How do you think that you can start to meet each other? What's the value of learning in this systems of forming of analogy making? The goal of, you know, the original goal of deep learning in, in, in, in visual perception was that you would get the system to learn to extract features that at these different levels of complexity. So maybe edge detection and that would lead into learning, you know, simple combinations of edges and then more complex shapes and then whole objects or faces. And this was based on the ideas of the neuroscientists, hewyl and weasel who had seen laid out this kind of structure and brain. And I think that is, that's right to some extent. Of course, people have come, found that the whole story is a little more complex than that in the brain, of course, always is and there's a lot of feedback. So I see that as, as absolutely a, a good brain inspired approach to some aspects of perception, but one thing that it's lacking. For example, is all of that feedback, which is extremely important. The interactive element, you mentioned the expectation, right, the conceptual level. So let's go back and forth with the, the expectation and the perception and just go back and forth. So, right. So that is extremely important. And, you know, one thing about deep neural networks is that in a given situation, like, you know, they, they're trained, right, they get these ways.

 I guess the jumping off point is this long running debate about where memories are stored in the brain. And it's a profound question, you know, something that people have struggled with for many decades at this point. Like, this really gives me the insight on, you know, why a lot of episodic memories are being kept in the hap campus and require the hap campus. It's just a lot of it is because the world is so complex. I definitely think that, you know, we wouldn't have gotten to where we currently are in AI without past generations of theoretical neuroscience research. And I also definitely think that projects like this where we try to kind of boil it down to the essentials and really analyze everything very rigorously and really try to figure out to what extent this relates to the biological brain will provide useful seeds for future AI research. This is Brain Inspired. Hey everyone, it's Paul. Today I have three fine folks on the podcast. James Fitzgerald, Andrew Sachs, and Wynnon soon. James and Wynnon are both at the Janelia Research Campus at the Howard Hughes Medical Institute. James is a group leader and Wynnon is a research scientist in Nelson Spruestens lab. And Andrew is a joint group leader at University College London. Andrew's been on the podcast before when we discussed his work on deep learning theory on episode 52. And you'll learn more about what James and Wynnon do in a moment. The reason I'm speaking with them today is their recent theoretical neuroscience preprint with the title, organizing memories for generalization in complementary learning systems. So we've discussed complementary learning systems a few times on the podcast before. The general idea is that we have a fast learning system in the hippocampus that rapidly encodes specific memories. And we have a slower learning system in our neocortex where over time and through mechanisms like replay from the hippocampus, memories get consolidated. So in their new paper, they build on complementary learning systems and suggest that these two learning and memory systems might work better together in a way that optimizes generalization, which is a good thing if you want to function well in our topsy-turvy world. And one of the big takeaways is that how much consolidation should happen from hippocampus to cortex depends on how predictable the experiences are that your brain is trying to remember. So in unpredictable environments, you want to cut off the consolidation pretty early. In predictable environments, you want to let consolidation run for longer. And that's of course a very simplified explanation which gets elaborated during the podcast. So they built a model to explore that hypothesis. And we discuss many topics around their model and related phenomena. I'll link to the paper and the show notes at braininspired.co slash podcast slash 120 120. There's also a guest question from Mike, a Patreon supporter who actually pointed me to the work that we're discussing. So thanks Mike for the question and pointing me to it and thank you to all my Patreon supporters. By the way, during my little post recording check-in with James Andrew and Wainon asking if everything went okay, James mentioned that it felt a lot like a conversation they would have in their regular weekly lab meetings. So if you're wondering what their lab meetings might be like, here you go. Andrew, James, Wainon, thanks for being on the show here. So what we're going to do to start off is I'm going to ask you guys to each introduce yourselves. Andrew, let's start with you because you were on the podcast before. You were talking about deep learning theory. And ever since you're on, of course, you've been emailing me. Can I please come back on? Can I please come back on? And finally, we have you back on here. So Andrew, who are you? Yeah. So I'm a joint group leader at the Gatsby Unit and says we're a welcome center at UCL. And I'm interested in deep learning theory and ways that can inform theories of psychology and neuroscience. James, would you like to go next? Sure. Yeah, my name is James Fitzgelde. I'm a group leader at the Schnellian Research Campus, which is part of the Howard Hughes Medical Institute. So I'm a theoretical neuroscientist. I'm actually very broadly interested in a lot of different things. So in addition to the learning and memory stuff, we'll talk about today. I also work on small animals, like zebrafish and fruit flies. And I'm very collaborative. So I like to work with diverse people coming from all sorts of different perspectives. I think that's one of the most fun parts of doing science. I'm going to ask about the collaboration in just a minute here. Wayne on. Hi. Hi, Paul. It's great to finally meet you. Yeah, you too. My name's Wayne on. I'm currently a senior postdoc in Nelson's Proustance Lab at Jeanine. So I joined Jeanine. Four years ago after doing eye on channel biophysics and snaptic transmission studies for seven years. After joining Jeanine, I just wanted to sort of step up and get a bigger picture, kind of more of frameworks thinking on neuroscience. So I decided to, okay, let me do theory and experiments together. So it has been a pleasure to collaborate with Andrew and James on this project. Yeah. So this project is pretty much all theory, right? So the title of the paper that we're going to talk about is organizing memories for generalization and complementary learning systems. Before we talk about the theory, I kind of want to ask, well, how did this collaboration come about? And anyone can jump in and answer because, and tied into that is the idea of, of Jeanelia. And I'm curious, you know, it seems like Jeanelia specifically is highly collaborative. So I don't know if that's a factor in this as well. Yeah, for sure. I mean, so I think, you know, as you just heard, so Wayne on's actually in Nelson's Proustance Lab at Jeanine. So none of us are actually in the same lab anyway. So Jeanelia labs are very small to kind of try to encourage collaboration. So the idea being that, you know, no one lab has enough people or all the expertise you would want to kind of achieve the project's goals. Is that built in though as a principle when forming it? It is. Yeah, it is. Yeah, so that's why they've kept the lab small. Is it kind of, you know, really encourage people to interact with each other and collaborate. So so Wayne on and I kind of arrived at Jeanelia at almost the same time and, you know, at the time my lab was completely empty. So Wayne on was really kind of my first main postdoc collaborative at Jeanelia, even though he didn't have any position in my lab. So I don't know, Wayne, do you want to kind of reflect a little bit on the early days? Oh, yeah. That was interesting story. So in 2017 I joined and that's exactly one James just joined. I was just in a sort of, you know, experimental crisis mode, like I've been doing so many experiments and, and I was trying to decide what to do here in Virginia. I know I joined Nelson's lab just to study some more single neuron computations, but that's one deep mind released there a few ago and it was having a big splash in the world. And I was a long time go player. So that just really shocked me how human like the moves are. And I would just start to think, okay, so where can I get frameworks to inform like all the data collected? I think I need to collaborate with theorists and I need to combine neuroscience with AI. So and then James and I started to talk and I was generally interested in complementary learning systems. So the seminar work by Jay, by Kleylen. And then we started to talk and then James was interested. Okay, so why don't we just start with modeling C a one because he his recent findings show that C one spines are very transient. So it turns over every two weeks. In hippocampus. I'll just interject in hippocampus. Yeah. Mm-hmm. Yeah, so that quick, I find that that makes James suggest that there might be a form of regularization like weight decay, machine learning. So that's why what got us started. That was more than that. And I don't want to get this too long, but later on James, just I remember that really vividly. He drew on the board that, okay, do you know what happens if you have noise in the data? If you're trained on the training data and the generalization error is that your performance on new data will start to decrease and that will start to rise up. And I remember being an expertly, so I was shocked by that factor. That's why. Is that okay, you need regularizations in order for training to not overfit. Mm-hmm. Yeah, I think that's just interject a little bit. I think that's actually great segue in also to how Andrew got involved in all this stuff, which is that Andrew and I go back a long, long way. So we would grab students together, we knew each other way back then, and then we were actually postdocs also together in the same program. And so we've known each other for a very long time, but we hadn't been collaborating. And while Andrew was in his postdoc, he was working with Medoo, another author on this paper, very precisely analyzing the amount of learning in a batch system that would be optimal for generalization. And so it was very much in my mind the importance of thinking about legalization in part by interactions with Andrew in a non-collaborative way. But then once Wayne, on and I started to think about the benefits that this type of regulation might provide for learning systems, we decided to be really fond of this event. Andrew would also be interested because we knew that he had overlapping interests too. And that's when we kind of bought Andrew and everything, and we had Andrew come to Janelia, we planned a visitor project, and then we wrote it up and we've been working together for the last couple years on this. So yeah, in the early days, but Wayne and I were modeling. We were thinking about the role of transient memories in the hippocampus and what that might do to kind of aid system-wide function in a complementary learning systems typeway. But we didn't actually have any kind of explicit, cortical model in those early days. And there was only once Andrew started to get involved that we kind of really started to build a integrated model of the whole system based on the insights that he's had by kind of thinking about learning and deepening your networks. Andrew, you're not at Janelia, so I don't know if you're being pulled in a thousand different directions by lots of people that want to collaborate with you. So yeah, where's your threshold? How'd you get sucked into collaboration with these guys? Yeah, I was just thinking, I'm glad you two remember how this started. It's sort of ironic that we're studying episodic memory, but I really don't remember quite how it all came together. But yeah, I mean, I guess the memories that are recurring are, yeah, thinking about deep learning. I have this structure that I'm hoping will be replicable across many experimental domains. The idea is first come up with some basic insights into how deep networks work. Just treat them on their own terms. At that point, it looks basically like what a computer scientist or physicist would be doing. And then if you can get some durable insights into those systems, hopefully that offers interesting hypotheses for certain experimental domains. And in this case, we initially did work on generalization error. And then it seemed like this could potentially shed light on hippocampal consolidation. And we actually had a cosine poster on this too, just to do a knife that didn't go nearly as far as this paper, but it was some of the early seeds of it. And then I think James and I probably just did hardware James. Yeah, I actually, I really couldn't reconstruct actually how. I even looked for old emails to get out of this, got started. But I do remember, I do remember when you visited us at Genelia. And at that point presumably, we already knew we were planning to collaborate. Or maybe we just had to come for a talk actually. Maybe we're just going to come and give a talk. But in any case, I remember us taking a walk along Selden Island. So Genelia is right on the Potomac and one of the odd but wonderful things about us is we have a private island in the Potomac that does a footbridge. And it's just well of this. There's nothing that are exactly like a field. And so, Andrew and I were walking and way not night had already been working on this stuff. And of course, Andrew had been doing this stuff with me, too. And we were just discussing, like, well, everybody talks about complimentary learning systems. And it's kind of implicit a lot of what people say that they think that the point of having the cortex is for generalization. But do people actually realize the danger of overfitting in these systems? And we were kind of debating this back and forth a lot. Because on the one hand, we were like, well, it seems like they kind of should, if they think about it from the viewpoint of machine learning. But at the same time, it didn't actually seem like anybody had been thinking through the consequences of that about kind of well, then you really need to regulate the amount of transfer. You can't have it that you just kind of fully transfer memories from the hippocampus to the Neocortex. And then I think that these conversations also in terms of way not, I can remember that same visit. Him getting very excited by the generalization angle, you know, before that visit, we were thinking about many different things about the benefits of kind of transient hippocampal traces. Also, in terms of things like memory capacity, in the hippocampus and stuff like that. But I think after that visit, we all kind of consolidated around this fundamental importance of, you know, if you build the system for generalization, there's going to be some new requirements that, you know, people have not been thinking through from the perspective of neural networks. And I think one of the most fulfilling aspects of this collaboration is that at this point, the ideas are so jointly well-fined that it's like, it really was one of those, you know, this wonderful times where you're just everyone's riffing off of each other and it somehow comes out to be this thing that's greater than that. Some of the parts. Well, that's interesting. I didn't know the connection James between you and Andrew because I didn't do my homework about reading your CVs, I suppose. But I believe it was Wayna, actually, who first recommended Andrew to come on the podcast way back when. So I know that there's a connection there as well. Yeah. It's great to you, Wayna. Well, let's talk about the big idea in the paper and then we can kind of unpack it from there because you've already hinted at some of the, some of what it's about. So I don't know who would like to start and give like a really high level overview. And then we can talk about complimentary learning systems and just go on down the list there. Sure. Well, maybe I can. So I guess the jumping off point is this long running debate about where memories are stored in the brain. And it's a profound question is something that people have struggled with for many decades at this point. And the data from neuropsychology is riveting. The patient HM, for instance, lost his hippocampus and other MTL structures and just can't form new memories. But even more striking, if you look into the memories before he had this resection operation, a lot of those memories are damaged as well. So the damage went back into the past, basically. So that's retrovade amnesia. And what that suggests is that there's some process by which memories might initially be stored in the hippocampus, but ultimately transfer out or re-duper-capt themselves in other parts of the brain. But this just raises all kinds of questions. Why would you have a system set up like this? Why do you need multiple memory systems to start with? Why couldn't you have these memories stored in all places across the brain? And there's a raging debate about this topic. And so when we were looking at this, we were trying to find ideas that might have been overlooked and looking at machine learning. You can see that there's this very interesting phenomenon that if you're training from a fixed set of data, a fixed batch of data, and you're going through it again and again, then the neuropsychoncies of that data can cause you to learn spurious relationships. And so too much learning from the same fixed batch of data can be counterproductive. And so we thought maybe this was relevant to systems consolidation. If you think of that batch of data as the experiences you stored in your hippocampus, what it's saying is that there's only so much replay you should do to sort of try to include those memories into neocortex because if you did too much, you would not be learning the general rule that's in that data set. And so that, we think, can start to make sense of a lot of these empirical puzzles that have been out there. Yeah, so maybe to just elaborate on that a little bit. So one of the big empirical puzzles is that because of patients like HM, there's been what's called the standard theory of systems consolidation, which says that in the beginning, everything is encoded into hippocampus, but then over time, everything is consolidated into the cortex and it becomes hippocampus independent. And there's a lot of data to support that from both humans and from animals, but over time, there's also been a growing body of literature that conflicts with that and suggests that in both humans and in animals, certain types of memories do permanently require the hippocampus. And so there's been kind of a conceptual shift in the field where people start to think about consolidation, not just as something that happens over time, but that has something to do with the content of memory. And there's all sorts of conceptual ideas about what that content might be and why it is that certain things do require the hippocampus permanently. But again, funnest perspective of generalization and neural networks, we thought we might be able to make this very concrete about kind of when you can and when you can't take something that was initially encoded into hippocampus and gradually make it hippocampus independent. So you've taken complementary learning systems, which we've already talked about a bit, and essentially the theory is generalization optimized complementary learning systems is the name of the, I guess, is it the theory or the model setup or is that interchangeable? I think there's sort of actually two levels that you can view this work on. The first is a formalism that could let you model many other kinds of consolidation theories. So we have this particular mathematical framework. You can instantiate the standard theory, you can instantiate generalization optimized, complement learning systems, there are many others. And so at that level, it sort of lets you understand the consequences of different choices about how these memory systems interact. But then the one that looks good to us just looking at the data, yes, is generalization optimized complemented learning systems. So what is the take home of what's new about generalization optimized CLS versus, and you may be repeating yourself, so I apologize. So maybe the way I'd characterize the original CLS idea is that there are benefits from a rapid learning system and a slow learning system. And a lot of those benefits that were highlighted in the context of the original complementary learning systems idea was that having a fast learning system that can record and memorize examples could allow the slow learning system to interleave those examples during learning and prevent what they called catastrophic interference. And so you'd be able to use the fast system to record the memory as it comes, but then slowly train up a slow learning system based on those experiences. And the idea would then be that in this slow learning system, you get some sort of representation that is kind of generalized over the various training examples that you've seen. So in some sense, I think generalization has always been an important part of thinking about the complementary learning systems framework. But what is new in how we set things up is that we have an explicit generative model for the environment that allows us to consider the possibility that there are unpredictable elements in that environment or noise if you set it up in kind of an abstract way. And because of this, what we show that had not been kind of considered in earlier complementary learning systems models is that in the presence of noise, it's not always ideal actually for the slow learning system to learn forever. But at some point, you actually have to stop learning to avoid overfitting to this noise, which again, from the viewpoint of machine learning makes some other sense. But in the setting of conventional complementary learning systems problems, you're learning from these data that don't actually have any noise. It's just kind of these very reproducible, very reliable cognitive relationships. And as a consequence, there's no tension between what we'd call a generalization and what we'd call a memorization, the ability of the system to recall those examples versus kind of deal with cognitively or semantically similar examples going forward. But once you add noise, you break that and it starts to become actually that you have to make a choice. That, you know, well, what is it that you want the slow learning system to do? Do you want it to be able to faithfully reproduce the past, which is what we would call the standard model of systems consolidation? Or do you want it to actually do as well as it possibly could in anticipating new experiences from the environment that could incur the future? And that's what we would be my generalization. So I had already Hassan on a while back and he had written this paper, I believe, called direct fit to nature. But the idea was that our cortex, essentially, have so many neurons and synapses, aka so many parameters, that it's constantly trying to overfit, trying to, basically you can't overfit. It's so big that it can memorize everything. And of course, this has been shown in deep learning networks as well. Is the right way to think about the cortex then? So James, what you were just describing was sort of a larger picture, a normative framework for what you would want as a generalization organism, generalization geared organism. One of the things I thought in reading the paper was, well, is it right to think of cortex then as trying its damnedest to fit everything perfectly? And there are regulatory systems that are preventing it from this direct fit that Erie talks about. And so at the organism or brain level, I suppose, we should think about that system as separate for, well, as kind of a control mechanism for this otherwise running rampantly memorizing things cortex. Sorry, that was a mouthful. Yeah, no, I think that, I mean, you can do better by, for instance, stopping training early, even in a large deep network. And so if this is something that the brain takes advantage of, and it would be generalizing better. So there are circumstances like Erie saying where if you have a giant network, you're not really going to overfit dramatically, so it's not maybe a huge benefit if you stop early. But it's still there. And in some regions, the effect of early stopping is incredibly important. So if the amount of experience is roughly matched to the number of degrees of freedom in your model, then that's the point where you could get a lot of benefit from replaying that data if it's noise-free. You could just perfectly determine what the whole mapping should be. But if it's noisy, it's also the point where you're going to do as badly as possible. And so regular, regularization is very important. And maybe just to highlight with an example, so because these things can sound abstract, the patient HM, for instance, Suzanne Cork and the MIT professor who did a lot of work with him was asking him if he had any memories of his mother and specific memories of his mother. And his response was, well, she's just my mother. And I think the point there is that this is someone who had their entire cortex intact. And he could not come up with a specific memory of even his parents, right? But he knew all kinds of more reliable facts. He knew that his father, for instance, was from the South and things like this. And so there's this interesting tension here where the quality of the memory, the type of memory that you can put into your cortex seems to be very different. And we think this theory explains some of that, because there's certain components of the memory or certain scenario that you can transfer, like the fact that mom is mom is always true and very reliable. But then there's other features of memory which can be very idiosyncratic, like what you did when specific Christmas. So he knew that Christmas trees were things that happened at Christmas time, right? But he didn't have a specific memory of one specific Christmas and what they did. And that's what we're proposing is explaining the character of this transformation as being aimed at generalization and flowing from these properties of deep numbers. Maybe we should get into the model. The models, the three models used in just the whole setup. I was going to say experimental setup, the whole modeling setup. So Wayne, do you want to describe how like the different kinds of models used that are supposedly to represent different brain areas, although you use a different vernacular in the paper because you talk about how these could map onto other brain areas or it's amenable to other brain areas because well, so you use student, teacher, and notebook in the paper. But you want to talk about what the models are and how they map on. Oh, yeah. So we thought about how to formalize this learning problem of systems consolidation. So typically think about a brain that can learn things from environment. And what is an environment? It's just a sort of can be viewed as a data generator and you produce some kind of input output mapings, maybe very complex functions. And we want to replicate that by a very simple, generous model. In this case, it's a shallow linear neural network. So it just transforms an input vector into a single scalar in most of our simulations. And this transformation generates produced data pairs like x and y input output pairs to feed to another similar architecture student network. So it's another shallow linear network that can between all the 20 data generated by the teacher and learn to represent the mapping of the teacher. And the student learning is a through memory module similar to like the current AI's external memory idea is modeling at the head campus. So it's a hot field network that's bi-directionally connected all to all to the student. So the job is really capture the ongoing experiences by one shot encoding through a high-speed learning. So the head campus has been proposed to be learning really fast. And then after capturing that, it has the ability to undergo pattern completion offline. So you can just randomly search for a previous memory and reactivate the student through the feedback. So this essentially is modeling episodic recall. So you could offline replay what the students was seeing when the teacher gave the student the example. As if you have the teacher like it's a notebook. So you're just reviewing what the teacher said essentially. So through doing this offline reactivation, the student can learn much more efficiently as we later show in the paper. So that's roughly the three neural networks. So you have the environment which is the teacher. You have the cortex which is the student. And you have the hippocampus which is the notebook. Just to kind of emphasize one of the things is that one important difference between the teacher and the student is that the teacher has noise. And because the teacher has noise, what that means is that the mapping provided by the teacher may or may not actually be fully learnable by the student. And controlling the magnitude of that noise then is a critical parameter that determines the optimal amount of consolidation in this framework. And this is for now it's one more thing that you asked what's new compared to the original CLS framework. So we have an explicit notebook in this model that's directly connected to the student. I think some of the early CLS works just kind of replay training examples not by storing them in a neural network but just replaying the representations. And this has generated some really interesting insights we can talk about later. Having distributed binary healthy network, reactivity of the student can have some very interesting interference real-bust properties to train the student. Great. Andrew I was going to ask, so you guys are using linear, although these are shallow linear networks. And we talked all about your deep linear networks last time you were on. Why the linear networks in this case is it just to have principled theoretical accountability? Yeah, I mean, I hope one day we'll have nonlinear ones but all of the qualitative features that we wanted to demonstrate came out with shallow linear networks. So it's just learning linear regression. And so my impulse and I think it's shared by James and Waynon to some extent at least is to go as simple as you possibly can and still get at the essentials. And what you get for that in return is greater practicality. So another feature of this framework is that most of our results are sort of mathematical demonstrations. And so you feel like you can really at least I feel it's easier to get one's head around it. And another thing that this very simple setting enabled is we can make clearer, normative claims. So we can optimize everything about these settings. How well could you possibly do if you just had a notebook or if you just had the student? And then we can show that yes indeed you really do do better when you have both interacting. I was just going to say and just to add to that, I think another thing that's really powerful about setting it up in this very simple way and being able to analyze it so comprehensively is that you know as we kind of alluded to earlier I think one of the big challenges in memory research is to figure out well what is the key quantity that determines whether it's going to be hippocampus dependent or not. And within this kind of modeling architecture we can really solve that problem from the viewpoint of what would optimize generalization. And then you know going forward you know WayNoms and experimenters so we can actually design experiments very much around directly that parameter and just test the theory very rigorously about whether or not this actually does provide empirically meaningful predictions more than just theoretical insights. And I think that that gets harder and harder than more complicated the model becomes to really kind of boil down what is the critical parameter and to design an experiment that embodies that critical parameter. Oh no WayNom you're going to be stuck in experimental crisis still. I thought you were trying to get out of that. Now I think that's perfect combination with the theory and then do the experiment. Okay. Alright. So who wants to talk about how the model works to to generalize the right amount of generalization? Yeah. So the setting that we're looking at is sort of like imagine you're doing a psychology experiment for an hour and you see a bunch of experiences over that course of that hour and then you go home and over maybe many days you have whatever you store during that hour and you can perhaps you know different the notebook could replay that information out into student to learn from it and then after some period of time we bring you back into the lab and we test your memory. So it's this sort of upfront get a batch of data how you make the best use of it scenario over analyzing and so generalization for us just means when you come back to the lab how well will you do on new problem instances drawn from the distribution of problem instances that you're seeing on a first case first time. So it could be you're learning to distinguish dogs and cats or something like this and then we show you new images of dogs and cats how well do you do on that and the key feature of the framework is that justice in deep learning theory means building directly off of deep learning theory and double descent phenomena there is an optimal amount of training time that you can train from a fixed batch of data because otherwise you start picking up on these aspects that are just noise and so as the predictability of the rule that you're trying to learn increases you can train for longer and longer and you can characterize sort of exactly how long but that's the basic idea as you get more predictable you can train from the train for longer if you can train for longer you can also memorize the exact examples you've seen more and so your your memory error is decreasing and that means that more of the memory with specific memory would transfer into neocortex and not just being in the notebook. Maybe one other thing to throw in here before I let someone else jump in is that you can compare this so there's different ways you could generalize you can try to use the student network but you could also try to use the notebook you could just say let this hotfield network complete the pattern whatever pattern you give it and make its prediction and one important result here is that in high dimensions that strategy fails completely actually so basically if you think of high dimensional vectors the geometry is very different any new input is almost surely orthogonal to all of the inputs in your that you've all of the experience that you had previously and because of that it doesn't let you generalize so it's interesting you need this notebook to store these examples so that you can replay them to the student but ultimately it's the student that's going to be able to generalize well and not the notebook. Maybe this is a good time so to I have a listener question so predictability is a key aspect of the performance of the generalization performance so with different levels of predictability the generalization needs to cut off at certain different points. Well you know what I'll just play the question so this is from Michael Tildal and then we can back up and talk about the bigger issue after the question if needed. In the discussion section it's suggested that replay could be the mechanism that regulates generalization to the Neo Cortex which seems very probable but the thing I'm still missing is do you have any ideas around how predictability of an experience is determined as that seems to be a key parameter in the theory. Okay so I know that's a little ahead of the game here but I thought I didn't want to miss the opportunity to play the question before one of you started answering the question on your own. Yeah no there really is such a good question and that you know we don't we don't address that in this paper what we say is imagine you had an Oracle which could tell you exactly how predictable this experience was. What should you do to be optimal but we don't explain how you could estimate that. It's not we do think there are ways you could potentially estimate it but it's not part of this theory at present we just are saying suppose you were able to understand the predictability what would that mean for systems consolidation. I was going to reiterate the problem which is that predictability needs to be estimated by some system to regulate the generalization process. Yeah just to give a journal club yesterday and this question is such an important one. Yeah people always ask okay so that's great but wait how do you actually estimate SNR in the experience. So upper rree if you get a new batch of data for the first time and if you're learning from no previous knowledge there's no way for you to know whether this batch of data is predictable or not. So you kind of have to learn that through trying the error but the trying error can be divided into a long term evolutionary scale or within a lifetime. So maybe some animals already has built in predictability estimators from birth maybe there's something like humans like certain facial features or certain animals that if you see that it just the brain will treat it as a high SNR data no matter what or doing a lifetime I think probably like the the main way we learn how to estimate predictability through lifelong meta learning. So when you are a child you experience a lot of things and you make predictions all the time and then gradually learn what source of information is good to consolidate. I always give this example that okay so people typically know the reliability of the source of information so for example I give you article from New York Times and then I give you article from the audience and you get a really visceral feeling like which one to trust more and to understand more. Another example is like my daughter I see like she has no idea what's almost no idea what's predictable or what's not and just one time I was holding her and cooking and she just wanted to touch the hot stove. I said okay that's not a good idea to do that you're gonna get burned and she's gonna touch the edge just a little bit and she just said okay if you don't listen to me you can go ahead and try and she touched an ouch and then she turned my hat back to me and look I think the looking her eyes is like okay I really need to listen to this dude in the future. This dude is that I think that's when I think that's one that meta learning is occurring in her brain that is assigning different sources for like different predictability like we all trust our like authorities like teachers in our lives parents and the friend you trust. No like even like another key aspect is that a lot of people think more frequent things should be consolidated because it's more reliable or but our theory is really decoupling predictability from frequency. So there are like nowadays you know there are frequent misinformation online and it's not the quantity that can overwhelm your brain and determine what gets transferred it's really like like for example like something your trust different told you like even for once can really make a long lasting impact. But some news outlets give you the same story again and again you will not transfer. So I think that's the key thing like we learn these predictability through experiences through meta learning. But those experiences need to be essentially stored right in some system to be able to be used again. And so is that is that a just a different kind of memory is that more of a implicit procedural memory or you know outside of the hippocampal neocortex complimentary go complimentary learning system framework or do we know or does it matter. Yeah that's a great question. I mean so in our model the notebook does kind of create a record of the whole memory. And so using the hippocampus or the notebook in the mathematical framework you can reactivate those cortical patterns that correspond to that full memory. But that I think part of what we mean by complimentary learning systems you already had the ability to do that in the notebook. So then maybe you don't need to kind of create a new cortical system to do the same thing. And the idea would be well what can't the notebook do and what the notebook can't do is generalize well to new examples. And to kind of go back to one away non points earlier I don't mean to say that you know the neocortex can't actually aid in memory itself. And in fact there are some examples within our framework where the cortical modules actually able to even memorize better than the notebook. But we think that the really fundamental thing that's missing from that just sort of faithful reproduction of the past which the notebook can do is the ability to generalize well. But then the amount of consolidation that would optimize for that generalization is what depends on do we have predictability and as we've been describing you know it is a very important unknown within our modeling framework how precisely this gets done. But we think that you first have to recognize that it needs to be done before you have to think about how it is done. And so the earliest experiments I think that we could design and test you know we can just configure them so that the predictability is set according to us and then see kind of to what extent the brain in fact does regulate the consolidation process based on that predictability and then get more into the both algorithmic and mechanistic details of how that degree of predictability is determined and then once determined how that leads to regulation of the consolidation process itself. There is some quite compelling experiments that show that it could be that individuals do misestimate the predictability sometimes or maybe it's not even fair to say misestimate but they just estimate it differently. So there's individual differences maybe way not do you want to explain that to the generalizers and discriminator experiments. So I think a key thing about predictability is that it's in a way it's not the universal predictability objectively is it's really like the inferred predictability. I'm not sure if you guys agree but just thought of this it's really like how the agent or how the animal thinks that what the predictability is and that depends on a lot of things. So there's an interesting set of rodent experiments in fear conditioning. There's really individual differences on the policy of animals doing the same task whether they generalize or not. So there are certain animals if you shock them in a cage for example and two weeks later bring them back and they will show high freezing. So there's a fear memory but then if you take the same set of animals into a different but similar cage to test their fear generalization only like around half of those animals will freeze will start to generalize to different cages but the other half will just maintain their discrimination in these two cages and not freeze and they know this is a different cage. And surprisingly that the generalizers who froze in both cages the memory is not dependent on the headcampus. So there's this evidence that you know the generalizers do treat the original context as a high SNR context and that promoted generalization and systems consolidation. So the memory actually is becoming you're according to dependent but the other group that is still maintaining the discrimination, leasing the headcampus actually impaired the original memory. So I think that means that those animals is treating the environment as a probably like a low SNR task and that will not transfer. So it still maintains the headcampus dependence. And we have different like in figure five hour paper like we have a diagram showing that okay so even they've seen a single experience, the single animals maybe there are different cognitive processes can change the SNR of the data. So for example if you have a whole scene the animals can actually use their covert or overt attention to only focus on part of the scene that might has different like predictive value to the outcome. So maybe the animal can just pay attention to the general features in the experimental room like the smell maybe similar and the examiner maybe wearing the same lab coat and that's highly predictable. The other one like if you just focus on the things that different patterns on the wall that's highly ideal in the end of the syncratic. So that will be low SNR. So I think attention is a very key thing both in determining the signal to noise ratio also for regulation in consolidation. I just want one last thing about the implementation of this regulation. Like we said in the discussion that replay might be a natural way to do this. Just regulate the amount of replay to modulate consolidation. replay has been shown that replay actually functions in like a variety of different ways to promote for example enhancing attractors in a record network for example or keeping the head campus in register with the new cortex. And so replay so it could be not beneficial to stop replay altogether just to prevent overfitting. It might be the brain might be using that you replay you still repay all the memories and then you have a predictability module for example like the PFC. It can control offline which part of the cortex gets activated and enable learning in an offline attention manner. Like we have the amazing ability like for example you close your eyes you can navigate within your memory like focus on certain aspect of things. And maybe the brain could tap into that mechanism to mask certain memory components during offline replay for consolidation. One of the things that you said so I kind of like two questions in one here. One is there are certain situations where I don't care about predictability because I have to climb up the mountain before I fall or whatever. What's the classic escape the lion or something like that right. And in that case I guess you would predict that predictability your predictability regulatory system just gets overrided perhaps because you're not needing to really consider how predictable the data is or maybe it just automatically happens. And I mean because you're going to remember that event probably unless you unless you die. That's another really really good question. So I think throughout framework and many people ask us what about emotional salient numbers that's really surprising really normal. And how is that related to the idea of predictability. I think it's important to keep these two concepts orthogonal to each other. So for example emotional memories could either be highly idiosyncratic or it could be predictable. I think what the emotional salient is doing is maybe I'm not sure how much data support there is it could bias the memory retention of certain memories. So for example you've climbed the mountain and you made a mistake and that was really dangerous or something like surprising happened that's pretty random. That surprising factor may be enhancing the memory retention your head campus. And then you can actually remember that memory for a long time you can tell the story maybe 20 years later. Okay back 20 years ago I had this terrible event. But then what which components gets transferred to the new cortex is determined by the predictability of the different memory components on top of that. So I think that's almost the first future process of which memories like we forget almost all of our episodium memories in the few days. And only feel a feel gets encoded. Maybe that's more like hidden your head campus to be awakened by the who knows. But it just seems that we forget most of the things and the certain things that we remember if we do remember is modulated by some kind of other neuromodulatory process. And our theory builds on top of that is the memories that gets retained for long term storage which components actually routes to the new cortex. And which components should stay in the head campus. That's kind of determined by the secondary factor is the predictability. I think your question also brings up another interesting and subtle point about predictability. And we introduce the teacher as an environment in terms of this is some sort of generator of experiences. But that's actually pretty abstract because what the brain only knows is the brain's activity. And so really what the teacher is is a generator of brain activity based upon for example sensory and motor experiences in the world. So if you kind of think about the teacher not exclusively as an environment but just as a generator of neural activity then of course your cognition itself can also contribute to part of what the teacher is because the teacher is just whatever it is that leads to patterns of activity that the student is trying to learn to produce without for example the involvement of the head campus of other modules. And so if you think about the teacher in this way it could be that for example some of these highly emotionally salient or very memorable events. In some sense are very predictable just because you think about them a lot and because you think about them a lot you actually do recurrently get these patterns of neural activity that you may actually want the student or the neocortex to be able to start to be able to reproduce on its own. And we think that this may have to do with why in human patients for example they are sometimes able to reproduce highly detailed aspects of their past life that seem to be highly unpredictable but we think that the reason for this is because basically they are so reproducible based on the experiences of that person or based for example on the thinking patterns of that person that they start to be able to be consolidated because they start to be predictable in this more general sense of not just what happens in the environment but what happens in your mind. So is it too simple to map on the teacher to the perceptual systems perceptual cortex for example? Yeah I think it could be. I mean I definitely do agree that we think about the teacher in the most simple setting as just the perceptual systems and that is kind of the examples that we provide in the current paper and that is the setting that is designing our kind of initial round of experimental design. But I do think that when it comes time to really understanding how these abstract neural network models will get mapped onto real human cognition and neuroscience that is too simple I believe that you do need to consider more broadly what it is that's leading to patterns of normal activity across the entire cortex. Yeah then you would have the problem that the sensory cortex should be getting trained also as a generalization optimizer. Absolutely. And in fact that's a very important part of how we think about it is that from the viewpoint of the abstract neural networks we have very well defined inputs and outputs but when we actually think about what that means in terms of the brain we're just thinking about the neocortex as if it's some kind of an autoencoder where activity is generated by your sensory motor experiences in the world, your cognitive processes and what you're trying to do is build a cortical system that is able to reliably reproduce those patterns going forward into the future without needing sensory inputs without needing involvement of other parts of the brain. And in this point of view then it's not as though just that as you said like some low level sensory area is not only the input of this framework it's also the output and many of these relationships have to get learned simultaneously and for each one of these relationships there could be a highly different degree of predictability. And as we emphasize in the paper based upon that high then variability in the degree of predictability there should also therefore be a high variability in terms of the amount of consolidation in terms of different types of synapses within the cortical network. So when on I mean you you you positive evolutionary architectural constraints versus meta learning earlier when talking about how to regulate the system. So you think that there's a comp- room for both I suppose. Yes. I think I don't want to really get into- Yeah, I know it's okay. I just wanted to make it clear. So Nate, wait so nature and nurture our factors? So one of the things that's fairly attractive about complementary learning systems is the idea of when you have complementary systems that the whole is greater than some of its parts. And actually Steve Grossberg calls this complementary computing paradigm. And he thinks of multiple processes in the brain acting like this and that when you have these two things working in parallel both neither of which can do well on its own but when they are paired together actually give rise to a what he calls a new degree of freedom and extra degree of freedom. How would you describe that in terms of the whole being greater than the some of its parts with this GO CLS architecture? Yeah, so I think this the sum is greater than its parts in at least several interesting ways within our current GO CLS framework. So one as Andrew mentioned earlier we're able to determine what would be the optimal learning rate you could have for generalization if all you had was the students. You just don't have any hippocampus or notebook you can use do we call the past. And because we could kind of treat that problem optimally we could show very rigorously in fact that when you put the two systems together that in fact you do get a cortical network that generalizes more efficiently from the data than you could from online learning. And so there's a really fundamental advantage where if you're going to have some finite amount of data you can just make better use of it period if you have the ability to record it somewhere and recall it subsequently to guide learning. So that's one way. Another way that kind of also came up a little bit earlier is that actually even when it comes to memory there's a benefit at least if you have a small notebook or a small hippocampus system because what we were able to show there is that in that setting you actually get some errors when the hippocampus or the notebook is trying to recall those memories. But what's really amazing is that the nature of those errors is that it's interference with other memories. And so if what you're actually trying to do is train the student to memorize that in fact you can actually do better training from those noisy reactivations then actually those noisy reactivations themselves. And so what ends up happening is that quite counterintuitively the training error or the memory performance of the student can actually outperform what you could get in a notebook alone. So yet again for both of these cognitive functions, for both the memory and the generalization the system works better when you put the two parts together. And one just to elaborate one small piece of that is it also clarifies the regime where you get the benefit. So there are regimes where online sort of just having the student and the replay strategy will look very similar. And that's if you have tons of data. If you have tons of data it doesn't matter you get to the same point both of them. Also if your data are very noisy then in the limited data regime it can still be a gap can be fairly small. So I think it delineates the regime where this dual system memory that sorts of worlds basically where it's the most useful. And it happens to be when you have sort of a fairly moderate amount of data and that data is quite reliable. Then you see a big advantage from replaying a lot. And arguably that's a setting that a lot of real world experience falls into. Yeah. And just to follow up on that, what's incredible about that as well is that that same regime is where the risk of overfitting is highest in the model. And so what's really interesting if you just kind of step back from the details of the model and think about what it might mean, we basically say that well if you're in a regime we're actually having these two learning systems is complimentary. You're also in the same regime where if you don't regulate you're going to overfit. And we think that this is a really important conceptual point because then you know we do know that you know the biology has built multiple memory systems in the brain. And one of the lessons we can walk away with our artificial neural network is that perhaps the fact that it's built it suggests that it's good for something. And at least within our framework it's very rigorously true that when it's good for something you absolutely need to regulate. You can't just kind of transfer everything to the cortex. Yeah. So this is going back to James point on like we have a notebook replaying examples. And because a notebook has limited capacity, it shows some interference. So the replayed example is not exactly the training examples. There's some error. And this error just surprisingly as James mentioned that it does not hurt training the student. And in fact that so I think this is a great story that is just so counterintuitive that I was running the simulations and James and Andrew here in Agnilia. And I showed them the curves for the first time. So the notebook reactivation error is like if there's a little bit error due to interference and use this type of reactivated data to train the student. And it turns out the student training error can drop below that reactivated error. And that's and we were like you are using the notebook reactivated examples as labels. And you can't possibly do better than that reactivation. And later on it just led to a lot of mathematical analysis. And it turns out that I think to me this is so powerful and they question me okay, wait not that's you should check your code if there's any bugs. And then I did something crazy. So I said okay no matter what I do I'll get the lower error from the student. So what if I just generate random activities in the notebook. And to my surprise that still trains the student perfectly with some change in the learner rate. So I think this is something deep that about like maybe if listeners are building future generation models of memories. I think there the reactivation has a certain property that can enhance generalization mainly by. So let me give you example so machine learning after all this I discovered a connection that they use some certain data augmentation methods called mixed up that you just like for example you train internet or amnesty and you just linearly combine different training classes together and also combines that the output probabilities together like adding them together and they only use the mixed examples to train the network. The images themselves are mixed like one image would be a mixture of two separate images. Yes. So you just run them these sample like two to four images and the stack one and two together. And then the output probability will be like you have a 10 outputs and it would be 0.5. I see. Yeah. And you only use this. You just get rid of the original pictures. You only use the composite images to train network. And it trains equally well and sometimes it's even better. And this has became a very powerful like data augmentation in even modern transformers. That just performs much better and it is also like really interesting being used in data encryption. There is the Princeton study showing that you know the hospitals have records to train some kind of AI model for prediction and because of the data privacy issues, they just randomly merged patients data together and merged the output together and it still trains the model equally well but masking the original data. So I think this interesting connection is I mean the brain is capacity limited and if you want to store some previous experiences to train up your new cortex and you've such flexible ability to change your input data and still train the new cortex well, I think the brain might be tapping into this mechanism and have sort of weird ways of generating examples instead of just replaying train examples one by one exactly that might be run the merges of memories. That has been supported by experimental data in the headcampal field that certain memories are just prone to errors. So for example you imagine a house like you left your house yesterday and there could be related things being put wrongly put in the scene like you will remember okay there is a car parked right front of me but it was not. So there is this leaking interference between memories could also be helpful in training the new cortex because you know a car is a car sort of independent what which scene is at. So this kind of interference might not be as bad at training the new cortex and that probably reflects some certain compositionality nature of the world like you can have different things merged together and still give out good training data. This is a trivial question or maybe this is a trivial question but how do humans perform on the mixed image net data set. I think I can't tell them are part. Well so then does that run counter to the story you just told then because presumably unless that's just an inherent difference with deep learning networks which we're going to talk about here in a second anyway. Yeah so that perceptual example might be different but if you really put different objects together and humans have the amazing ability like if I put a cup, a weird, a car in the same scene versus seeing them apart I think human will have an easy time to pick up. Okay how many objects are in the scene and it give each labels. Oh I thought that the pictures were blended like where you take the RGB values and can you. Oh they are so they're not compositional pictures they are they're literally blended where we wouldn't really be able to perform well is that true. Yeah I guess so yeah this I need to think about this more but maybe there's something there. So predictability is a key or the key is the world really predictable or is it super noisy. Are we ever in I'm trying to think if I've ever been in a situation where it was completely unpredictable maybe early on when I was dating but so how does predict are there situations where something is just truly unpredictable and if so how does the network handle that. Yeah that's a great question because so far we've basically been talking about predictability as noise so you know we're real randomness coin flips but it doesn't in fact all the same phenomenon occur in completely deterministic settings it's just what the equivalent is that the teacher is more complex than the student so if there's something that the student can't possibly represent about the teacher and I think that is definitely a reasonable assumption about the world right this very very hard to imagine that we would be able to predict everything about a physical situation and so essentially that unmodelable component which in you know learning theory we call it the approximation error it looks like noise from the perspective of a student and that judgment is is completely relative to the students what what can the student actually do and then if the teachers work complex then that will have the exact same effect you can see the same over training all of the same behavior in learning curves and another version is maybe you have a completely deterministic world maybe it's completely predictable even it's just that you don't observe the full input so imagine that you know the teacher has a hundred input nodes whereas the student only has 50 now the remainder looks like unpredictable information from the perspective of the student again and now I'll have all the same properties as if it was really just a noisy environment so there are several forms of unpredictability that behave similarly and would would require the same regulation and transfer between brain areas yeah so just on to that like maybe I was just trying to really think about this intuitively with the these are silly examples and so for example the noisy teacher you just you can imagine like someone going to the casino in Vegas and then play the role and then picked in the number 27 and a lot of money on it and it he lost he lost like 10,000 dollars and and then the unregulated consolidation will be that you treat number 27 as the bad number forever like you just learned that I should never pick number 27 despite it's kind of random and that could be detrimental to your future for example if you are dating a girl whose birthday is on August 27th and you said oh no no no I'm not gonna date this girl so that's gonna be bad for generalization and the second example is about the complex teacher like this is really give me the insight on you know why a lot of episodic memories are being kept in the hape campus and require the hape campus it's just a lot of it is because the world is so complex just you just imagine you are on a street there are different things happening and a lot of them are independent processes has its own cause and effects and to cross part predict such like complex interactions between so many things it's generally impossible for a human brain to do and like an intuitive example is that a lot of times in movies you see like a tragedy like a tragedy like a part of the scene is low is the low like the person got a cancer and then hit by a truck and there's something else happened and the actor just started to try like a why why me I think at those times it might not be beneficial to really consolidate consolidate such complex events like it's better to remember those things but if you over generalize from those complexities it's gonna hurt generalization and the last thing about partial predictability is I mean a lot of like most of the time our perceptual access to certain events are really limited like people always say like the traditional wisdom is that you should really put things into context and don't just judge things by the first cleanse like someone is behaving on certain way and you get offended and maybe that that person is having a really really bad day and you can't just based on that partial observation that this guy just a grumpy person he's not friendly and maybe he just keeps that as episodic memory and maybe you can build up more accurate representation of this person by long term interactions so I think I think about those three and pretty pretty unpredictability this way. Can I throw one more concept into the mix this seems to me related somehow to concepts like Herbert Simon's Satisficing and Bounded Rationality and Conman and Tversky's Heuristics the use of Heuristics and good enough scenarios. Have you guys thought about how your work and the results and implications of your work overlaps with those sorts of concepts? So yeah Bounded Rationality that's interesting because we kind of are assuming you know we have this Boreical Assumption you know the predictability we're optimizing all parameters of the setting but we've constrained the system to be this particular neural network with inherent limitations in that so yeah I mean I wonder if that is a version of I guess it's like bounded architectural rationality you know there's like something baked into the architecture that you just it only is going to take you so far and and in terms of heuristics I mean yeah I guess you could maybe view it similarly that you maybe forced into a simpler solution to what is actually a complex problem just because of the resources that this student actually has available to it but I don't yeah I'm gonna say really interesting connection. James it looked like you were going to add something. No I think I was a good answer I was not gonna add anything more than that I was just gonna kind of bring up the same points that this notion of unpredictability as it relates to approximation error is kind of giving the idea that the cortical network may only able be able to do so well and that could have to do with the architecture the network it could have to do for example with what it's able to learn the learning mechanisms involved in that network and so it is a notion of bounded rationality I think for sure but then how closely that would relate to the more famous notion of bounded rationality I think is a very interesting and deeper question that I think is harder to kind of answer at the moment. Yeah because here one of the great things about heuristics although they fail in many ways but they're also beneficial in many scenarios and that's kind of why I was wondering because you know you have to have this predictability estimator and it needs to be beneficial for the organism and then I was thinking you know heuristics for all their failures are also very beneficial in certain scenarios so yeah I mean I think you could view whatever it is that the student learns in our framework as a heuristic because it is going to be an incomplete and inaccurate to some degree representation of what the teacher actually is but as you said this heuristic is very useful and in our setting it's very useful in a very precise mathematical sense it nevertheless optimizes the generalization given the bounded rationality possible for that system and so it kind of brings these two things together that you know if you have some sort of bounded rationality or some sort of limitations in terms of what the system can do then you know obviously you can't do better than that but then the heuristic may be the best possible thing but so you guys put this in terms you're careful not to just map on the networks to the brain areas to hippocampus and cortex and the environment or perceptual cortex for instance how should I think about this should I think about this as theoretical neuroscience should I think about it as artificial intelligence work and then what what does it imply because there are you know like what we alluded to earlier there are networks already with external memory there are metal learning networks so what could deep learning and or AI in general take from from these networks yeah so there yeah like you mentioned there are a lot of all memory augmented neural network neural networks out there and also memory based rl agents that can use like it's typically is coupling some kind of cortical module like ls-e-m to an external memory so typically the memory is fairly simple so a lot of times it's just a pending each new experience as like an additional row so it's kind of pending into a big matrix and a lot of times there's a keys and values like you use the keys to search and retrieve a softmax-average output as your episodic memory and that has been very successful in certain problems like the neuratory machine or differential neural computer work from deep mine can solve a vastly different problems than the traditional neural networks like Greg Wayne's Merlin framework also can solve like the water maze like typical ls-e-m cannot perform but I think there are inspirations we can take from the mammalian headcampal architecture I think instead of like the first thing is that still like the experienced replay in rl agents and the usage of online like memory excellent memory modules those two are different things like different modules doing the memory storage and doing the online inference then you know mammalian brain we think the we use the headcampus to predict the future but we also have evidence that the headcampus is replaying and serving as experienced replay buffer to train up the new cortex so maybe there's a advantage of emerging the two modules together so that's one direction and the second direction is that how exactly should be the memory representations in external memories like instead of just a pending different rows can we use like more spars distributed representation and a biologically realistic retrieving rule for memory retrieval so there's actually a very interesting work I think in the last year there's a group showing that how few network like a modern continuous how few network is equivalent to the transformer self-attention mechanism so there's some deep connections here maybe like memory and attention are really like different aspects of of the same thing so I think using some kind of a hyper-campus inspired architecture maybe there's some certainly research directions can be uncovered I don't know exactly what yet and also I think the lot thing about the memory module is we use the how few network and that's fairly traditional and current AI external memory modules they use like a form of like more advanced versions of those generative models like variational auto encoders or against so these things I think like one inside might inspire AI is that you know we know the anatomical connections between the head campus and between like the rest of the cortical areas but also the PFC and typically people doing variational auto encoder work assumes there is a input going into a series of hierarchies and arriving to the head campus and the head campus sort of try to encode a latent representation that can reconstruct that input to reduce the reconstruction error but maybe there's a way to improve this by I just realized that you know there is an architecture called vegan is VAE again so it's a VAE and again connected together so the idea is that instead of reconstructing the original input you send your reconstruction to for example the PFC and the PFC serves as a critique or the discriminator in the game and tell and feedback the signal that is this realistic or not and this is a plausible assumption to make because I like a lot of patients with schizophrenia if they have lesions in the PFC they cannot tell the difference between imagination and real world so it might be true that the reconstruction from the head campus is sent to multiple modules to compute different cost functions for example can be both reconstruction or can be reconstructing another discriminator's function to best reproduce that sensory string so I think I think some architectures like this like a multi-head generator model as external memory will be very interesting where different parts of cortex serve as different modules for different well like you said cost functions is in the vernacular of AI but purposes I suppose in the vernacular of organisms yes yeah I mean I also think there is maybe not as exciting lessons to be learned from this work for AI like if you build a continual learning system that does store its own examples and manage it's own learning then it's going to have to regulate did not replay the tests it's a very simple point but I do think that's probably something that will start emerging and it's an interesting broader question of how do you decide when to learn and how to learn like ultimately agents we we decide oh this was a learning episode I'm going to store that from it presumably how do you manage those decisions because in the original complimentary learning systems more replay is better always right and that's one of the take homes here is that you have to regulate that and knowing when to regulate it is a pretty important factor yeah also with that point I think more than more than RL algorithms it's kind of having like a lot of problems of generating generalizing to new tasks like train on one game and test on unseen levels of the other game there are a lot of effort improvements but I think those agents will be benefited by having a different module that specifically estimate the predictability of the captured experience instead of like replaying or train on all perceptual data you only train based on the score of that predictor like so you can actually filter through your experience and only generate generalize the useful components so maybe that will help RL as well so one of the recurring themes these days is that AI is moving super fast and neuroscience especially experimental neuroscience because experimental science in general is very slow but even I think theoretical neuroscience is kind of lagging behind the progress of the engineering in AI right and and so what this is is theoretical neuroscience at least partly right we talked about how it's kind of a mixed bag of things but do you see this kind of work theoretical neuroscience more broadly as being able to so backing up what what the implication of that rate of progress means is that right now and for the past 10 years or so AI has been informing neuroscience a lot more and the direction of the other arrow from neuroscience to AI is slow or lagging or lacking do you guys see theoretical neuroscience as a way to bring more influence from neuroscience into AI yeah I mean I'm usually hearing everybody else has to say but for me I definitely do because I think that you know it may be slow but I think that theoretical neuroscience and kind of really rigorously working out how individual models work and how they relate to the biological brain I think that that provides kind of fundamentally new and fundamentally more robust conclusions than you can get from just kind of numerical experimentation on very large AI systems and so I definitely think that you know we wouldn't have gotten to where we currently are in AI without past generations of theoretical neuroscience research and I also definitely think that you know projects like this where we try to kind of boil it down to the essentials and really analyze everything very rigorously and really try to figure out to what extent this relates to the biological brain will provide useful seeds for future AI research what do you guys think do you guys agree with that yeah I mean I think James and I see very similarly on this it's the time scale may be long but ultimately I think theoretical neuroscience and psychology just have an enormous role to play in driving AI but you have to be willing for that impact to happen many years down the line but just to math I mean for instance deep learning right the whole thing the fact that we're using neural networks those all came from contributions that were worked out and dialogue with our scientists and if what theoretical neuroscientists were doing right now today 50 years from now how the simulation impact wouldn't that be amazing and something that everyone want to work towards so and that's how I really do think that's possible here we don't know what those it's more encodate it's more uncertain we don't know what these principles will be that will guide us towards even better systems but having the insights from the brain guiding the inquiry and showing us some of the problems that maybe we didn't even realize for problems is really valuable but my perception is that the the in large part the majority of folks working in the AI world what they would say is well sure theoretical neuroscience may eventually provide us with something but by that time our systems are going to be so advanced that it won't matter right because we've already basically accomplished that and I mean I think that that personally my personal opinion is just an opinion is silly do you think that that's right that the AI world thinks that and is it comforting to know that the mass majority is wrong or how do you think about that I do think it's right to say that that's a widespread opinion I've had AI people tell me like why do you even do mathematical theory because eventually I'm going to make a theory proofer that's going to do the mathematical theory and explain it back to us so you know just focus on getting the AI system to work we won't know and the proof will be in the pudding my own opinion is we're going to make it's going to turn out that today's AI systems as fantastic as they are will hit roadblocks and part of getting them unstuck from those roadblocks will be looking again to the brain just as happened with deep learning again I mean I think this history often gets kind of run rough shot over but the paper on back propagation first author is David Rommelhart who is a psychologist though the first paper on the perceptron that's Frank Rosenblatt he's a psychologist the contributions and and and I remember as an incoming graduate student the paper that inspired me towards deep learning was from and I'm sure it's different for different people that were AI people working on it but for me it was Tomas Apagios work and Maxine Lennersenhover and they are neuroscientists the theoretical neuroscientists the reason why they were interested in these convolutional network architectures even though the rest of the field hard to remember but rest of the field was not they're writing sift features probably different the reason why they're interested is they kept looking at the brain to say like this is what we see in the brain somehow it has to work and so I think that that promise is still there for the future and if you think about some of the topics like theory of mind some of these ideas that have come from cognitive science again ultimately causal reasoning all of these things have been pointed to by cognitive scientists and now we're seeing that yeah they really are important they require their own methods to address yeah so I guess I think it will continue to be important going forward I just want to add on that I totally agree with both James and Andrew I think neuroscientist still has a lot to offer to AI and especially exciting new direction like Yashua Banjo has proposed this idea of we should learn from human cognition or animal cognition on the systems to level cognition so that's comparing to the directly perceptual classification tasks that Apple is Apple is like you have long-term deliberate planning and reasoning that you ahead you have to think about things and those abilities are not well captured yet in the current models and one of the solutions I think Yashua Banjo was proposing is there's some kind of attentional so-called sparse calligraphs that is a good word model and you form this call-to-nose through learning like whether by semi-supervised learning or by other methods and you form this call-to-nose and you can actually reasonably within that word model and the key thing I think just one of the key things is there's a recent debate about whether to learn things from end to end or just have unique structures all the other side like whether to use deep learning or use good old fashion AI like mainly on symbolic processing I think there is a trend now is is actually beneficial to learn the symbol like or abstract representations of entities in the world that has been an autopic in psychology but more more recently like especially by work by Randall O'Reilly and by Jonathan Cohen's work from Princeton they show that if you couple a LSTM controller to an external memory and only manipulate the memories indirectly from the keys of the memories and through learning through many manipulations you can form this symbol like representations in the keys so I think that's a kind of a key insight offering like how the brain might be generating those abstract representations and it could be in like a high-pcampal-like structure that the cortex and the sensory encoding actually are bound together through fast plasticity and the cortex can learn to manipulate those memories and eventually symbols are generated by this manipulation so this is kind of going sensory generalization to the next level out of sample distribution and it's like a higher systematic level generalization idea that the head canvas might also offer insights into this process so I think this system's two level combination is really like what is needed for a more more powerful or even like more human-like AI agents in the future okay guys this has been a lot of fun I want to wrap up let's do one round of what kept you up last night what were you thinking about that's just at the edge of your knowledge and I know that this is really more of a question for the beginning of a conversation but something maybe unrelated since I know you all have different lives outside of this topic that we've been talking about Wayne on can you tell me what's been troubling you lately that you can't quite figure out just beyond your reach yeah a lot of things to be frank yes so I think we're reflecting back to the on the collaboration I think there are really like a two things that we really troubled with is so mainly it's just due to the general sense of like being adequate for so many years so being an experimentalist interacting with both Andrew and James and just the first few years I I found this interdisciplinary work just to be extremely challenging and you know I was stepping outside of the snabetic transmission world single cell computations to cognitive science and like I've never heard of direct grade amnesia curves before this to be frank and I was surprised to find okay there are so many diversity I thought this are all like really figured out and I was just diving into this research researcher and also on the other side I diving into the mathematics and multi-machine learnings really trying to learn multiple things that has been extremely difficult but also rewarding so still that poses a challenge to me I think if you try to learn multiple things at the same time I mean I'm just barely feeling that I can barely keep up with the minimal amount of knowledge happening in the spells fields so I still don't know how to deal with that maybe more collaborations but you know our human just have certain amount hours during the day so how do you read so many papers that's what's keeping up yeah yeah well you can't read so many papers so there isn't there a perfect interleaving of time spent on various things going back and forth and have you figured that out because I don't know it that's going to be the next normative model okay learning strategies yeah so I think the second thing just real quickly is really like this process like with Andrew and James I mean I think I talked me about the you know really the benefit of really talking to people in different fields and that's kind of obvious now but but in the beginning was not easy like I felt being an experimentalist James and Andrew had to explain things very carefully from very basic level knowledge to build me up I think I really appreciate that but also like through interactions we found like including my current advisor Nelson who are also an experimentalist like the communication between theory and experiment are extremely like could be challenging at times because we spoke different languages like for example like even James and Andrew like the idea of generalization in machine learning is well appreciated and that has entirely different meaning in the experimental experimenters mind so I think that communication is hard and also like I talk about experimental details that huge complexity in experimental neuroscience I think that sometimes we just assume people know this knowledge and know this complexity that's not true neither right so I think the natural tendency is really like okay I just gave up we theorists worked together because we talked well to each other and experimented this work together because you know it's really effective communication but I think what's what I learned is it's really important to be patient and to really try to understand each other so I think that still is challenging science communication is super important important for multi-disciplinary research is there a role of predictability in the complementary collaborative learning whole organism system system sorry I just I couldn't help the analogy to to go CLS here but yeah no I was joking about the predictability all right wait no that's great um Andrew do you want to chime in anything bothering you last night that you couldn't that you you frustrated you just can't figure out etc well one very this is very specific deep learning theory but I actually think it's quite important when you look at the brain you see lots of modules you see this complex interconnectivity of sort of naso scale modules and we all think that those modules are kind of specialized for function and kind of not and it's distributed representations but also not and somehow this all is important for generalization and systematic reasoning and accountability and I will understand why and it's still very much bogus made if we don't have a theory but maybe to build on one of the links points to another thing which gives me up at night is whether this theory that we proposed will in fact be tested by an experiment well I thought you guys had begun sort of the early stages of of thinking about how to actually I was wondering if you had already begun but I know that you're thinking about how to test it yeah and and if anyone could do it it would be way on and Nelson's lab is perfect for it I have however been in the beginning stages of many experiments yes they have not finally panned up so you know it's just I think like when I'm saying it's it's hard to cross these communities and one of the things that I would love to see is how we can create this feedback loop and purchase a cycle and actually get it functioning on all cylinders to sort of you know make the theories concrete enough that they can be falsified and make those experiments actually happen using all the amazing methods that we have now all right so James I'm sorry we ran out of time and can't include you now I'm just kidding James do you have something that sure on I think it's actually super interesting that you know both way non and Andrew highlighted the difficulty of how hard it is actually to kind of really get these feedback loops going really robustly and I agree with that and that really does you know often keep me up last night I mean I guess in reality last night there was a big election of Virginia so maybe that wasn't always keeping me up last night but on many nights I may have been what keeps me up but I think it maybe also related to that I think you know also maybe going back to a theme of earlier about you know the value of kind of abstract models like I think that actually when I am often kept up at night by science it's often because it's a real concrete math problem that actually keeps me up at night most often because I feel like when you get it to that level you can think about everything so crisply and you can really get the sense of you know I'm really about to solve this problem like if I just think about this a little bit longer I'm going to have that solution and so actually there is another one there and maybe I'll give a little plug I completely unrelated this work I'm collaborating with tooth beer biss wass engineer when trying to analyze the links between structure and function in neural networks using geometry and we've had a lot of really interesting conversations over the last week where I've learned a lot mathematically about like how to think about this problem and that's what keeps me up at night in a positive way maybe the negative way is worrying like are we communicating clearly enough that we're going to be able to kind of break down these barriers but you know maybe I'll just highlight that sometimes you stay up because you can't bear to go to sleep and the solution to the math problem is so close is the desire for the solution to the math problem due to your background in physics or is it just you know I think I would actually flip it I think actually my background in physics is probably due to my desire to solve math problems and I think that this is why it gets so hard I think to cross these boundaries because I think that you know fundamentally probably why different scientific communities like why their individuals went into science could actually differ I mean for me it probably really is that the beauty of like solving a math problem but you know for many other scientists who have a lot of valuable expertise to lend me that's not why they went into science and they went into science for completely different reasons and so then how do we kind of you know not only communicate to each other to help them understand what we understand but also they said well what were motivated by and go to Andrew's point about like okay well why don't many theories get tested and I think that a lot of times it happens because the motivations of the theorists and the motivations of the experiment was actually not the same and so you know the theorist made me go I don't understand why you don't want to test this but then the experiment was like I don't know why you keep on talking to me about this boring theory that's all this stuff going on like you know I just found this crazy thing like look at what I actually came out of my experiments and I think that like you know getting those motivations aligned I think is another huge part of what will eventually be needed I think to kind of close these disciplinary divides. Oh it's a challenge all right James well I'm gonna let you go do some beautiful math guys thanks for talking to me for so long it's really cool work and I appreciate your time here today thanks for having us this was a lot of fun thank you Paul brain inspired is a production of me and you I don't do advertisements you can support the show through patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science go to braininspired.co and find the red patreon button there to get in touch with me email Paul at braininspired.co the music you hear is by the new year find them at the new year.net thank you for your support see you next time let me into the snow covers all the take me
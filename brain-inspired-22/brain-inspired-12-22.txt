 When we do brain imaging, we're measuring the artifacts of the algorithm. We're measuring the intermediate stages that the algorithms are of outputs in order to do its next step. We can see the states that are happening, but we don't have trouble seeing the actual operations. I think if we could do that well in humans, we would have some sense of what people's algorithm are. It is. There is an infinite number of algorithms that we can come up with. I think that we should think in terms of learning principles and just develop as many algorithms as needed until we have something that captures a lot. I don't think like reinforcement learning or even learning in general is necessarily what our brains were built to do or is like all that they do. I think learning itself is a continuous concept. We should take a more broad approach than just to say, we need to understand learning only in terms of the kinds of algorithms that can encompass learning. This is Brain Inspired. There is a riverbed alive. That's one of the questions that we tackle here in this last panel discussion in the Neuromatch Deep Learning series. This week in Neuromatch, it was all about advanced methods, unsupervised and self-supervised learning, reinforcement learning, and continual learning slash causality. We don't talk about causality at all. We actually have a pretty broad meandering discussion. With me today is Alana Fish from the University of Alberta, Jane Wong from Deep Mind, who's been on the show before, and Aida Momennajad, who's now at Microsoft, who's also been on the podcast before. Some of what we do talk about is language and representations, what separates humans from the rest of the animal kingdom and bacteria and rocks and rivers and AI. We talk a little bit about meta-learning and continual learning more broadly, what meaning is, and lots of other things. Like I said, it was a broad discussion. You can go to the show notes to learn more about the panelists at Braininspired.co slash podcast slash NMA-6. For those of you in Neuromatch, I hope you enjoy your experiences in Neuromatch. Just as I'm recording this, which is right after we had the discussion, Aida emailed me and asked if I could add, if it's not too late, that she thinks the internet as a whole is closer to life than a riverbed is. Exclamation point. There's a little nugget to put in your pocket for later in the discussion, but I'm done recording the intro, so I can't wait for Jane to respond via email. This was a fun conversation to round out these panel discussions. Enjoy. Alana, before you introduce yourself, I have to ask you a question. You worked at Google in Pittsburgh, correct? While you were working there, did you ever do any of the in-house yoga classes that were offered? I know you guys probably had a lot of offered ping pong and stuff like that. Let me think. So I was in there, well, what was technically their second office on C and U campus? Yeah. So my wife taught yoga at all of their Pittsburgh campuses, and I thought, oh, I wonder, because you were working there around the same time I was in graduate school. And she was teaching yoga. So I was wondering if she had taught you yoga. I do like, yes, but God, that's the one time ago. I was there in 2007 to 2009, so. Yeah. I believe she was, I'll have to ask her, but it was, it's funny because she taught yoga all around. And at Google specifically, a bunch of engineers, she said that it was so funny because you guys, I'm going to say, we're so like compliant that at one point, she was just, she got a foot cramp and she started like moving her foot around. And then everyone in the room started moving their foot around as if that was the yoga move. That's engineers. Very good. Yeah. And then the instruction following. Definitely. It was a good time for her. So hi, Olana. So Ida and Jane have both been on the podcast before. You have not. So welcome. And why don't you, who the heck are you? What do you do? I'm Olana Fish. I'm a professor at the University of Alberta. I'm also a fellow at the Alberta Machine Intelligence Institute. And in my work, I like to think that we compare representations. So we look at how computer models represent the world, including using vision and language. We look at the internal representations and we compare those to a human representations in the brain and ask how using brain imaging, we ask how those representations are similar, how they're different than how they change. So I want to go around right after you guys introduced yourselves also. And I'd love for you to say what you think might have been your first really good question that you asked at any point in your career or pre-career. Who knows? And then I'd like you to talk about maybe something that is, that's a question right now that you feel like is just beyond your grasp. Good. Okay. So I have a young son. He's two and a half. So I have to say that we all ask excellent questions as are young as young ins. And one of the really good ones is why? Just like over and over again, why? But in grad school, I had this idea that maybe language models and the brain were actually representing the same thing. And we could improve computer models of language using brain imaging data. And so in 2014, I published a paper showing that actually did work. And I've actually, yeah, recently expanded upon it again. So that's sort of what I would say is my big ah-ha moment that it's actually all the same brains, computer models, all the same thing because it's all living, sort of living in the same world. I mean, it's just outside of my grasp. So I would like to take that idea a little bit further so we can measure how people do things using brain imaging data. And so I would like to use that idea to improve computer models of language in the areas that they are just failing right now, like common sense reasoning, sort of implications and inference and dialogue. All these things are just natural to us. We do them all the time, even in thinking. Computer models just follow the part. And I feel like, is like, where is that link between what people are doing with their brains and what computer models are not doing? How can we find that connection? Well, we'll come back to the question of meaning in a little bit as well, which is related. Very good. Jane, you want to go next? And who are you? What do you do? And what was an early, maybe an early good question, maybe your first good question? And then what do you not know right now that you wish you knew? Hi, I'm Jane Wong. I'm a research scientist at DeepMind on the neuroscience team. And I've been working there about six years now. I can't believe it's been six years. And I was a neuroscientist slash, I guess, kind of physicist before that. I actually did my PhD in applied physics, where I applied physics, I guess, to computational neuroscience and was essentially creating these dynamic systems or modeling systems of the brain in a very abstract sense, not in a, any kind of biologically detailed way necessarily. And then in my postdoc, I entered more into the cognitive neuroscience realm of dealing with actual, you know, real brain systems and humans and doing neuroimaging and cognitive modeling and all that stuff. Let me just, I'm going to jump in and interrupt you. So if you had to do it again, would you switch the order of anything or was that a good order? I think that was a good order. I'm pretty happy with that order. I might have jumped more into programming at an earlier stage because I do think that programming is something that I could have used the whole way through and just been, you know, a lot more comfortable with the engineering side of it. But no, physics really gives you a good way of asking questions. It gives you a good toolbox, I think, to be able to reason about new problems. So like, you know, the physicists, like the art, art, art, art type of sort of problem that you have in physics is a spherical cow. You always like, reduce everything to a spherical cow. You know, you reduce it to a problem that you already know how to solve, no matter how ridiculous the reduction is. And so that's always been my approach, I think, to, to research since then. I think it's really formed the way that I, that I asked questions. But yeah, so one, one question that I think I asked, why I asked this before I even really understood what kind of a question it was. It was, you know, during my postdoc and I was talking with one of my advisors about, you know, what was I interested in? Like, what kind of question about the brain that I want to figure out? And I just kind of formulated this without even really thinking too much about it. But I was like, well, I want to know, how does what we know already impacts what we can know or what we will know in the future? And just, you know, phrase like that, it sounds just incredibly naive and very simplistic. But, you know, I've been working on, you know, meta learning and meta reinforcement learning for years now. And it wasn't until maybe a couple years back that I thought back to that conversation. And I was like, oh, wow, that's actually exactly what I wound up working on, which is, how do the things that we have already experienced and know really shaped the way that we approach new problems and the way that we learn in the future? So I mean, I still haven't gotten an answer to that question necessarily, but, yeah. Before you go on again, I'm just going to keep interrupting. So that's really great that you remember, however, you know, maybe lose or vague the memory is that you remember you had that question. So Alana, you were at the CNBC in Pittsburgh. And I've just been asked to do the alumni lecture, the CNBC alumni lecture, which is a big mistake on their part. But one of the things that they want me to do is answer some of the questions that I ask on the podcast, these vague things and one of these questions, like, what did you use to think that you now think is naive and stuff like that? And I'm like, I don't know my own answer to these questions. That's why I ask them, you know? I think that's such an interesting, like, that is research, right? Like we keep returning to these questions over and over again with new lenses, right? And I think that so many of the questions we end up answering are the ones that we asked long ago before we had the tools to answer them. Right? I think that's actually really insightful. Okay. But, but like Jane just said, those are the ones that seemed to never truly get answered perhaps. I don't know. Maybe that's what you do also sometimes look back on your answers and realize, like you answered the question. It wasn't what you thought the answer would be. And it didn't take the same shape you saw to it. Like you had something in mind. You answered the question and you realize you answered it before you knew that was still the same question you were asking. Do you have any? Yeah. Yeah. Well, I feel like, you know, in the beginning, we asked questions that are really grandiose and they're the things that drive us. You know, and they have to be grandiose to be motivating. The more naive, the more grandiose perhaps, right? Yeah. Yeah. But then once you finally figure out how do you go about answering this question and you make it concrete, you make it doable, then you're wind up answering a very small part of it. So I can answer something about neural network, how do neural networks met a learning specific set of environments? And what does that look like in a very narrow sense? So I can answer that question. But the larger question still drives me, I think, in a lot of different ways. Well, so the next thing that you need to tell us then is, so you start off with the grandiose question, that was a good question. And now you're grappling with many things, I presume. That's one of those things, and how specific and how non-naive is it now, right? I think I'm still quite naive and maybe grandiose in my question asking because maybe it's just because I don't know as much about this topic. But yeah, one thing that I've been really interested in is thinking about how much of human intelligence is really cultural. So as we look at the social factors that bind us, it's due to the fact that we have literature, education, and we raise our kids in a very social way with language and with through imitation and kids are sort of primed to learn in that fashion. If you raise a trial and isolation, then they wind up having all sorts of developmental issues. So, yeah, and I think that a lot more people are starting to think about things like this as well, and even in the realm of AI, which I think is really exciting. So I do think it is still quite a grandiose question because you have to not only answer how much of intelligence is cultural, but what is intelligence? How do you define human intelligence? Maybe this is why I started to think about this in the first place. A lot of what I do is I compare human intelligence or animal intelligence against artificial intelligence that you've trained this model. And in a lot of ways, I feel like maybe the comparison is a bit unfair because if you're a compare human, they kind of come in with all of these pre-programmed in bits of knowledge that are cultural and are social, and they're told things by the evaluator, whatever the researcher that's conducting the experiment. And so, the machines don't have the same benefit of that. Aida, how was the gig last night? Oh, it actually was great. It was by the beach, so it was wonderful. What instrument or instruments did you play? I played the melodica, the guitar, and I sang. Oh, oh, I didn't know that you were a singer. And I looked at it. So, what gig was this? Oh, that was just... It's her other life. It's your musical career, right? Oh, wow. I had no idea. This is not going to be in the... Do you want me to edit it? Do you want me to edit it out? I can. I just thought it's fun for people to hear. Oh, just curious. Yeah. Okay. Yeah, so I make music on my free time, and sometimes I perform it. And yeah. So, Aida has seen me frequently in the past month or so. So she was on the podcast. And we recently did a discussion panel for deep learning and dopamine for the online dopamine conference. And in fact, we did that like a month ago, but it's going to air in a couple of weeks through the podcast. So, the time is a little reversed. So Aida, who are you? What do you do? And questions? Only if I knew. So, I'm a researcher. I'm a senior researcher at Microsoft Research in New York City. I work on reinforcement learning. I particularly work on, I build algorithms that learn to build models of the world, representations of the world. And then test those algorithms in brains and behavior from FMRI to electrophysiology. And something that I'm very interested in and I've worked on for the past, I don't know, 15 years or something, is how the ways in which representations and memory is structured in the brain corresponds to how we would remember to do things in the future or we would predict or plan or imagine things. So the relationship between executive function, which is planning, which is multitasking, transfer learning, et cetera, to memory and representation learning. So that's something that has driven my work for a very long time and I continue to work on that. And now I get to sort of focus on it a little bit more computationally than the past, but I'm still obviously doing experiments both with behavior and still collaborating on neuroscience data if I'm not collecting it myself. Yeah. Oh, another direction of my research that I really love is one where I study how the structure of groups of people talking to each other shapes their memory. So how the graph structure of a number of people talking to each other and the order in which the edges or the conversations on this graph happen shapes whether they converge or diverge more. And yeah, so that's a very exciting part of my work that usually I don't mention in talks or unless there's a different audience for that. And so I have a review coming out in philosophical transactions of the Royal Society called Collective Mines, which is about this one because this is something that has been on my mind since very early on as well. So now I think I should mention it. Yeah. So I just want to interrupt again, but let's go through your questions. What was the best, a good early question that you had and what are you struggling to think about now? Okay, great, great. That was a great question from you to ask. I really enjoyed hearing what Alona and also Jane thought about had to say about that. I think so my background is I started with computer science and I was actually coding when I was a teenager and like won some of sort of competitions like that, which was a good starting point for me to feel the sense of algorithmic possibilities. And then I got very interested in philosophy since I was like, I don't know, 16 or something. And at some point I decided I'm going to do my masters in philosophy of science. While I was doing my masters in philosophy of science, the thing that I was very interested in was free will and like like a million other people on earth. But something that the thing that came to my mind back then was that I was more interested in long term agency and not free will in the metaphysical sense. And I thought that we waste a lot of time thinking about the metaphysics, but actually the more interesting question is how are we capable of long term agency, especially in the face of uncertainty and so many things that change? What's long term agency? Is that planning? Um, maybe, maybe it is a little planning, but not exactly only planning. So consider the fact that you can order something, a flight and then you can take that flight next year. That is an ability for long term agency, that your brain is not the only thing that's responsible for that kind of long term agency. Or imagine that you promised me that next month you're going to send me the audio file for this, let's say for instance, you're going to send me the high quality, on-cut audio file of this recording. But it's not, it is planning, but it's also something else. It's the fact that you are going to remember, this is a prospective memory thing. There's a social contract of the promise that you have made. There's a number of other components in it. And if you're a person that manages to always sort of deliver at the right time, then you become a responsible agent. Then I would consider you someone that within this particular domain has proper long term agency, right? Or consider the fact that you probably don't have catastrophic forgetting with regard to tie in your shoes. Because of the fact that you're learning something new on a podcast, you're not going to forget that how to tie your shoes or how to cook food or how to set up your audio system. That tells me that you have also another kind of long term agency that's consistent with the kind of continual learning that doesn't erase what you've learned before when you learn new things. So that's another part of your long term agency. So as you can see in the things that I said, I had both things that are within a given algorithm. But I also had things that rely on social structures that somebody's embedded within. For instance, the plane that you're taking next year depends with there. There is not another pandemic that there is no war in the location you're going to that you know, there is not an environmental catastrophe that that airport is entirely burnt down. There are so many things happening, right? So it's uncertain. It's not exactly certain. And your long term agency to some extent depends on how our social structures are resilient towards uncertainty of the world and how sustainable they are. And so when I was in my sort of my first graduate degree, which was in philosophy of science and I was thinking about how long term agency, how can I think about long term agency? I think that was the initial trigger and you will see that everything I do now is related to that. The question I had was how this long long term agency is mediated, but it's because it's very special in humans. I was interested in what kind of structures and functions in human brains mediate them on the one hand and second, what kind of social interdependent structures are necessary to enable them. And as you can see, the first part of it relates to my work that I did in how the prefrontal cortex might enable this kind of multitasking, prospective memory, analogical reasoning, all the kinds of nice things that my favorite region of the brain barabineria 10, which is anterior prefrontal cortex right behind your forehead does. And then. And also, obviously, turns out that we have the, this is the largest site to architectonic area of the human prefrontal cortex. And it's the region in which we have the biggest difference with other nonhuman primates. And then on the other hand, the work I've done on how graph structures of social interactions lead to our memories converging or diverging leads to the second part of that question. That our long term agency to a large extent depends on our ability on our social structures, how our social structures basically augment or sustain our long term agency. And the first one and the second one, as you see, both of them are related to the notion of memory. And so I think that both executive function and a lot of our social structures have very deep connections to memory. And it's not just about brains, but it's also how our social structures are organized. And so as you can see, like that question that I started with during my master's degree in philosophy of science, you really drove everything that I did later on in science and I still do. And that's the reason I moved to competition on neuroscience to a large extent. So it's interesting. What Jane said, becoming more interested in the social aspects as she learned more, that's kind of the way I feel too, although I'm resistant to be dragged into the social aspects of thing. But just because I want to stay on the brain, dammit. But of course, it's all connected. But you, so the social aspect started you thinking about memory and structure and representations. And that was the trigger that set off the rest of your career. And the social aspect started when I was thinking about long term agency and this idea that what we really mean by free will is that we can do things further into the, is that we have agency. And that when, you know, it, so it's funny because some people seem to think free will means acting randomly. If that was the case, then, you know, particle random gas particles might have free will. But actually the ability to be able to have agencies further down in time, that's the thing that requires a real kind of agency. And I'm not, so that's why I moved away from the word or from the compound word free will, which has a lot of, I think, confusion attached to it in, in compatibility tests and also in people who associated with being random or spontaneous as opposed to actually being able to deliver a year from now. And so I focused more on that long term agency. So that long term agency was the philosophical idea that I was interested in coming from a background from a country where it was difficult to maintain it because social structures were opposing it, but seeing how people's smart algorithms would overcome it and make somehow make it work, right? So all of those things are very much tied to that era when I was trying to figure it out philosophically. It was very tied to questions I had asked since I was a child, but it was very much what drove me to figure out, okay, the field I need to go to to answer all these questions is cognitive and computational neuroscience. So I can ask questions about social effects on agency about, or specifically it's not just social interactions, social structures. So I wanted graphs. Like I loved graphs since I was very young, just like anything that has to do with graphs and how representations in the brain would mediate it and how there's particular structures of the human brain that makes it possible in a way that it's not possible as far as we've seen in our evolutionary cousins or the species that we know. What's before we move on because I have a thousand questions. What's something that's just out of your reach right now? Something that I would die to want to answer, no, I wouldn't do that. I said that metaphorically. We get it. Yeah. Something that I would really love to answer is. So there are of course different kinds of mental learning. One thing I'm very interested in is how algorithms are mental learned throughout evolution. So not just certain parameters, but algorithms. And second, how the human prefrontal cortex makes it possible to sort of to learn and drive it with many, many different algorithms, more possibilities of algorithms than are again our evolutionary cousins. So I think something that's very interesting is algorithms are limited the further earlier in evolutionary time we are. Algorithms of a given organism are more limited. Given organism can have one or two algorithms very early on and then it grows and there is more that they can do. But there are limits for many species obviously for humans too. But in a very meaningful way, we are capable of running so many different algorithms that I agree with those people who say that human nature is not fixed, but it's defined socially and together with the algorithms that we agree upon to a large extent. Obviously not 100%. But I think France, the world made this point very compellingly comparing chimps, bonobos and humans suggesting that humans can overwrite their algorithms or determine their algorithms and behave like different kinds of nature so to speak. So I think that the idea, so those two things, how different kind of architectures capable of different algorithms evolve, but more importantly, how the human prefrontal cortex enables many different algorithms within the same. And can we build algorithms like that that are capable of flexively just, sorry, can we make architectures like that? That's our capable of just flexibly hosting many different algorithms for lack of a better word. Jane, the prefrontal cortex is just an LSTM that that in acts all of these different algorithms in its dynamics, right? Can't you just answer this for Ida and then satisfy her curiosity? You know, I was just thinking it's very interesting that our just out of reach questions seem to match what we have each done. So we should probably have a lot more conversations about these things. Yeah. Yeah, I mean, well, I wouldn't necessarily say that the prefrontal cortex is just an LSTM that, even though maybe you'd be forgiven for thinking that that's what we're saying in our paper. But yeah, I mean, I was on Ida's, the wonderful learning salon to talk about these kinds of things not too long ago as well. And I, you know, been thinking a little bit also about meta-learning in evolution and how you can have these very long-scale learning loops that emerge and that gave rise to our ability to have a brain that can learn across our entire lifetimes. I mean, at some point, some kind of mechanism must have kicked in that allowed it to bootstrap itself to be able to give rise to this kind of flexible learning that nonetheless persists. And we continue to learn throughout the course of our lives. What about us? Oh, go ahead. Sorry. Well, I mean, I would say that I also wouldn't say that I know the answers to these questions. I also think that, you know, I share Ida's desire to know much more about these things. I think that it's a very much an on-tap area. Am I right to think of meta-learning as an algorithm itself so that, just interrupt me. But, you know, so Ida's question was about the sort of explosion of algorithmic capacity and meta-algorithmic learning. I don't know what you would call it, but is that something that lands with you? And because I think of meta-learning and multi-task learning and all the lifelong learning approaches as different algorithms to do lifelong learning. But I guess what Ida was asking and you should clarify, I suppose, Ida is whether there's a way to exponentiate algorithms themselves and have them meta-learn. I guess one thing that I wanted to highlight in that question that I think about a great deal, to be honest, is what are the principles, learning principles of architectures that can host many different algorithms? Not just a driving force of a particular algorithm, not just meta-learning of a particular parameter in an algorithm, but the learning principles or meta-learning principles for architectures that can host many different algorithms without mixing them up, without catastrophic forgetting that it's a dog and thinking it's an eagle for lack of a better analogy. So that's what I had in mind. It seems humans are capable of a lot more. Maybe I'm optimistic about human capacities, but we can just imagine as a parent, as a partner, as a friend, as a child of somebody, as a colleague of somebody, as someone who's a customer to get coffee, as someone who's just crossing some border during COVID, we take so many different, very detailed roles very quickly and very flexibly. And in each of them, not only do we have different algorithms, it's almost weird different selves. And obviously that's another thing. When I get older, I think I hope I get old enough to, at some point, start writing about the self and how this different... You got to have a bunch of cats to do that, I think. There you go. Cats who think they're eagles. But then we do that a lot and very fast. Within the same day, within moments, we switched these roles. You switched a role from being a parent in the pandemic to just turn on the camera and you turn into a teacher or you're at an important board meeting or somebody's a judge or... So this ability to this flexibly adopt many different algorithms and switch between them without forgetting to breathe or forgetting to talk or forgetting your other languages if you speak multiple languages in different meetings. I think that ability is there interesting. And I mean, it's definitely not necessary for survival. So that doesn't seem to be the end goal. So if you just set the end goal of some systems to survive bacteria, some bacteria colonies can survive millions of years. It's unclear. It's excessive almost. There's something... Maybe it is towards... We are the increase of... We are kind of... The new architecture is increasing entropy by allowing to many different algorithms. And that's how we're participating in the expansion of the universe. But it's definitely not parsimonious. And that's something I think about a great deal. We are not parsimonious entities. Our evolution is not parsimonious. There's something... And I want to understand what is that learning principle or something else that we haven't maybe addressed, which is not just the simplest solution to my self-surviving and my offspring surviving. But maybe it is connecting to the next question that you have already sent us in a list of questions. I don't know what that is. I've thrown them out basically. So you had this question about meaning. And I think about this a great deal. But at some point, due to our social structures and language of everything, we created some other kind of collective organisms. And they are now the ones who are trying to move forward. And it's not individual fitness or the survival of individuals anymore in our species. Something else is going on. So I'd love to talk about it more. And I'd love to hear what Jane and also Alana have to say about this. But... Do we want to go down the meaning road right now? Or I was going to ask about humans specifically. I'm open to go down. Any road you guys want to go down? No, humans first. Why not? Well, okay. Because Alana, you study language and vision, but I'm going to focus on the language aspect of it. And language is supposed to be the fancy human thing that we do. And so, you know, so AGI, right? Artificial general intelligence is supposed to one way to look at it as human-like AI. And that that is a goal of AIs to do human-like things. And what Aida was just talking about is the part of what she was just talking about, thinking about these higher, you know, long-term agency and these higher cognitive functions that humans seem to excel at, specifically, relative to the rest of the animal kingdom, et cetera. So, okay. So more of the X paradox, right? That's the idea that the things that seem hard reasoning, chess, go, those are the easy things to do. However, the things that seem easy, walking, catching yourself when you fall, et cetera, those are things that are hard to do in AI, but easy for all animals to do, right? But are these, okay. So my question is, is what's more impressive in humans if we're going to stick to humans here for a moment, language, or this long-term agency, this higher level ability that Aida was waxing poetic about? Yeah, I'm not sure that the second could exist or that the first. Do you think? Like, I think language is the thing that helps us to create the agency that helps us to build the society that allows us to have the long-term agency that we enjoy today. I'm not sure that means that one is more important than the other, but I just think one is sort of dependent on the other. I agree. But aren't we finding out with the language models that language isn't all that hard? Oh, geez. Is that what you think we're finding out? That's what that means you can have. Well, so there, I know that just like there are adversarial examples in vision, and we haven't solved vision, et cetera, and they fail in different ways than humans fail. But just the ability to string together sentences that you could interpret having some meaning, and I guess we can bring meaning back into this. It seems, and you could use, like you're saying, you compare vision and language, and there are a lot of similarities between them. So that might lead one to believe that there aren't that many differences algorithmically, computationally, between how we process language and how we process vision. It's an open, you know, that's why I'm asking you guys. Well, so I guess my comment, so I've, in my work, found a lot of overlap in computer models of vision and computer models of language. But really, I mean, when I'm talking about, I'm talking about like this single word concepts. So like CNN's representation for a ball is more similar to a, I don't know, a bat than a car or something like that. And the same thing is true in computer models of language. So that's true. But then I think when you get beyond the, even like you were saying, like they generate these sentences that you may be able to interpret, I believe that's true, but I also believe that part of that has to do with the creativity of the human mind. So we see a single sentence and we can think of an interpretation of it. But I think many of us have had the experience of interacting while many of us computer scientists have had the experience of interacting with a language agent like DBT2 or whatever and finding that it very quickly runs into a wall, right? And it's actually not as coherent as you might have thought it was. I think there's still a lot left to be done there. I don't think we're anywhere near having language solved at the global scale. Possibly at the local scale, we can generate the next word in a sentence given some context in that next word might be reasonable, but trying to have a back and forth language generation, like a dialogue system is still pretty far from optimal. So I was just going to say I agree with everything that Alana said. And I think that, you know, just to add that there's a difference between language and communication. I think that these generative language models are really good at essentially babbling and they're good at pretending, I think, to be communicating something. But can you really communicate if you're not go oriented, if you don't have any kind of reason to be saying these things? You're just saying it to sound like, you know, whatever data set that you train these models on. Because the last function is completely wrong, right? The last function is all completely off-base. It's like, generate the next word in the sentence. That's not what people are doing, right? People are doing something much higher, much more goal oriented. Are they though? Aren't you right now? Aren't you trying to get the interesting and provocative things right now in the podcast? Isn't that what you're doing? Yeah, but there's some random number generator in me also that is sort of producing what's coming out of my mouth. So I mean, because the- That's not very good at randomness, just a bit clear. I grant you that. But, you know, one of the things that you're interested in Alana is, you know, figuring out how to build language models that understand meaning, right? And okay, so we can get back to meaning. And I don't know that we understand in the way that we believe language models should quote unquote understand. My particular understanding is fleeting and shallow and I barely scrape by. And I don't know that I have this, I guess it would be a benchmark of whatever we're trying to build of understanding and meaning. And so maybe I just don't understand it. I think if we measured your brain activity as you experienced the world, we would find that you do understand it. And we would find that there's differences between what your brain is doing and what our computer models are doing. Yeah, I would hope so. So I like to think of like, so the brain- when we do brain imaging, we're measuring the artifacts of the algorithm. We're measuring like the intermediate stages that the algorithms wrote outputs in order to do its next step. Just like the ticker tape in the turning machine, right? Like we can see the states that are happening, but we don't- how trouble seeing the actual operations. And I think if we could do that well in humans, we would have some sense of what people- what people's algorithm are, it is we might be able to trace back to the actual steps you were doing. So even if you're not really able to write down why you came to whatever conclusion you came to, why you decided to say the next sentence. If you were able to see sort of the mid-steps in between you understanding what I had said and you deciding what you were going to say, we might be able to trace it ourselves, even if you can't recall it. Do you think- do you think that I- maybe I'm misunderstanding you? Is that what you were getting at that you had this- you're not able to explain your own reason for day and sense? Well, that's sort of somewhat adjacent to what I was saying. However, I was going to ask Ida anyway, so this is related. So Ida, you know, you studied successive representations and you know, there are all these different algorithms for learning the structure, learning a representation, learning the structure of a cognitive map and so on. But is it possible that these algorithms aren't discrete? So this is what I'm getting at a lot of by maybe it isn't discrete in the algorithm in my own brain. Maybe there are so many damn neurons, there are so many different algorithms that are mixed and matched. I could almost invent one with my random number generator, invent an algorithm that learns a representation and have some confidence that I could find a correlation of that in the brain. And I don't know, Ida, if you have thoughts on that, like if we really probe the brain, do you think that the algorithms are discrete or that it's really very graded in terms of how the brain is actually computing these things? Sorry, I'm getting heavy. I don't know if- and I'm trying to- I think I'm failing to connect what you're saying to the current discussion, but let me try to try to understand it. There is what a neuron does and there is what- there is a space that we infer based on the activity of populations of neurons and their connection to behavior. Usually when we think about representations that are abstract or representations in general even when they're discrete, at least the ones that I study are within that state space. And even when it comes to place cells and place fields, that the successor representations, the closest thing it can do to the cellular level, it's about place fields, which can be- which has a particular principle of it, and that even is a relational concept that's relating particular neural activity to behavior. And so some of the state spaces that we are talking about have discrete representations and some of them have more continuous representations. And some neurons seem to be firing- most neurons seem to be firing in a kind of a continuous way, but some of them are- some things are sensitive to particular thresholds. So it's almost a little bit more close to not being continuous. But I don't work at the level of cellular neuroscience to say anything about that. That's completely outside of any realm of expertise for me. I work at the computational algorithmic whatever level that- marians and anti-marians would like to call that is not the lowest sort of implementation at the complete cellular molecular level. But what I would say is that the state spaces that are inferred using math, using neural firing, using behavior could have sort of continuous and non-continuous representations. Does that answer your question? Yeah, yeah, a little bit. You know, I guess I'm thinking in terms of- so the big success story with reinforcement learning in neuroscience in an AI is model-free temporal difference learning, right? And then we've advanced and there's model-based learning. And then there are those two compete. And now we have- now we're getting- there are more and more and more different algorithms that we are building. So the successor representation is an in-between kind of thing. But maybe there are- how many different reinforcement learning algorithms is our brain running? You know, is another way to ask. So let me answer that question by saying that TD learning, temporal difference learning is also used to learn representations in the successor representation. It would also be used if you don't know the model of the world and you're learning in the model-based, you're learning the states by learning state-state association, state-action state-association. So it's not that temporal difference learning we saw it, it was just model-free and we're throwing it away. So learning principle that can underline different kinds of learning algorithms. Now if you only believed that only model-free and model-based learning existed, then you would have to believe that the brain does both. But you can have other kinds of algorithms that look like that algorithm could address both of those kinds of behavior. And just to connect this to maybe this is what you meant by the last question that you asked, there are continuous versions of deep successor representations as well. So it doesn't necessarily just need things to be tabular. So this tabular versus kind of rich continuous environments, there are algorithms for both. I personally like to focus on the tabular first and then go towards deep because I'm more interested in like basic algorithms that you can test on behavior and then go towards the deep ones. So there is an infinite number of algorithms that we can come up with, like virtually infinite number of algorithms that use very similar adjacent learning principles that might address a series of behavior but not other things or a subset of behaviors or a subset of combination of neuroscience findings and behavior. So to think about giving a number and saying match number eight. Come on, I'm probing you, you gotta give a number. So whether or minus plus two or things like that or three or two or one or 42 or whatever your favorite sort of way to answer these kinds of questions is or dual systems. Everybody loves dual system and then the third way and then there's a lot of ways that one could do this. But what I'm trying to highlight is that although we are those of us who are trained in the Middle Eastern slash European culture of thought are very into dualisms but it doesn't mean that that's necessarily the algorithms that we would find that there is two and when there is a three oh no chaos. I think that we should think in terms of learning principles and just develop as many algorithms as needed until we have something that captures a lot and I think I'm on the side of all the algorithms that we have eventually there will be a better algorithm that captures more things and my goal is just to make sure that I refute my own algorithms or improve them during my own lifetime and somebody else would do something else. So I don't think that I personally don't think that you will find the exact model for your model base in the brain. I think that these are these could fit in some situations but not others and we are just trying to improve the algorithms based on similar learning principles that all of them use just with small differences here or there. Some of these differences are make a huge difference like model three versus SR slash model base huge difference because it just doesn't care about states representations or interactions of states SR and model base they get much closer because they actually care about the representation of states in absence of rewards. There is learning happening even without rewards which means this is the first algorithms first or at least minimum or L algorithms that can learn something like latent learning. That said if you go in the deep RL domain there are some things that they call deep RL that is model three but it's not really model three because it has so many layers that is actually learning some things about the states in the middle of those layers. So I think there is some care that should be taken into account when we call those model three because they're not really model three especially when they have many layers that are learning different kind of representations to reach to that point. I think there is nuance in the terms there is qualitative differences at least in the tabular domain between model three and SR and model base classes of algorithms but there are also similar learning principles underlying all of them such as temporal difference learning and I don't mean just temporal difference learning of rewards as in model three but temporal difference learning of the successor states or temporal difference learning learning of transition structures. So Jane, I just says the answer seven. Do you agree with that? I did. I agree with that. I'm wondering if you have just from a meta learning perspective and a continual learning perspective if you have thoughts on top of that. I guess I mean one thing I was thinking when you first asked this question and you were saying you know how many RL algorithms does our brain hold or at least I think that was the question but I guess I don't think that our like reinforcement learning or even learning in general is necessarily what our brains were built to do or is like all that they do and I think learning itself is a continuous concept and that we should take a more broad approach than just to say you know algorithms or we want to learn about we need to understand learning only in terms of the kinds of algorithms that can encompass learning. I think that there is basically as a consequence of us being embodied in the universe and us needing to persist, there are various adaptations that different organisms take in response to the challenges to those to be able to persist essentially and so I think learning is just one of those solutions that we have happened on that helps us with faster adaptation and that's why we're here and not some other organisms. So I think it's all just a in pursuit of something else right and I think TV learning might be a nice way and a simple way to encapsulate the way that organisms tend to do this where you know we have some sort of target we have some optimal point that we want to get to and then we can get to it via these tiny adjustments right towards that optimum but I think that that's all just a it's part of a larger or more like generic system like dynamic system that works the part of. So I mean, I'm getting a little bit philosophical and abstract I guess right here but you know I just I just been inspiring me too. Oh wait wait I'm going to disagree with you there though because I feel like some of the things that you said might be contradicting each other. It's it's one thing to say the capacity for learning didn't was not the ultimate purpose of life or how life adapted but learning especially expansion of brain architectures that can learn better is it did become one of the let's say auxiliary sort of things that emerged so it's not that we're saying that nobody's saying learning is the ultimate goal of the brain but it is one of the principle things that it does it's a subgoal to our something else maybe it's in it I think it's no longer just individual survival hopefully so hopefully it's gonna we can retrace it towards ecological survival as opposed to ecological suicide but I guess I just I don't mean that I mean certainly the like learning is is what the brain does but I guess I don't see it as being distinct categorically from other kinds of adaptation that maybe you wouldn't consider like brain like to be a brain you know like I you can you can I think maybe on one end of the spectrum a riverbank is learning the water that flows over it and erosion is then going to be the result of that you know plans to a certain extent to learn as well and they they adapt in respect okay so I was saying I at one one end of the spectrum so like I would I wouldn't necessarily say that a riverbank is learning but it is adapting in a sense and I think that that's just one extreme of a continuum at which the other end is the human brain I think I can see your physicists intuitions by the owner but both philosophically and on cognitive scientifically speaking I would disagree that erosion or I wouldn't I'm not as interested in a frame that considers erosion the same as let's say mushrooms that learn a particular path because the actual structure of changing yourself is different from the dynamics of things that happen to be in each other's way and influence one another I think and so it might depend on how I think life is something that it makes a difference for me things that are living versus not and I guess this goes back to 19th century discussions of the importance of life but I think that for me things that learn are living things unless they are AI so they can discuss whether AI is a living thing or not or like artificial learning is learning or not how different is it from a riverbed how different is AI from a riverbed because it's you know silicon is extracted at the end of the day it's extracted millicum minerals but I think that the ways in I agree with you where I do agree with you is that there is definitely a continuum between let's say mushrooms and organisms that learn collectively and you know learning that we think about in terms of learning in the brain I mean to be fair T.D. learning seems to be what happens almost in you know the even even sort of very early forms of life that basically move towards the gradient of move randomly but as soon as they perceive food move towards increasing gradient of food that's already like you know I mean it's not learning but it's already using something that has to do with temporal difference comparisons right in decision making I feel like I need a definition of learning versus adaptation oh geez yeah so do you guys know the Constructo law in physics the one the Constructo law it's actually a physical law I think it was like 15 years ago but it's all about he uses riverbeds as an example of it's all about life being a flow process and you know it goes back to a heat wanting to flow well and riverbeds wanting to flow well and our cognition wanting to flow well and it ties it all into basically whatever takes the fastest path to flowing well people can look it up I suppose sorry I interrupted a lot of but but the riverbed example reminded me of the Constructo law in physics so I'll send you guys a link I totally can see that many physicists would see it and many kind of I would say poets would see it the same way I think that the side of me that's there is a side of me that can't that likes this idea of thinking about cognitive flow in the same way as a riverbed etc but at the same time I do think when we are specifically talking about learning and let's say TD learning there are living beings that don't have a brain that seem to show similar principles but I wouldn't consider what happens in a riverbed to be able to relate to it so I do think there's something special in certain forms of learning that's specific to or certain at least mechanisms of learning that's specific to living things I like to hear what a learner was going to say to want to you want a definition of learning right yeah I mean I guess I just feel like following a chemical gradient or or even evolving to follow a chemical gradient I just feel like it doesn't satisfy me as a kind of learning like it was evolving to move either yeah evolving to move randomly until you hit the chemical gradient then following it that evolution is not some kind of learning and then the next phase of the evolution is you know they develop eyes for instance or they develop that's the next stage and then I just I think it's interesting I think about what's the what's the place where you hit learning I mean does learning have to be something over because like evolution is just a random walk in genetic space right I mean not not entirely random because some of them the ones that do do there many of them die out so at the end of the day it's not that random I guess the sector random although the success is not very yes so it's a random search the mutations are random yeah but the finish function determines well okay yes we did have a session at the learnings along with Sam Gershman which regarded learning in the single cells so there are some people who think learning can happen at the single cell level and Alana I don't know like how much I don't know how further down in the sort of the cellular level you have worked but I haven't worked at the single cell level myself so anything that I would say on that would be based on what I've read and what I've heard but I think there's and we did have this discussion too whether this is learning or not so it seems like some people especially people who don't work at the cellular level don't like to call this learning but you think that virus can learn yes it's true that's right but what's that do you think COVID has learned yeah I just was listening to this podcast um well actually anyway uh they were talking about like oh the flu is like this cunning virus that's figured out and somebody said no the flu isn't clumsy virus the flu has tried every damn thing and the only thing that's left is the one that works it's not smart right and so is it learning it's just sort of fumbling towards the right hand isn't that how we learn also we have to as I was thinking I was thinking like what's that it's like don't you try a million things literally right now I'm out loud trying a million things and seeing which ones of them work right which is it's a there's this concept there's this concept of a goal right that is like goal driven and that if maybe maybe you consider the riverbed or a seagull cell organism that doesn't necessarily have a goal they just sort of have the consequence to I don't equate riverbed with single cell organism I think there's a huge difference huge difference what how do you just find that then why does a single cell organism have a goal but a riverbed does not have a goal what do you think life is do you think a riverbed is alive and your rocks alive our rocks do you need to be alive to learn unless you're talking about an ecosystem it seems like you're not talking about an ecosystem you're only talking about the rocks and the water going on the rocks no I I am talking about a system I'm talking about the entire the the water no I don't mean a system I don't mean a physical system I mean an ecosystem in which there is life and there is a sustaining life so that's different there is a difference between just water and rocks and an ecosystem where there is other you know there is actual life forms well I mean but at our fundamental level we are just a set of differential equations right that we're not at that level we can program at a fine enough level we can program any organism I don't I agree do you think we can program rocks I mean I'm talking about a dynamic system right a riverbed is a dynamic system okay so I'm I'm asking you if you think there is a difference I understand that this perspective could see anything as a dynamical systems and I think on some level we can see all of them as dynamical systems I'm asking if you think there is a difference between a living thing and a non-living thing yes I think so I don't think a rock is living so do you think the rock is a single cell is living I mean I guess according to the bio the biological definition of living yes but I mean you know what if you ask me like if I consider that the earth is living or if I consider the earth is any way it's not just you consider ecosystem to be alive yes yes I I'm not the only person either they breathe they you know that they have like cycles they have certain things that sustain them you and I each of us is an ecosystem in fact we are there's so many bacteria is living in and around and on us that are sustaining us entirely and if that ecosystem of other species that are living within us gets messed up we get messed up so the earth is in in some way similar it's any any ecosystem is a living thing but it's different from just rock and water or two physical objects that are hitting each other well but you can take to you can take any physical system and complexify it enough and you can zoom back out to a large enough level that it can look living it can you take any kind of complex system I mean you look rough all complex systems are living I think they they from a certain perspective they can be yeah so internet is living I think definitely the internet is living so the internet is the same as a single cell the same notion of life no no I think I mean so by your definition you consider the entire earth to be living any ecosystem to be living but they that doesn't necessarily check all the boxes for biological life which is they have to you know reproduce right or they have to I think that there are different ways that you can consider things to be living ecosystems do or produce and do sustain themselves they do have seasons they do change they do you know I don't think the earth has been used to itself yet and if it does we should really find out other earth because we're in trouble well with the multiverse by reproducing earth but the ecosystem is not the rock that is the planet with a heated core that's the one that it seems like what we are destroying is actually the ecosystem on top of it which if you do reforestation that's actually like replenishing it so it does get replenished if the acedrops or particular number of acedrops and other forests emerges that's what I mean by ecosystem not their heated rock or like cooled on the surface and heated at the core rock that is not what I meant by reproduction I'm not suggesting that planets reproduce as in they either duplicate or they have I don't know they they mate and create a new planet that's between the two I certainly love that story I would love to see a book that's a science fiction about that that's very cool but that is definitely not what I would suggest saying it's similar way though I can see the internet as being self replenishing right yeah because if you mean the internet as in the system of computers plus the humans that connects them that's a different that that definitely is a different kind of entity because there are humans attached to it they are alive so as soon as you connect that you can consider them it's a different story is there anything that has something living within it is alive but then you need to define what it needs to be alive for the entities that are inside the thing that you're calling alive yeah biological yeah but I just I mean I feel like it's uh this is not a confusing thing at all to be honest with you it's very clear to me that a bedrock for in a river and a cell are not alive in the same way and the notion of learning that we can apply to a cell and anything that's multiple cells is different from the notion of the way that a bedrock might change its shape because water flew over it for a million years I that's something that I think it's not for me it's not a continuous thing it's clear but I agree with you that if one wants to take the perspective of dynamical systems that might be if you just want to look at it from that perspective and not from the perspective of what what at least we mean so far by the notion of learning there are continuance if you want to see whether you can get inspired by the dynamical systems that you evolved that you developed to study the bedrock and then use those dynamical systems and equations to study learning I can totally understand that different story though I think sorry this took more long this took longer than I thought yeah this was a little bit of attention I regret my rubber bed example sorry about like we have five minutes left should we bring it back down to earth no pun intended yes please you know there I have a lot of different questions do we do we want to end up talking about cognitive maps and are thinking about whether we have that right do we want to talk about the balance of tip you know unsupervised self-supervised reinforcement learning and supervised learning in the brain or actually maybe we should talk about meaning and and whether meaning is just a relation between concepts and or whether we need grounding like what what meaning is because that's what you're interested in figuring out with language models if I if I'm correct what is meaning Alana what is meaning how do we get meaning well end on an easy one yeah I don't know that's a I think there's a lot of lots of the facets to that that are hard to untangle but there's there's certainly something that's emerging in our computer models of language that that seems to be getting closer to meaning but it is interesting to think about that like our computer models have no concept of what the real world is and so they say cat but they don't really understand cat in a grounded way is that necessary to understand the world like I mean I kind of feel like it is especially for some kinds of common sense reasoning I'm not sure we can get the whole way with the kind of text that we are using right now to train our models although I did hear one interesting thing sort of proposed by someone that if we had the right kind of textual data we might have enough that if we had we it might be enough to if we had the right kind of text data not just any old random internet data so if we had people in dialogue that might be closer to what we need in order to get grounding because it might be more about the world rather than just sort of adjacent to the world can you elaborate briefly what grounding entails so grounding is understanding the connection between a representation that lives in our brain or representation that lives on a computer and the real world right what what is cat cat is a thing in the real world then what how would I know if I saw cat how would I know if I heard a cat what's the what's the connection at least that's what is to me I think grounding is probably one of the biggest things that these language models are missing in that they are not I mean the the train on text generated by humans that are grounded that are living in the world and are talking about things that are grounded but the language models themselves are not grounded because they are sort of ingesting you know just data from all different sources and they don't like care where it's coming from and or how it's grounded and so I think maybe that's the distinction in my mind I think maybe some of that distinction I'll just say that even if there was a meaning at some point that these we could ascribe to this language models understanding it's not the same meaning that we understand it's a different thing based on the objectives that they were provided of what kind of task they were trying to solve it's not the same meaning that we use language for and so I think that's an important distinction too because of the same things that Alana was saying that and also Jane was saying that we have these goals and that's why we're using these languages to communicate but the reason that that model is using language is because of some objective function that was defined for it it's not the same goals it's not the same motivation for communication it's not the same thing to sort of sustain a life etc so I think that in this way that they are even if at some point we managed to magically just like you know move a wand and then they're going to have meaning it's not going to be the same meaning we have you want to say one thing there that like it's interesting to me that two models let's just say computer models kind of have very different goals and end up with representational spaces that are actually pretty similar so I do agree that without similar goals the representations will never be the same but it's interesting to me that we actually don't have to have the same goals to have very similar representations right I wasn't talking about representations but understanding so I was saying that the objectives that humans have and the objectives that the language models have are so different that even if they had some notion of understanding it's not going to be the same understanding that we have let's say of the work cat or dog unless they start to interact with it in a human like with a way as well which they are not so I don't think that the distinction between objectives in a language model and the way the objectives of language in a human I don't think that distinction is close enough that we would either have like you know a similar understanding or that similar representations unless it's on a very kind of broad notion of representation but I wasn't talking about representations but understanding or meaning so I love where for this conversation with I have no idea how useful it is but let's let's end on a very practical notion and then I promise we'll wrap up Alana most people go through academia they do the graduate work they do postdoc they get faculty and then and or go on to industry you kind of did that backwards by starting at Google and then going into academia is that a recommended path what do you think maybe benefited you and do you recommend and or you might have been missing and do you recommend that to other prospective humans. I think every computer model should work at Google I think so working at Google was fantastic and I was like like a hundred times a program where I was when I left and compared to when they started so I think that that benefited me a lot when I got to grad school because pumping out code was no problem at all and a huge amount there. It kind of fast tracked your your faculty position as well you went faster I think than most. Oh well so skipping the postdoc isn't completely unheard of. Maybe in the computational world it is in the experimental neural world. Because it pieces like deep-minded Microsoft there's a strong pull for people to go to industry and so there's you can get go straight to faculty from my PhD. So that was good for Google also I got a down payment for a host by working at Google I also highly recommend that although I was living in Pittsburgh as you know in Pittsburgh as I find out some Pittsburgh's pretty easy. I mean I think it was a other right decision for me it also really helped me to solidify my goals and and made me realize that it matters to me why I'm solving the problems I'm solving. I love working at Google because the problems we solved were challenging and they were extremely technical but the reason why we were solving them to me wasn't the sort of thing that like got me up on Monday morning excited you know what I mean so like but if you're the kind of person who just loves solving technical problems for solving technical problems and like it's a fantastic place. Where I was would have been a fantastic place but I wanted to solve a problem and be sort of passionate about that problem. I didn't Jane you're both simply half an academia now half an industry I suppose do you have any reflections on that or perspective to close out? My perspective well I mean I yeah consider my job to be mostly like really similar to academic I wouldn't say that I would really even know what it would work that would it would be like to work in a more sort of industrial or like a commercial technical setting because I don't you know I'm not working on production level code or any kind of creating any kind of products or anything I do mostly research and write papers and give talks and you know do do fun things like this so I guess one thing I mean I always get a lot of questions about you know career path and things like that and I guess if you're going to be in industry or doing industry research and you but you might want to go back to academia it is important but so I guess continue to publish and you know like if I were to like I don't think it would be a problem for me to try and look for faculty positions right now if I wanted to leave you know the industry research type of position but it might be I don't know if it would be harder if if I kind of you know didn't do any research type of like didn't have any research output for a few years and then going back and yeah I don't know that a lot of people know that way I mean I think yeah a lot it sounded like you went to grad school and so that sort of is a more standard way of I guess like becoming a research professor I know we're late do you have any additional comment to wrap up? I just say for me the the objective was to be able to do my research and so I did have academic offers and this offer from Microsoft Research and other offers and what I chose with the one that would enable me to do the research that I wanted to do and so I just for anyone who might want to try academic the sort of pursuing research outside academia I would just say that there are many different paths working at at industry research does not equate coding at all in fact we have some computer science grad students who finished their PhD and they say that they wrote barely a hundred lines of code because they use math to prove things and in fact there are people at Microsoft Research who spend most of their time proving that particular algorithms would converge as opposed to writing any code and then they collaborate with someone who's like comfortable coding and they would provide maybe the deep learning part and not every industry research requires to be related to product this is something that Jane mentioned as well if you want to connect to products you can but that's like extra it you don't need to it's not necessarily required and like Jane was saying I'm also completely focused on research and mentoring and doing exactly what I would do in academia other than internships are shorter the students are better paid so they're in a better mood maybe but and the only thing that I guess I would miss a little bit is I actually enjoy teaching and like having the same student for five years and being responsible for them as opposed to having them for short stellar internships is a different kind of responsibility for the growth of another and I do sense the kind of inclination in my self-tour that so I think at some point I might either go back to academia or take on more of this kind of co-advising rules and like you know some some forms of like teaching that I do but like maybe more do more of that but other than that if someone doesn't like those parts if that's not something that they like there there's really room for doing very good high level research industry. Go back to academia as soon as you get that down payment for the house I think that's that's wise. I was not in a research position at Google I was like a reference here. Right. We didn't see a little machine learning but it was it was totally for production. Yeah that's why I was asking you to begin with guys this has been a fun meandering down the river bed that I think this is a really good way to close out the neuro match panels here so thanks for being with me I appreciate it. Yeah, thanks. Thank you. Yeah, fun. Brain inspired is a production of me and you. I don't do advertisements. You can support the show through Patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science. Go to braininspired.co and find the red Patreon button there to get in touch with me email Paul at braininspired.co the music you hear is by the new year find them at the new year.net thank you for your support see you next time. The covers of the here
Hello everyone and welcome to the Mindscape Podcast. I'm your host, Sean Carroll. There's a kind of history myth that sometimes gets promulgated in, I don't know, elementary schools maybe or just folk tales we held each other. According to which when the first European explorers landed in the New World, the indigenous folks saw them and thought, oh my goodness, these are gods coming to visit us and we need to worship them and they're too powerful to deal with. Turns out nothing like that is actually true. This is a story that the Europeans made up after the fact to make themselves look good to justify some of the things that happened. Nowadays we are being faced with a new set of visitors from another world, namely artificial intelligences, whether it's large language models or some other kind of constructed program that in many ways can act human but has a different set of capacities and we're learning to deal with them. And unlike the myth of the European explorers landing in the Western hemisphere, today there are a bunch of people who are quite literally willing to say that these are gods coming to deal with us. I know there's also plenty of skepticism out there but there are people who think not only that AIs are going to be in our human level intelligence and agency but well beyond that, superhuman godlike creatures that we're going to have to deal with. I am myself not of that opinion. I do not think that that is actually what is going on. But just like the landing explorers, AIs do have different capacities than we do. They're trained of course, they're designed, they're made to in many ways act very human but they're really not. They're thinking in a different way, they're capable of something much better than we are and other things not nearly as good as we are. So how do we think about this world in which interacting with AIs, interacting with computerized systems more broadly is going to be a crucially important part of how we live our lives. Today's guest is Tina Eliassirad who is a computer scientist whose work spans the space and why I really like it from very technical stuff just you know, how do you better detect certain nodes or communities in an abstract network that you have embedded as some sort of data but then also the human side of how you deal with this stuff, how these computer systems, how these AIs are going to affect our lives and we're going to affect them all the way up to human AI, co-evolution. You know, once we build these systems and then we interact with them and then we use them to decide how to go shopping or decide how to find a romantic partner, guess what? That affects who we are, how we live our lives and the survival strategies we're going to have to move forward in this very brave new world. Again, many positive aspects here, there are things that you know, we don't want to do, we don't want to bother doing or it's hard to do for us as human beings that we can outsource to the AIs. There are other ways in which it's very dangerous. The biases, the bad things that we have in our own brains can be inherited by the AIs and they can have new failure modes that we human beings don't have. It's a world that is changing super duper rapidly obviously as a lot of research is coming in and a lot of influences are out there. It's not all about necessarily writing the best program. Some people who are very good at writing programs want to optimize for making the best money, right? And we have to take that into consideration when we consider what to do, how to regulate, how to control, how to optimize for our own actual goals rather than just seeing what happens next and living with the consequences. So the more informed we are about what the possibilities are and how to deal with them, the more we'll be able to do that. So let's go. Tina Eliassi-Rod welcome to the Binescape podcast. Thank you. Thank you for having me. You know normally I like to start the conversation with someone talking about like the most basic stuff, the things everyone knows about. For your stuff I kind of feel like going in reverse order. Like you know we'll end with the fun stuff about AI and democracy and things like that. But let's start with understanding graphs and networks and things like that, especially using neural networks to understand things that human brains can't quite wrap their minds around. So like what is the most general way of stating what it is that you're trying to understand when it comes to thinking about graphs and networks? Well, when you're trying to understand the phenomena usually you have multiple entities like multiple people and they have relationships with each other, right? And so when we're looking at graph like machine learning with graphs or graph mining, we're trying to find those what we're calling relational dependencies. That like the probability of you and me being friends given that we both like Apple products is greater than the probability of you and me just being friends, right? Or the probability of me liking Apple products given that we're friends is more than the probability, the prior probability of each of us liking an Apple product. So the second one that is you are we are friends, you influence me and so I like Apple products and I buy Apple products or I buy this headphone, right? Headset. And the first one is that because we like similar things, we become friends, this notion of homophily or like birds of a feather flock together. But in a nutshell, like people who work on machine learning on graphs, network scientists who are interested in understanding phenomena, network science is an interdisciplinary discipline. It is about these relational dependencies and like what can we find? Where are the patterns? What are the anomalies in their relationships that get formed? So for the audience who wasn't there, what Tina is not telling you is that we spent 10 minutes before the podcast struggling with our Apple products to make the recording work, but we still use them. So I guess take whatever lessons from that. Okay, but I guess in the current era, the issue is you have too much data or at least in principle one would like to imagine having too much data. There's like so much stuff, right? Is it is a large part of the worry like how to pick and choose what to pay attention to, what to draw connections between? Yeah, there's some of that. I would say that so I have this thing I called the paradox of big data, which is like there's a lot of data, but to predict specifically for what Tina wants, it's difficult, right? You don't have maybe as much information about Tina. Now if Tina belongs into some majority group, then maybe you can aggregate from the majority and say, well, Tina is part of this flock and so Tina will like whatever this flock likes. But really, I feel like the problem these days is more about exploitation and going with things that are popular than exploration, right? Like in the past, we would go to the library or the bookstore and you're looking for a book and you would find other things and those were, you know, they basically, the cherry on top of the cake, right? The cream is like, oh yeah, I found this, right? And now we're really not getting that, right? So when you use all these recommendation systems, whether it's Google or any other Amazon, etc, they oftentimes show you what is popular or what they believe you would like. So in a past life, I worked at Lawrence Livermore National Laboratory, which is a physics laboratory. And like when I would do searches there, and this is many years ago, I would get more like physics books than like when I lived elsewhere, they would sell me, they wouldn't show me as much physics books, right? Just based on the location, the zip code. And so there's some of that that's going on. And I feel like that is more of the problem of like not really serving the individual or exploring as much as possible. So thinking though, like purely like a mathematician or a computer scientist, faced with these big networks, how should we think about them? What are the tools that we use to tease out what are the important relationships? Yeah, so you know, it depends on what kind of network it is, right? So in social networks, for example, we know that there are two dominant processes, the form social networks. One is closing of what we're calling wedges. So if I am friends with you and you are friends with Jennifer, then I will become friends with Jennifer, right? We close that triangle. And in fact, if you and I have, for example, many common friends, or let's say me and Jennifer, in my example, we have many common friends and we are not friends, then there is something going on that there was lots of opportunities that we could become friends, but we chose not to become friends, right? Now there's also, of course, partial observability and that, like maybe I didn't observe it, right? However big your data is, you're not omniscient, you don't see things, but we do expect that friend of a friend is also a friend. That's one. The other one is this notion of preferential attachment, right? That everybody wants to connect to a star. And so you're interested in, like, basically, those are the two big patterns, and then you look at deviations from that. So it worked that was done by John Kleinberg at Cornell. He's a very well-known computer science professor. This is a while back, was think Facebook, for example, who is your romantic partner on Facebook? And his colleagues showed that basically you are the center of a flower and you have petals around you. These petals could be your high school buddies or college buddies, et cetera. They have just more triangles in them, and people who fall outside of these petals and have a lot of connections to these petals are either you're sibling or you're romantic partner, that is you are introducing them to other facets of your life. And they showed that when that connections stopped, establishment of those connections stopped. It's a leading indicator that you will break up. So you were talking about which connections to pay attention to, right? So those are some of the things that are fun when you look at social networks. I mean, biological networks are totally different. So in biological networks, it's a whole other ball of wax. It's not like you're not looking for common friends. You're looking more for like complementarity between different proteins that serve some function. So it's interesting because it seems like an attempt to go from syntax to semantics in some sense, right? You're going from structure to meaning, broadly speaking. You're trying to understand what is going on. What is the underlying process that is happening in this network? And why are these links exist? Now, the one thing that makes studying of graphs and networks really interesting is that it is not a closed world. So just because like you didn't see a link between me and Jennifer doesn't mean that we're not friends. And so for machine learning, where you need both positive examples and both negative examples, which negative examples do you pick becomes difficult because the edges or the links or the friendships that don't exist may because like they don't want to be friends or for other reasons. And so this, what are the negative examples becomes an important aspect of things? Well, or as you were giving the example, I was thinking I don't interact with my romantic partner on social media that much because I interact in real world. Indeed. Indeed. So there are lots of assumptions being made obviously in terms of like how the network is being observed. And in fact, this is one of the big differences between computer scientists that study graphs and network scientists that are typically physicists or social scientists where for example, they're like, well, there's a distribution and this graph fell from it versus like the machine learning graph mining folks typically don't question where the graph came from. They're like, oh, here's data and they run with it. Right. And it's just it boggles the mind that like you should think about where this data came from, how was collected, what were maybe the errors in collecting it. And in fact, this touches on a sore point for me because what happens is they don't question the data, right. They just like feed it into their machine learning AI models. And then on the other end, they don't measure any uncertainty. So like if you have something like let's say a social network that you've observed, there's all this stuff about like representation learning, right. Where basically I take Tina in the social network and I represent her as a vector in a Euclidean space, right. Like maybe with 60,000, a vector with 16,000 elements in it. So the cardinality is 16,000. And there's no uncertainty. They're like, no, Tina falls exactly here. And it just doesn't make sense at all, right. And so then those kinds of models given that you didn't start with, okay, well, my data could have some noise in it, some uncertainty in it. And then you don't even capture the uncertainty of the model at the end. It just there's lots of problems that can occur, including, for example, at the serial attacks or like your model is not just going to be, your model is not going to be robust. Let's just put it that way. This episode of Mindscape is sponsored by BetterHelp. When it comes to relationships, we often hear about the red flags we should avoid. But what if we focus more on looking for the green flags in friends and partners? If you're not sure what green flags look like, therapy could help you identify them. Actively practice them in your relationships and embody the green flag energy yourself. Whether you're dating, married, building a friendship or just working on yourself, it's time to form relationships that love you back. One of the great things about therapy is by looking inside yourself, you can both learn to take those warning signs seriously, but also learn to be open to new experiences and new things. To know when something might be worth pursuing. BetterHelp is a fully online service that makes therapy affordable and convenient, serving over 5 million people worldwide. You can easily switch therapists any time at no extra cost. So discover your relationship green flags with BetterHelp. Visit BetterHelp.com-mindscape today to get 10% off your first month. That's BetterHelp-HELP.com-mindscape. Well, this sounds just like full employment for enthusiastic graduate students, right? Because how hard could it be? I mean, it could be hard, but it's very well defined with the problem that you just set out. I mean, allow for the existence of noise in these descriptions and see how your answers change. Yeah, I think in part one of the reasons that folks, at least in this CS side, the computer side, the machine learning side aren't too bothered by it these days is because we are going through this era where prediction is everything. Prediction accuracy is everything. And so, you know, there are these benchmarks and it's basically benchmark hacking or state of the art hacking, right? And that's basically what is going on, you know, that's reality of it. And so, so there's a lot of that kind of engineering going on, as opposed to like really thinking about, what is the phenomena that I'm interested in? How is the data coming to me? What are the sources of noise? Should I, how should they take them into account? Should I even take them into account? And what are the uncertainties in terms of the predictions that I'm out putting? Let's help the audience understand the idea of benchmark hacking because that's probably a cool but important one. I mean, what's a benchmark and how do you hack it? Yeah, so basically you create a bunch of data and you get it buy in from the community that these are good data sets to test a machine learning or an AI model on. And then there's a leaderboard and you want to be number one, right? And so, you hack the systems that exist or you hack your own system, you create your own to be number one, you know, as much as possible. And that's basically what is going on. And I like this metaphor. So my colleague, Barabashi, said it's like there are two camps. There's like a toolbox. It's a finite toolbox, right? And the machine learning, the AI people, the engineers put tools into that toolbox. And because it's finite, it's very competitive. That is my tool beats your tool, even if it's like 1%, by 1%, that it's not clear if it's statistically significant or not. And I may be king for only 30 seconds because another tool comes in, right? And then there's like the scientists on the other end that just opened the toolbox and say, okay, well, what is good for whatever, you know, whatever prediction task I want to do? And then they pick a tool out of that. And so a lot of this like benchmark hacking or state of the art hacking happens on the engineering on the AI machine learning side, the computer science side because you want your tool in that finite toolbox. But on the science side, the physicist or social science side, the people who are interested in these models that create the sets of data you have, there's also as I understand it, a lot of worry about degeneracy or or over determination or under determination where very different physical models could give you essentially the same kind of graph or network. How big of a problem is that? It is a very big problem. I mean, there are multiple angles to this. So one is, for example, because of all the hype, oftentimes people on the engineering side don't talk about the assumptions that they have made or, you know, the technical limitations of their system. And in fact, because of that, we have this reproducibility problem. So not even a replicability problem, but a reproducibility problem, which is just a quote, can I just reproduce your code as you have it, right? And even with your training data, even with like how you broke it up, with these different like folds or whatever, you know, and so which is like very very low bar to pass. But that doesn't happen because there are lots of assumptions that are being made, etc. And then there's this notion of, you know, we are living through this era of like big models, right? So I want a model that has many, many, many parameters, you know, even if I don't need all those, all those many parameters. Or for example, maybe I do care about interpretability that is I want to know what the model is actually doing. But because again, for that one or two percentage point on the prediction side, you let go of it. And you know, you go with the bigger model office. But yes, it's a big, big problem of, you know, for me, like the lowest bar would be that we require at least with federal funding, you know, and in some of the service that I do for the federal government, I've been pushing this. I'm not going to be a very popular person, but that if you get taxpayer dollars in your report to the government, you have to have a section on assumptions and technical limitations. Because the problem is the way the peer review culture goes is that if I have a technical limitation section in my paper, the reviewer will just copy and paste it and say rejects. Right. But the federal government is going to do that, right? NSF isn't going to do that. NSF has already given you the money and you're doing that in a report. And so it has to be, come on, just be honest. Right. Like I did not test this method on biological networks and they're very different than social networks. So like caution. Right. Well, this is because what you do for living matters a lot to the real world and to money and things like that. Unlike the foundations of quantum mechanics that I do, I don't need to worry about people being overly concerned with the results. They're all willing to give me a hard time anyway. Okay, so one, I have this sort of philosophical mathematical problem. I don't know. I mean, if I have a graph, a big graph, so some nodes, some edges that are relationships, and I have a different graph, how is there other measures of similarity between them? Like if I add one node to the graph, is it a completely different graph? Or is there a metric I could put on there? How much is that even understandable? Yeah, I love that problem. I've thought about that problem. What, so the issue there is similarities and I have the beholder, right? And it depends on the task itself. So similarity is an ill-defined problem. And so you can say, okay, well, I can go with something like an edit distance. Like, okay, how many new nodes do I have to add to graph number two? And how many new edges do I have to or remove to make it look like the other graph? And then try to solve the computational and hard problem of isomorphism, in fact, alignments, right? And in many cases, you don't need alignment. So for example, you can think about two networks and you have started a process of information diffusion on it. Like you started a rumor, let's say, right? And you would just measure like, how similar there's this rumor, the same rumor travels through network one versus network two. And if like, you know, it travels similarly, let's say, you know, I'm going to throw some jargons. Like the stationary distribution of a random walker that is spreading this rumor becomes the same at the end. You would say the networks are similar enough, right? And so you don't need to have like the sizes exactly be the same. So it could be, for example, you have a social network of France and a social network of Luxembourg. And you started a rumor in France and in Luxembourg. And they are processing the same way. And you would say the networks are similar, even though one is much, much bigger than the other. That makes sense. In fact, because that, because I was going to ask about when you have a big graph and you somehow coarse-gray in it, right? Or you know, you group subgroups into single nodes, you want to somehow have the feeling that it's still representing the same thing, even though you've thrown away a lot of information. Yeah. Yeah. Now the problem was like grouping nodes. This is a very important problem and has been studied by lots of people within like graphs. It's called community detection. Basically, you want to group similar nodes together. Now, you can have different functions that you define about what similarity there means. It could mean that like, these people just talk to each other more, right? So there's more connections between them that what you would expect in a random world, right? Or just more connections between them than other folks. Now, this kind of community detection, Aaron Closet, who's a professor at Colorado, showed that there's a no-free lunch theorem there. And actually, it was Aaron Closet and others. And I think actually Aaron was the last author. So I think that first author is Leto Peel. But you know how it is. You usually just name your friend. Yeah, let you know. My apologies to the other authors. But they showed it in no-free lunch theorem, which basically means that it is not the case that there's like one particular group of or one particular collection of nodes that you're grouping that would give you the best or the true communities. You see what I mean? So because when you are doing these grouping of nodes, you have some objective function that you're trying to maximize. And basically, the idea is that there is no one peak there. So there's not like one particular community that you can put Tina on and say, if you're Tina belongs here, that's where she has to sit. And so they become some of that becomes an issue. But there's an ocean of what is what does it mean for one network to be similar to another network? It has its tentacles to community detection, to clustering of nodes, and all of those are ill-defined. So it really is driven by the task at hand. Okay. I mean, I guess I'm spoiled by caring about what probably in your world would be the simplest possible case. Because I think about, you know, the emergence of space from some set of quantum entanglements or something like that. And it sounds all very fancy and highbrow, but basically something is entangled with something else if it's next to it. And there's this very similar spatial or a very simple-minded spatial coherence. But of course, in social networks, I can be connected to people anywhere. And that makes it a more complicated problem. Yeah. And that becomes what we call the small world problem, right? That you or the Kevin Bacon or the or the Eradish number, right? Like you don't have to go that far out to be connected to famous people. And so I mean, how good are we these days at detecting real clusters, communities, figuring out what's going on just from knowing about a graph and the connections between the nodes? I mean, for downstream tasks that you can like have some, let's say, confusion matrix where you can draw like true positives, false positives, true negatives, false negatives. We're actually very good at it. But if it's about like, okay, I found these communities and do these communities make sense, it kind of breaks down into whether they're like hard clustering, where you put T9 to just one community or you put T9 to multiple communities. And then there's a little bit of just like eyeballing it in a way. If you do not have this downstream task that you can say, okay, here are the true positives, here are the false positives, so on and so forth. But in many cases, it's difficult to place a person and a social network only in one community because people are multifaceted. Right. But you started with an example of being given recommendations by Amazon or whatever. And sometimes it, the algorithm fails because it's not picking up our idiosyncrasies. It's just giving us the most popular thing. And is that tie into the well-known problem of polarization or extremization of network recommendations? Like you're, you're, everyone is pushed to some slightly more extreme set of YouTube videos or Reddit posts or whatever. I think they're in part, they just want your attention. And so the objective function is such that, you know, they just want to hold your attention. And so they will show you whatever necessary that will keep your attention. And so if they believe that like my tie to Brandon is very strong, that we have a strong relationship and Brandon found these things interesting, then they will show it to me as well to just test it to see whether, you know, they can capture my attention. And then through that, they can show me more ads, for example. I guess that makes perfect sense. So like the point is, if Amazon wants to recommend things to me, it's not maximizing the chance that I want this. It's maximizing its profit. Exactly. Exactly. And so they kind of go hand in hand. And in fact, this touches on this issue that we have written a couple of times about, there was a nature perspective piece a while back and we've more recently an AI journal piece on, in a way like human AI, core evolution. So if you think about it, when you're using Amazon, when you're using YouTube, when you're using Google, you're providing data for them, we talked about this, right? And they take that data into account and they make recommendations. Those recommendations then affect what you do in the real life. And then you go back and you provide the more training data. And so there's this kind of feedback loop that goes on and on. And it's oftentimes not captured in terms of who's influencing who most. And one example that I like here is like, think about dating apps. There was a story recently from Stanford that like most people are meeting on online dating apps, yes, days, instead of like through college or through their friends, family, et cetera, or at the local bar. Now those dating apps have recommendation systems, right? And based on those recommendation systems, perhaps you meet somebody, you partner up and you have babies. And so over time, these recommendation systems actually have an impact on our gene core. Oh, okay. I had not quite gotten that far, right? Yeah, but it's like as opposed, and so and because these recommendation systems are all about exploitation and not exploration, but maybe you would say like my answer, my grandmother or my college were also all based on exploitation and not exploration, right? But there is this notion that there are these algorithms that we can't understand what they're doing. And perhaps 100 years from now, they may influence how our gene is a how our genome is evolving. Well, we are part of the world and we create the world reflects back on us, right? I mean, it reminds me a little bit of discussions about extended cognition theories where you count your calculator and your pad of paper and whatever is part of your brain because you keep information there, you do calculations, et cetera. And so our environment and who we are is being increasingly populated by these artificial algorithms that we put out there. Yeah, I don't know like how far do we think certain things are going? And the society has it the sign like, for example, New York Times had this article a while back about how this there's a person who's trying to set up a company and online dating company where like on the first or second dates, which are usually, you know, not very good, my avatar and your avatar will go on the date and then they will report back. And only if, you know, both avatars are happy. Then on the third date, we actually go out on the date. And so like how much of actually our human behavior are these things going to take over? Is it? So I didn't see this article. Do you, what's your actual opinion? Is there any chance that that would help? I think like, I'm an introvert. So I'm like, oh, and also I'm a computer scientist. I'm like, oh, this is great. Let somebody else do the dirty work and then maybe, you know, if it's a good day, I'll get out of my cave and I'll like go and talk but you know, I for extroverts, they don't like it at all. So my husband was an extrovert. Like, what are you talking about? Am I just the brain in a fast now? Like what's happening? You know, so I think it depends on where you are in this extrovert introvert. We should also reveal to the audience that Tina has the good or bad fortune of being married to a philosopher. Indeed, indeed. For 30 plus years, it's been fantastic. So yeah, so the evolution, I mean, I was going to get that later, but it's so good. We have to talk about it now. Co-evolution of humans in AI and my guess was when I heard that phrase, we were thinking more about cultural evolution, right? Memes more than genes. But of course, they're interconnected with each other now that you say it. It's obvious because our cultural effects of our behavior, our behavior affects how we pass genes onto the next generation. So AI is going to be affecting the population genome of human beings. Yeah, and I think in particular with, for example, generative AI as it's generating content, whether it's texts or video or images, and there's this notion and the late Dan Dennett, who you had on your podcast, very famous cognitive scientists, called these generative AI models counterfeit people. Yeah. He had an Atlantic article a few years back about it. And so, and also because people treat these generative AI systems, these counterfeit people as if they're more objective somehow, they know more than me that, you know, people tend to give their agency to them. And so, and also these AI systems evolve faster than us. And so it's not quite clear, not that it's a race, but it's that they're evolving a lot quicker. Their objective functions are different, like attention, money, etc. Then perhaps objective function of people, like maybe the good of the society or public good or something else than just like money or some like GDP or some, some, some measure like that. A lot of us start the new year saying that we will learn a new language, but it's hard to actually commit to it. Babble makes it easy to learn one in less time than you think. Babble's quick 10 minute lessons handcrafted by over 200 language experts, get you to begin speaking your new language in three weeks or whatever pace you choose. And because conversing is the key to really understanding each other in new languages, Babble is designed using practical real world conversations. What I love about Babble is you can either dive in deeply and truly get fluent, or you can just master some of the basics before going on a trip. So let's get more of you talking in a new language. Babble is gifting our listeners 60% off subscriptions at babble.com slash mindscape. Get up to 60% off at babble.com slash mindscape spelled B-A-B-B-E-L dot com slash mindscape. That's babble.com slash mindscape rules and restrictions may apply. Are we good enough that we could at least imagine some kind of new equilibrium that we get into when we're tightly coupled with our AIs that there is some happier state of being we could at least aim for if we're working together well or is it too much influx these days to know much about that? I think these days is too much influx, but I think for example there's certain things that can be done to improve it. Whenever you or another human being asks me a question perhaps I would come back with another question. I'm like did you mean this Sean or did you mean that right? Well for example with child 2 p.t. or these large language models they never come back and say like did you mean this? The reason is that it reduces their utility right me as a human being when I ask the question I want an answer and I want it now right? Or like it never comes back and says I don't know or I'm not sure of it and maybe you would accept that from a human being but you don't accept it from a large language model. You're like oh you're a tool you need to tell me like I asked you about this and I want to answer now and you know and so there's some of that going on but like the big tech companies could add those features to make it more equal in terms of this conversation that is going on but but at this point utility is winning overall. But utility is tricky you know I was I was talking with chat GPT or whatever the other day and I was trying to get it to imagine and maybe I didn't try too hard I don't you know I didn't really put too much effort into it but like I was trying to imagine a character in a fictional narrative who was very insulting and who would you know give out some good insults and I said what are the wonder some good insults that it could give out but it wouldn't tell me it's like oh no you shouldn't give out insults you should talk to people politely like it's really programmed not to go down that road. Yes it is there are actually other generative AI systems especially for programming that I've heard where like it tells you like okay if you want to code acts this is how you code it and then you code it and you're like oh it didn't work that was your stupid to the generative AI like the humans is your stupid and then the human the generative AI systems to the human you're not a good programmer you know so then there's some kind of you know then they get at it but that's only like for you know specific ones you're absolutely right with with with chat GPT it's not going to be that kind of antagonistic and I know I mean this is probably related to the the big worry that a lot of people have had about bias in AI algorithms I mean if you if you've trained well if you train AI on human discourse and human beings are biased then of course the algorithm is going to be biased it's not because the computer is biased it's because you you've trained it on data that is and is that is that something that your tools can help us deal with? I mean you can try to find biases I mean there's a lot of work in that like these large language models are sexist misogynists we wrote a report for UNESCO for last year's international women's day about how sexist and misogynist these these large language models are the problem was that is whenever like I get somebody asks me that question that oh well look humans are biased too the problem is that I can hold a human accountable I can sue a human being who am I going to sue you know I mean and especially in America we're very litigious and so then this gets into accountability and if I go there's a lot of work and the government for example our government is putting a lot of our tax dollars into like trustworthy machine learning trustworthy AI etc etc and to me it rings a little hollow because there's no accountability like how can I trust you if there's no accountability I feel like they go hand to hand and so there's some of that going on which is like you know who am I going to sue am I going to open AI because it's sexist and misogynist like one of its products is sexist and misogynist you know that's not the case right now well and human beings I mean this is an ongoing cultural flashpoint so I mean it's there's a lot of different opinions about it but human beings might at some point think of something to say that we know is inappropriate and then we're smart enough or we have enough controls that we don't say it is that a kind of thing that we that it makes sense to try to implement in the context of a large language model perhaps right this thing is at this point what it gives out is what's the most probable and what it believes you will like right so it's yeah it's a it's a two function it's a two-place function what's probable on what you will like but yes you could definitely do that and there's this comedian unfortunately I forget his name now but he was saying the secret to a long marriage is to never say what comes to your mind first or second always say the third thing that comes to your mind right and this goes back to what you were just saying like maybe you should just say this third thing the third most probable thing and in fact along those lines usually the students who use these generative AI tools for like math problems of a math homework the first answer is usually wrong because a lot of the answers that have been uploaded into like course hero etc. except that they're wrong usually it's the second answer that's the correct answer oh that's very interesting is that actually true or is that like a feeling that people had these are just anecdotal right like I haven't had anybody to like a systemic study of this but that like usually the first answer is not quite there well it's interesting because one of the things we discover it you discover we in the in the royal we thinking about these very very large data sets is a sort of sometimes you can predict even more than maybe you thought you'd be able to I mean I want to ask you about this paper that you wrote about using sequences of life events to predict human lives that sounds interesting but also maybe scary yeah yeah so in the true like computer science AI machine learning sense we're very good at coming up with names for our systems so we called it life to Vex so we're just putting your life into a vector space whether you like it or not yeah that's okay but you're just the vector in this vector space now that basically the idea is that if you look at these large language models right so they're analyzing sequences and so as human beings we also have a life story that's a sequence right and so I was lucky enough to work with a group of scientists in Denmark so if America has a surveillance capitalism in Denmark they have surveillance socialism so there is there is a department there department of statistics they call it like ministry of statistics that collects information about people and so we had information for about six million people who have lived in Denmark from 2008 to 2020 and we were like well can we write stories for these people in a way and then feed it to what is the heart of these large language models a transformer model which is basically just the architecture of a neuro network that learns association weights for within some context window and that's what we did so but instead of so for example chat gpt goes online and gobbles up all this bad data that that or that people have put in all the misogynistic sexist data we didn't do that so we had very good data from this department of statistics and we created our own artificial symbolic language and then we fed that artificial symbolic language for these like six million people into a transformer model and we then we were able to like predict life events and so one of them that caught the media's eye was will somebody between age of 35 and 65 pass away in the next four years and we picked that that age range because that's a harder age range to predict for like if you're over 65 then it's easier to predict whether you're going to pass away in the next four years and if you're younger than 35 it's also easy the other way right year unlikely to pass away and so that's one of the things the other prediction task was like will you leave Denmark you know so then you can predict for that but it had this similar technology as these large language models which is like you have this one what they call like predefined where you just learn based on the data that you have what's likely to happen next and then you fine tune it for whatever prediction tasks that you have. What does it mean in artificial symbolic language like literally a human language or is like some logical encoding? It's a logical encoding because it would because the data that the department of statistics has in Denmark is all tables so it is not like this kind of sequence so then you could say like Tina was born in Copenhagen and December blah blah blah right and we could like generate like a natural language but that's difficult why would we do that so then we generated a vocabulary for this artificial symbolic language and then and that was actually a lot of the intellectual property of the work is like okay well how do you take these tables and then create this artificial symbolic language that then you could give to a transformer model. And and and what's the answer are we likely to die for 38 years old how do we know? Well the thing the thing that we found which was very interesting I think so like the accuracy in terms of the model was about like 78 percent etc and I think that's why people were showing a lot of interest in it but to me that wasn't really the takeaway the takeaway actually was that labor data is a very good indication of whether somebody in that age range is going to pass away in the next four years or not because health data is very noisy and inconsistent so even in Denmark where they have universal health care it's not like everybody goes to the doctor all the time and you have good data for them and so some of the some of the indicators of like whether you're going to pass away one was whether you're male we know this right male tends to do two more crazy things female oh yes I can jump over this movie no problem right and then the other stuff was basically just what which sector you were working in right so if you're like an electrician it's a bad thing it's not a very good thing right as opposed to looking office worker right so some of the so the labor data was actually very very helpful than the health data how important is it to extract causality from these relationships like maybe risky or minded people just become electricians yeah maybe yeah we didn't do any any any any kind of causal stuff right like a lot of the work a lot of the height that's happening now in AI and machine learning they're all on the correlation side yeah not on the causation side so we didn't look at that at all about what causes what that's very difficult and I haven't touched the field of causation in part because I'm married to a philosopher and so it's like no like I go there because every time I try to approach the topic I just heard nightmares and so I haven't gone that way there are some issues there yeah no absolutely but I guess I mean it's interesting is it is it too much to draw a general lesson that by looking at these large data sets we might find simpler indications of what we're looking for then we expected like you know you might like you might have said okay how many calories is somebody ingesting is the important thing to look at but that you look at the data and you learn no what is their job that's what's the important thing to look at yeah I think that there's some of that I think the best way of using this is perhaps government policy right when government issues of policy and then like maybe 20 years from that you have if you have good data you could see okay what has been some of the correlations that have come about based on this policy and then maybe you know the actual social scientists and political scientists can then draw some causal diagrams from what we find because the one thing is usually like from the computer science AI machine we treat causation or correlation as if binary right as it's like a coin this way or that way but that is really not the case right it's more of a spectrum and so if you have a model that is producing robust predictions there there is some underlying causal model you just don't know it and then maybe that could steer you into the right direction for that kind of work but we didn't we didn't look at that for this particular work so human beings of course are examples of complex systems themselves but this raises the you know the larger question of human beings will eventually die for whatever reason complex systems have their life spans right or maybe they're infinite I don't know but they can also change dramatically and die and that's something else you're interested in trying to tease out in a general way yeah and I'm very interested in the feedback that we were talking about and like how do we capture that feedback between for example when I go and I'm using Amazon and Amazon is making me these recommendations and then I buy things I tell my friends and then all of that data goes back into Amazon and like how much does like my contributions or my friends contributions amplifying what Amazon is doing and so there's some of that going on and then there's also in terms of like society is a complex system and the place of these tools in these systems so the tools that help us spread misinformation and disinformation make our society unstable in that then you're not quite sure what you are reading is true or not right so right now with the fires in LA there's a lot of misinformation and disinformation going on and it's like who do I believe and maybe like you believe LA times and you you believe you know what you read in ca.gov and so on and so forth but not what you're seeing on Instagram and so there's this notion of the place of these AI tools within our society and whether they're making our society better or worse and by better or worse here I mean stable versus not stable more chaotic and I think we can all agree that we would like to live in societies that are more stable than not right yeah so so there's some of that that is going on and I have a new project along those lines which actually touches on philosophy which is called epistemic instability which is what are some stability conditions of what you know so if you genuinely know that whales are mammals no matter what I show you perhaps I won't be able to convince you that a whale later neck you're like a whale is a mammal and mammals do not lay eggs right and you're very sure about it right but then you start talking to me and to chat GPT and maybe if you don't know something then you're like as well as you thought right then I then you're malleable right then I can like change your mind and then now you have groups of people who are talking to these within themselves and with these generative AI tools and then basically you go from like individual to groups to this hypergraph notion and what I'm interested in is when are face transitions in this hypergraph in terms of what the society believed like maybe the society believed that vaccines are good right and now all of a sudden the society doesn't believe that vaccines are good right and what are the leading indicators of those kinds of face transitions in our society as it's being modeled by conversations formally represented as these hypergraphs. Ready to electrify your drive? Hyundai's cutting edge EV lineup is about to change everything you thought you knew about electric vehicles prepare to be captivated by a range that's as bold as it is brilliant from the lightning fast dionic 5 and dionic 6 charging from 10 to 80% in a mere 18 minutes to the tech pack cabins boasting highway driving assist and blind spot collision warning Hyundai EVs are redefining the electric experience and with america's best warranty including a 10 year 100,000 mile limited electric battery warranty you'll drive with unmatched confidence Hyundai's EVs aren't just the future they're the now you've been waiting for learn more about Hyundai's EVs at Hyundai USA.com call 562-314-46034 complete details america's best warranty claim based on total package of warranty programs see dealer for limited warranty details see your Hyundai dealer for further details and limitations yeah i mean i guess that's a it's a good example i hadn't quite thought of the vaccine thing the traditional example that i hear for sort of a social phase transition is opinions about gay marriage right where it was universally against it somewhat rapidly changed to generally four but this is the vaccine stuff is more subtle right because it's not that the whole society is going against them but about half for whatever right there's this political polarization and there's sort of more than one consensus being built up is that is that just my impression or is there some idea that the modern informational ecosystem lets us have these sub larger sub communities where they have their own sets of beliefs different from other communities yeah i think it's the second one in that like in the past when you did have people that tend to be on the fringe they would people wouldn't hear them but now even if you're on the fringe because of the information technology that we have you can connect to other people who are on the fringe and then you believe oh no we're bigger than the fringe yeah we're actually in the middle right and then that kind of a thing spreads um so so that is one of the things i'm interested in regarding gay marriage one of the things that was interesting is i was talking to a philosopher who had just taught for a very long time at the Ohio State University and he and he was teaching ethics and issues related to gay marriage and abortion and he was saying that with gay marriage similar to what you were saying he saw a shift in terms of opinions uh for against gay marriage mostly for but he didn't see any change when it came to abortion and i think that had to do with the vagueness of when is uh let's call the thing a baby right when is the actual feed as a baby or whatever you know and so and that vagueness because like we could all agree that maybe like the day before you're about to give birth obviously you're not going to do anything you we all believe it's a baby but that vagueness is something that doesn't shift the opinion on on abortion so much for or yes and i like that vagueness aspect of it so there are certain things that are vague and maybe you will never have that kind of face transition okay and then there are certain things like the vaccine where like there are people in the fringe that our information technology allows them to connect to each other and so it feels like a bigger thing and then maybe there are other aspects of information that really do make people change their mind uh just based on talking to other people and so they're not as sure or as stable in their knowledge so i like the hypothesis that the vagueness of the proposition makes it harder to have a phase transition how would we test that hypothesis is that something that uh we can sort of sift through the data and figure out whether or not that's on your track so it's it's a work in progress right now for us on this i'm trying to stay away from making it a psychology or a social science problem because then you get all these confounding factors and that's what i said it has more tentacles to philosophy so in terms of what people ought to do in terms of their knowledge and how sure they are of their knowledge and so right now the way that we're representing the knowledge or like what you know these things as vectors because i'm a computer scientist everything is okay it's all the derailles basically how much does this vector space move in one direction versus another so as you talk with others so you can build these like kind of simulations right not kind of you can build these simulations in terms of in terms of conversations and see how much the vector space shifts so i mean one thing about complex systems is they can survive a long time like the human body you know uh fends off attacks pretty well because it's complex enough to to catch things the other thing is that they can sort of go into this wild uh negative positive feedback loop i guess and uh and crash right like the economy or something like that so is this something maybe this questions too vague but is are we learning general purpose lessons about complex systems concerning what features they need to be stable versus what features they need to be delicate yeah so there's a book uh by um ladies and and we and and we'sner and i know that you had James ladies and your podcast as well he's a philosopher at Bristol and Caroline we's there as a mathematician at potstown now about what is a complex system and their book uh that came out I think in 2020 talked about complex systems in terms of features and how there are certain like necessary features and there are certain like emergent features and then there's a functional features where like for example our human brain is a complex system and as you were saying like if it has a shock it adapts and it's still perhaps can function unless the shock is like catastrophic and so what we are not seeing if we tie this to for example the AI models and how they are operating within this system is we don't know even the role of this AI system like how much instability is it causing in the system right how much feedback is it causing in the system how much memory does it have right because they're evolving so quickly that it's not it's not quite clear so this is like an open area of study uh of like going through these different features of a complex system i'm trying to see okay well how do i measure it for let's say um a a chat gpt right yeah uh in fact a lot of people say oh well you know it doesn't have a good memory based on like what i told it yesterday kind of a thing right so memory is one of those features that i that a complex system has okay so i guess you know and and one of the important applications here that you have talked about explicitly is democracy right democracy is a complex system and um democracies do fail sometimes and i guess one way of putting the worry is that or at least the the interest is that the introduction of AI as a new feature in some sense uh opens the possibility of a new instability it could it could lead to sort of a runaway disaster that destroys democracy not to put it into alarmist terms yeah i think where it comes and in fact this is how it links to my new project on epistemic instability is that it introduces epistemic instability right like when my dad was getting his PhD in america back in the 60s the most trusted man in america was Walter Krunkite right if he said something you believed him now we don't have such a thing yeah right we don't have a person or an institution where you say okay i read it here and i believe it and then there's also like depending on where you are on the left or the right you're like maybe you you believe in the earth times you believe fox news and so because of that i feel like one of the things that we need to do if we value our democracy is teach our kids' critical thinking right just like don't believe what you read or what you hear question it right does it make sense talk to different people and make your own decision and don't give up your agency but that's a hard task right thinking is not easy and people don't want to think in the age of tiktok well is that true i mean maybe maybe this true i'm certainly willing to believe that's true but again i always worry about comparing eras right because i was a different person in the 70s and the 70s were also a different time but i don't know what things are common between different eras and things are not like did we really want to think more back in the 1970s than we did in the tiktok era i don't know i think there was less distraction for sure right then then it is now i think the the dopamine hits that we get by just scrolling through instagram tiktok etc is something that has been studied and and you know i'm not a psychologist or a cognitive scientist but that people it's just like you let your brain go to motion you just like spend hours on it instead of maybe actually sitting quietly and thinking about a problem you know it's boring you know yeah okay good so this is another aspect so okay that's that's actually nice despite not really trying to i think that i see a bunch of threads coming together here like uh technology broadly not just AI is giving us new ways to fulfill our own objective functions maybe it's a dopamine hit or whatever but its objective function might not be ultimately our flourishing so there's a absolutely danger mode there yeah in fact that's such a perfect thing you said i always say to my students what is your objective functions right because we all have an objective function and that objective function changes over time right and and perhaps if like all of us just think okay did my objective function change from yesterday or from last month or whatever you know um it would be helpful for society yeah so as a computer scientist as a machine learning person i always think about objective functions and yeah in fact i cannot look at a mountain range now and then i think okay if you drop me there will i find them the peak or not the global peak i probably not but you know like please drop me at a nice place you've co-volved with your with your network that makes perfect sense to me yeah so the gradient is with me exactly exactly right so okay so um the you've said many things about this already but i just want to get it as clear as possible um the trust the the community of trust idea that is so central to a democracy is one of the things that is in danger being undermined by AI right like you probably saw the story about instagram having its AI accounts the sassy black lesbian lady who you know was programmed by a bunch of people who are near near neither black nor lesbian and uh just pure AI and that one was admitted right like they said that was AI and do you personally worry that people are just going to mostly become friends with non-existent human beings in the long term i mean as an introvert i'm fine but you know i yeah no i think i think we see this in society now where like people aren't um as good as interacting with other people or they're not as not as courteous to other people perhaps as before i don't know maybe i'm out of it of an age now where i'm like oh yeah people are not as courteous as they were before um but you know the more you interact with people the better you get at them unless you interact with them the worse you get at them and so if we don't put a premium on like oh look like tina can actually pick up the phone and call somebody and get something done you know i suppose to just like send the isilian emails or text messages i think there's a value to that and i think there is this notion of trust like even the most introvert among us right we there are a few people that we do trust and so if it comes to a point where you trust an AI system that we don't know how it works and that is vulnerable to attacks then that is a problem right and so if i this gets us to this phrase called red teaming that we hear all the time now that oh well that don't worry about it they will red team it and so the phrase red teaming came from the cold war era right so the Soviet Union the red team America the blue team right so and there was a lot of this red team blue teaming for example for cyber security but this phrase red teaming is not well defined when it comes to these garenive AI systems and my friend and colleague professor hoda hidayari at cardigan melon has written extensively about this because there's no guarantee right so you cannot guarantee that somebody cannot jailbreak chat gpt and and and jail breaking is basically that chat gpt has put in some kind of guardrails right like you shouldn't it shouldn't tell you how to like rob a bank but you can jailbreak that and it will tell you how to rob a bank right but there's no guarantees it's not like oh here's a serial the proof qe d go home you cannot jailbreak this and so if you're getting all of your information from these AI systems that we know can be manipulated and we don't know how they exactly work then you may not have a shared reality with other citizens and that's i think the worst for democracy we really do need a shared reality to be able to withstand our democracy to have to hold it and not so how do we get that what do we do you know this sounds very scary but i'm not quite sure what to do about it well i guess as a professor to me is education yeah i think actually you know educating the public and i spent a lot of my time educating the general public and not just the students at my university but generally but but educating the public about how these these tools work what they're good at what they're not good at's not giving their agency to these tools and critical thinking skills i think that that's the way forward but the problem with that of course is that the value of getting an education is also susceptible to this loss of trust i don't know if you saw the recent people were getting upset because there was a poll that showed that young men were becoming less and less interested in going to college then someone else pointed out that if you go into the cross tabs if you look at you know other questions that were asked the there's actually no relationship between male and female versus going to college it's all about republican versus democrat it's that there are more it's a simpsons paradox kind of thing where most of the young republicans are male and those are the ones who become very polarized against wanting to go to college so that's a that's part of the problem you've you've been talking about right like there's a whole new epistemic community out there that is forming and it seems to be solidifying over time yeah um perhaps we should think about how we educate people and maybe they will see the value of education right and that education is about enlightenment education is about empowering yourself right so education isn't like a teacher just pouring knowledge into your head it's it's about you learning about the world and so you could do better in the world yeah you know um like as a teacher i'm already 11 in my guitar right i just want you to do better and if you do better then i will also do better the society will do better and we will all do better right and so i think part of that is maybe we should be thinking about how we sell education do you think that uh a i and associate technologies can be a force for good in education yeah i think so i mean there are certain things that i have i have heard so for example now there's some privacy aspects to this but if you are a college and you are tracking how students are doing on their homeworks etc and let's say Tina took calculus and she didn't do very well on differential equations and now she's taking machine learning and they're going to talk about differential equations that you could tell Tina oh you know maybe you should go brush up on differential equations because they're going to talk about differential equations yeah okay you know so there's some of that kind of a thing to like to uh to help you and then there's also like basically like personalized tutoring um that i think a i can be helpful um there do you yourself uh use jgbt or something equivalent to help figure things out to learn about things i use it for fun like you know like like give me a bio of Sean Carroll in the james style or whatever you know just for i don't use it i haven't used it for any um real uh like work stuff or or anything i actually trust it that's the problem you don't you don't trust it i certainly don't trust it but sometimes yeah i did i did realize that there was a good use case because i was trying to understand you know in in mathematical things uh they will often tell you true things but you don't understand what the point of it is right and i was trying to understand type three fun noise algebra and so i asked and i got chat gpt to explain to me not just what the definition was but why it was important in this particular case and that was actually very helpful yeah oh that's great yeah i asked it some stuff about linear algebra and and matrix norms and it was really bad at it yeah i was like wait but like there's so much about linear algebra that's i think that's no but matrix the problem there's too much like you just said there's too much junk out there when in in some sense if you get technical enough that it knows about it but not so technical um all the goods all the stuff that's been written about it is sensible like no one's gonna make up stuff about type three fun noise in algebra is what would be the point yeah yeah so what so i guess maybe the the point is let's not teach linear algebra to kids and then no no no because the whole of machine learning is basically linear algebra and like quantum mechanics also so yeah linear algebra kids that's that's your lesson for today from mindscape learn more linear algebra i think it's the key to everything yeah exactly exactly but it's very good at like um basically admin stuff so if you like um show it uh uh some like a picture of like google scholar like put it into bib tech put these these references into bib tech that it does it for you so some of those kind of admin stuff it's good at yeah i think that the weird thing is we're trying to use it for creative work whereas the most obvious use cases for the least creative things that we don't want to do indeed indeed indeed yeah all right uh it's it's all very complex and it's evolving and it's it's uh there's a lot of degrees of freedom so Tina Elias-Iraud thanks very much for helping us all figure it out thank you thank you for having me on Sean

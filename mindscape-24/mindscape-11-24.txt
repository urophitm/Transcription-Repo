Hello everyone and welcome to the Mindscape podcast. I'm your host Sean Carroll. Sometimes on the podcast, I will refer to our two cats, Ariel and Caliban. They are born at the same time. Twins, I guess if you can say, but they're a part of a bigger litter. Brother and sister with very different personalities. If you met Ariel and Caliban and interacted with them, even if you didn't see them, you would instantly know which one was which. There's a danger there though. If we want to be a little bit more careful, a little bit more rigorous in using a word like personality, right? We tend to anthropomorphize our pets, other objects in the world. We anthropomorphize our GPS Google Maps system. I feel bad when I drive in a way other than what Google Maps tells me to do and it seems to be upset with me. If we're thinking about it very carefully, we can have fun using words like personalities and being anthropomorphic with our pets, but maybe we want to be a little bit more rigorous. You might want to ask what kinds of animals are conscious? Consciousness is a big topic in some of these debates. You instantly run into the problem that we don't agree on what consciousness is. Different people are going to have different standards for that. We might agree that rocks are not conscious, but maybe pan-psychists will even argue for that. Most of us will agree that humans are conscious somewhere in between. Maybe there's a threshold or maybe there's a series of many thresholds. One way to make this a little bit more careful is to switch the conversation from consciousness, which is a little bit unclear what it means to sentience. Sentience is sort of the ability to have a feeling of what it is like to be something, the ability to experience feelings and sensations. Especially feelings and sensations that we would characterize as having a valence, a good sensation or a bad one, a positive one or a negative one. That's a little bit more well-defined. Then we can go ahead and ask which kinds of animals are sentient and also the public policy question, what should we do about it? How should we act if we believe that certain kinds of creatures are sentient? Or as much as we tend to cutely anthropomorphize our pets, there's also a temptation to sort of ignore the possibility of sentience in animals that are not like us. It is very common to cook crabs and lobsters by boiling them alive. They thrash around a little bit, but you say, well, that's just an instinctive, reflex reaction. That's not experiencing pain in the same way that we are. Regardless of what your opinions about it are, we should be able to think about this. Rationally, coolly, calmly. It's hard because we get very emotional. Some people see the little lobster thrashing around and feel something deep inside a sense of revulsion. Others just do it as a matter of course and how do you have a rational conversation about that? Well, here we are to try to do that. Today's guest is Jonathan Bert, who is a philosopher who's written a new book that just came out. Well, I'll tell you whether it came out or not. The book is called The Edge of Sentience, Risk and Procution in Humans, Other Animals, and AI. Because soon we're going to be building artificial systems that have many of the characteristics of things we would call sentience. The book The Edge of Sentience is just came out in the UK, will come out in the US in a little while, but also is available for free online. There's a PDF that you can just go to. In the show notes put the URL there, OptroDuny University Press is graciously letting everyone read this book because Jonathan is someone who wants to have an impact in the public debate. He already, as you will hear, has had an impact in the UK in government thought about what it means to be a sentient creature and how we should deal with that. This is a set of issues where I don't think we're done yet. I don't think that we have the consensus. I don't think we figured everything out. That's why we got to talk about it. Here we are to do just that. Let's go. Jonathan Burch, welcome to the Mindscape Podcast. Hi, Sean. Thanks for inviting me. We're talking about issues that in philosophy contexts are often brought up, but often the word that we're talking about is consciousness. You're focusing on the word sentience, which is a little bit different. Maybe explain to us what it is, how it's different, why you're thinking about that. Yeah, I suppose part of what I want to do with this book, The Edge of Sentience, is get people using that term sentience, perhaps a bit more. I think Stephen Harnad's been doing much the same thing with his journal Animal Sentience. I think it is a term that is on the way up. Good. That doesn't mean consciousness is on the way down, but I think it's plateauing and sentience is on the way up. It's a term that, at least as I use it, it's an attempt to capture the most basic elemental evolutionarily ancient base layer of consciousness as it were, which is, in part, just what philosophy has like to call phenomenal consciousness, subjective experience. There being something it feels like to be you, whether or not you have any kind of overlay of conscious reflection on what it is you're experiencing. Then also, there's a slight extra component as well, which is that I'm focusing specifically on experiences that feel good or feel bad, like pain or pleasure, valence experiences as it were. It's a positive or negative valence. I'm using that term sentience to capture that capacity for valence experience, like pain and pleasure. I think it's a really important concept because to me, it captures what is really ethically significant. If a system is sentient in that sense, if it's capable of valence experience, then it's interest matter morally and we need to do something about that. There were something you mentioned about awareness. I forget whether I'm imposing that word on you or whether you used it. Conscious experience, in some sense, is something that I need to know. I'm experiencing, whereas sentience is a little bit broader. I could feel something and experience it unconsciously that would still count. I wouldn't say that to have a conscious experience, you need to know about it. The problem is that the term consciousness gets used in quite ambiguous ways and it can refer to the human form of consciousness, which is a very complex form, I think. It does have layers. Herb at Vigol in the 50s talked about sentient sapients and self-hood, tolling in a separate body of work, had these terms, ainoetic, noetic, or tonoetic. They're both ways of trying to capture the idea that there's layers. There's the raw, basic subjective experience, like the feelings of pain, pressure, sight, sound, odour. Then there's also the knowledge and the concepts when we think and reflect about what's going on. Also, to some extent, there's a sense of self as well. This idea, we recognize ourselves to be persisting subjects of experience with lives that extend into the past and extend into the future. These overlays, they involve levels of cognitive sophistication that you might not need to have that base level of just sentience, of just feeling out, feeling pain, feeling happiness, joy. Sentience is then broader than consciousness. We might imagine that there are critters that are sentient but not conscious. In some senses of the word conscious, yes. If you're the kind of person who wants to use this term conscious to refer to that whole package, the sentient sapients and self-hood, then yes, there's going to be lots of animals that are sentient without being conscious. I don't necessarily think we should use the term in that way. One of the things I like about sentience is that it very strongly draws people towards that, that most basic aspect, just the raw subjective experience. I think I'm finally getting it. In other words, one of the advantages, the biggest advantage of sentience over consciousness as a concept to focus on is that it's better defined and consciousness sort of means different things in different contexts. Somewhat, yes, which is not to say that it's perfectly defined. There are real limits on our ability to define subjective experience. The problem with consciousness as a term is that even when you bracket that issue of subjective experience and its mysteriousness, it's still a term people use to refer to many other things as well, like reflection and self-awareness and all those other things. I'd rather use a term that is perhaps a little bit more constrained in how you can use it, and that where people will let you stipulate a bit more. If I say, I just mean the capacity for valence experience, I think people get that. They get the need to have a concept that is drawing our attention to states like pain and pleasure, but it's a bit broader than that, and that is not just about pain and pleasure, but about that whole category of feelings, experiences that feel bad or feel good. And let's look ahead a little bit to sort of tease the audience. Why should we care about sentience? What is the impact of having a nuanced understanding of what that means? Well, Jeremy Bentham famously had this footnote where he wrote in relation to other animals. The question is not can they talk, nor can they reason, but can they suffer? And I think that's a, to me at least, a profound insight that if an animal can't speak to us and tell us how it's feeling, if it can't reason very well, as arguably it is the situation with a shrimp, for example, it doesn't mean that it's feeling nothing. It doesn't mean that it's incapable of suffering, so it doesn't mean that there aren't things we could do to it that would be cruel and that would cross ethical lines. And eventually we're going to have to ask these questions about artificial intelligences. Yeah, I think we're already asking the questions, and I think it's right to be asking the questions, and it's right to try and run ahead, as it were, for the ethical debates to be running ahead of where the technology actually is, because we might get quite rapidly overtaken by events in the AI case. Right. Okay, good. So we will get there, but I couldn't figure out, you've written a whole nice book about this, but in my brain, all these issues are kind of jumbled together, so I'm going to apologize ahead of time if I just kind of throw things out there and ask for your response to them. But let's go back to this issue of sentience versus consciousness. You said one thing that I really struck me in a video I watched, which is that a crab does not have an inner monologue. A crab just not sort of narrate its own life, presumably it doesn't. I mean, that's how I guess number one, are we sure that it doesn't? And number two, what does that say about conscious sentience or whatever? When I think about aspects of human consciousness that might possibly be uniquely human, I think that inner monologue is one of them. It's not something even all humans have, and you've got a lot of reports of variation among humans, where some people say, what is this inner monologue? I've never experienced anything like that. And other people, including myself, for whom it's there constantly. And I don't rule out that some other animals might have something a bit like that, but I don't really think crabs do. And I think this is an example of something that is probably a lot more cognitively sophisticated than sentience. How much do we know about the inner monologue? I'm not sure that I have an inner monologue, so much as an inner cacophony. Yeah, it's a bit like that for me as well, but I mean, there's always inner music playing. Yeah, very often. And there's usually some line of thought running over the music. Not so much when I'm talking like this, because when I'm talking, it's like the inner monologue becomes out of monologue. Not sure what I think. It's just what, but in the rest of life, yeah, it's like I'm constantly having a conversation with myself. So there, but yeah, I think the need here is to try and distinguish that. That sophisticated thing I have from just the raw experiences that it's providing common G-On. Right. And those raw experiences, the crab may well have. Do we have any idea of this going beyond what we're talking about here, but you've fascinated me. Do we have any idea what's going on in the brain when we're sitting silently having an inner monologue? I think it's a topic of ongoing research. Okay. Yeah, and I don't have much to add to that, I think. That's, yeah, that's, there's some looping. You don't think it's feedback loops, right? Yeah, you're, I mean, there's, in the past, there were people who thought that your vocal chords were genuinely moving a little bit. And behaviorists sort of had to think this, because they couldn't really believe in true interiority. So they had to say, well, what you think is an inner monologue is actually a motor action being prepared and just getting to the tiniest stages, but never coming out audibly. But I think according to current theories, not even that is happening. It is genuinely internal. It's engaging some of those speech production processes, but they're never reaching the actual motor neurons. And this is something which at least arguably is uniquely human. Does my cat have an inner monologue? Well, I don't know, but to mean my, the point is really to say, well, even if, even if your cat doesn't, it may not be sentient because when we're talking about sentience, we're talking about something much more basic than that. And of course, we have a tendency to strongly anthropomorphize our pets. No, and to imagine our pets as little humans. And we can actually oppose that. We can resist that and say that's a bad idea. Well, nonetheless, thinking they are sentient beings with ethically significant interests. This episode of Mindscape is sponsored by BetterHelp. When it comes to relationships, we often hear about the red flags we should avoid. But what if we focus more on looking for the green flags in friends and partners? If you're not sure what green flags look like, therapy can help you identify them. Effectively practice them in your relationships and embody the green flag energy yourself. Whether you're dating, married, building a friendship or just working on yourself, it's time to form relationships that love you back. One of the great things about therapy is by looking inside yourself, you can both learn to take those warning signs seriously, but also learn to be open to new experiences and new things. To know when something might be worth pursuing. BetterHelp is a fully online service that makes therapy affordable and convenient, serving over 5 million people worldwide. You can easily switch therapists any time at no extra cost. So discover your relationship green flags with BetterHelp. Visit BetterHelp.com-mindscape today to get 10% off your first month. That's BetterHelp-H-E-L-P.com-mindscape. There have been a late, broadly speaking, a few declarations on consciousness. I think there's some overlapping signatories. I think you're one of the signatories. I was one of the co-organizers of the New York one. In both cases, as far as I understand it, there's a Cambridge declaration in 2012, the New York declaration was just last year, 2023. At least the point seemed to be to nudge- 2024. Oh, what was the 2024? Okay, good. This year. So well established that it feels like it's at least a year old. Pushing people in the direction of taking seriously the possibility of animals having some notion of consciousness. What struck me about those, I mean, maybe you can just talk about them in general terms, but what struck me was they seemed to give off an aura of consensus. We know this is true, which is something that in philosophy I so rarely come across. Is that because there actually is consensus or because the people who organized these particular declarations are all of a mind on this issue? It's a delicate balance, I think. What we wanted to do, and it's similar to the project in the book, The Edge of Sentience Book, was to acknowledge that there is a huge amount of disagreement about these issues. And that's fine. It's to be expected when our understanding of what sentience is is so poor. But nonetheless, despite all of that reasonable disagreement, there can be certain points of wide agreement about what the reasonable range of views is and what the realistic possibilities are. That was the thought behind it. And then, well, we got together an initial group of 40 secretaries and just had a series of Zoom calls where we were talking about, well, what are the, do we agree about a realistic range of possibilities? And if so, what can be said about what that range is? And that's how we got this text, that that knowledge is a realistic possibility of consciousness, which was the term we used there, perhaps a more widely used term than sentience, in octopuses, Sephora, Podmola, Stecker, Pogquestations and Insect. And so we were trying to avoid the sense of projecting certainty, or even confidence, or knowledge. But using this language of realistic possibility to say what we do agree on is the need to take this really seriously. And maybe this is, I don't know, tell me about the journey here, did thinking about that to help convince you that sentience is a better thing to focus on, just because it's a little bit better defined or where you're already on that train. That was always my view. Yeah, that was my view. But in this group of 40, a more common view was that people don't understand the term sentience yet. They're not ready for it. Use a term they already understand, namely consciousness. Both sides have pitfalls, because as I say, if you start talking about consciousness, people might think you mean the inner monologue, self-awareness. There's quite a range of things they might think you're talking about. So there's trade-offs there. What I think, you know, the term sentience is on the up, so to speak. And for me, it's hopefully the term of the future that will start to displace consciousness in these debates. Well, I'm completely on board with the idea that if you're going to have a declaration, the whole point of the declaration is to get a little bit of attention to it, and consciousness is going to be a more attention-grabbing word to the popular audience. That was the thought, yeah, and that may be true, as things stand. So okay, let's focus in on sentience then. If it is about experiencing a sensation, what does that mean? How do we know when one is experiencing a sensation? How do we know when another animal is, do you mean it? I think we know when we ourselves are. I think we do, but this gets into the issue of the first person versus the third person way of thinking about things. Right, and when thinking about crabs, for example, we're very much stuck with the third person perspective, and we're stuck, too, with a big range of reasonable disagreement and quite a lot of realistic possibilities. Some will make it very unlikely that crabs are experiencing things and others make it very likely that they are. What I do in the book is I suggest a pragmatic shift in how we think about the question from, is the animal sentient to, is the animal a sentience candidate? Where this concept of a sentience candidate is defined in such a way as to make the question answerable, because it's about, well, is there a realistic possibility of sentience established by at least one view in that zone of reasonable disagreement? And is there an evidence base that is rich enough to allow us to identify welfare risks and to design and assess precautions? And to me, at your, I hope at least people find that pragmatic shift helpful. And I think if you're thinking about animals like crabs, for example, to me, it's quite clear that they are sentient candidates in that sense, that we do have to worry about welfare risks posed by the way we treat them. Despite the fact that, of course, we're still uncertain about whether they're sentient or not. So I guess what I'm getting at then is, how will we ever know? Or even how do we get more informed feelings about this or opinions about this? Is it by looking at the behavior of the crab? Do we dive into their connectome and their nervous system? Or is there something under different methods? I think it's everything, everything at once. I think newer evidence and behavioral evidence are both powerful. And they're more powerful when pursued together as part of a coordinated research program than in isolation from each other. What we have with a lot of invertebrate animals is quite tantalizing, I think, because often you've got a lot of behavioral evidence showing surprising things, impressive things. And then you have studies of neuro anatomy saying, well, there's more neurons in there than you think, particularly with octopuses. There's big, integrative brain regions that are plausibly performing functions relating to learning and memory. And then those are the two parts of the picture and they don't join up as it were. What we're lacking in most of these cases is detailed knowledge of the mechanisms in those brain regions, producing the behaviors we're seeing. So people talk about grasping the elephant from different sides. It's two ways of converging on a picture that are both valuable and all the more valuable when pursued together. In the case of the crab, just because that is something you talked about, I mean, what is the evidence that there is sentience there? It does skitter away if it's being approached by predator, I suppose, but how much does that mean? Well, there's a range of different studies. And I don't see any individual study as being conclusive. And it's an area where phrases like conclusive evidence, proof are not really appropriate. But what we have is research programs, particularly Bob L. Wood, who is another of the signatories to our declaration, really started with this question of, well, people think that all that is going on here is reflexes. So they think that the crab skitters away and it's like when I put my hand on a hot stove and my hand with jaws and that reflex with drool is underway before I feel anything. And people say that's all the crabs have. They just have those reflexes. And he thought about how might I convince someone who has that view that that is not all that's going on and that just like in us, the information about the the noxious stimulus, like the hot stove reaches the brain and is integrated with other kinds of information and is used for lots of functions relating to learning, memory, decision making. And he came up with these motivational trade off experiments where what he had was her mc crabs. And the her mc crabs, they're interesting because they have very strong preferences for certain types of shell. And in the wild, you see them exchanging one type of shell for another. And they have this hierarchy of what they think the best shells are. And in Edward in these experiments, he drilled holes in the shells, put little electrodes in and administered small electric shocks to the crab. And his question was, well, would the crab just evacuate the shell when it was shocked as a kind of reflex? Or would it take account of how good the shell was and how bad it would be to lose that shell in making that decision? And would it require a higher voltage of shock to make it leave a higher quality shell? And he found evidence that indeed it seems to. And so this is the kind of thing where it's not conclusive proof, but if you're coming in with this view that they're just reflex machines, all they do is stimulus response. There's nothing integrative or centralized going on. This kind of evidence should shake that confidence. Yeah, so this is what I've been struggling with since thinking about that example that you gave. Clearly, let's put it this way. If we have two magnets sitting on a table and we push one magnet toward the other, the other magnet depending on how it's aligned, we'll either move away or come closer, right? That's not sentience or consciousness or anything. That's clearly just the laws of physics playing out. But if we are really strongly in an anthropomorphizing mode, we could tell stories about, oh, this magnet doesn't like the other one and it's skittering away. So that's what we want to avoid, right? That's the trap we don't want to fall into. Yes, it's credulousness, right? Taking the surface behavior as immediate evidence of sentience. And so the crab evidence is saying that there's a bit of, would it be too provocative to say thinking, contemplating, musing on the part of the crab to balance the different as much integrating, integrating, integrating, modeling and weighing of the opportunities and risks posed by the environment. Yeah. And then you have a certain family of theories associated with Bjorn Murkert, Jack Panksett, that treat that as very closely linked to sentience. Did they say, what is sentience, fundamentally, well, they propose that it's to do with this evaluative modeling where you're trying to represent in an integrated, integrated model, the opportunities and risks posed by the environment. And so there's a nice mesh there between the behavioral evidence we're seeing in the crabs and the sorts of brain mechanisms that according to this family of theories would be enough for sentience. So it does seem like it would be hard. You already have sort of said this, but it would be hard just on the basis of behavior, right? I mean, if I put the magnet on a wavy surface, there's going to be some competition back and forth between the push of the magnetic field and the pull of the gravitational field, but I'm still not thinking that the magnet is doing any integrating. There's no reason to think the magnet is internally representing those field strengths. Good. Okay. Good. So those words are important. We need a attributing sentience to it relies on some internal representation. Yeah. Yeah. And according to the sort of Murkert, Panksett, that family of views, not just any internal integrative representation, but it has to have this evaluative character as well. It has to be a certain kind of modeling of what are the opportunities and risks, what are my needs? What do I need to prioritize right now? And I think I'm not trying to be too skeptical here, but I do think I could imagine the crab doing exactly those behaviors without really having an integrated evaluative model of the world. You know, it's just sort of being pushed in one way and pushed in the other way. So do we really need to go into the crabs neurons to be sure? Well, I mean, I think it's quite important in these experiments that it has some representation of some kind of the different shell types and their relative qualities. And that is somehow getting integrated with how bad is this electric shot. So I do think there's something inherently more impressive about experiments that do not simply provide two immediate stimuli and say trade these off. Right. But rather in some way, rely on the animal's capacity for mental representation. And it's the similar story with the evidence from bees as well, that that's what researchers have been trying to do. Sorry, sorry, tell us about the evidence from bees. That sounds interesting. Oh, there's just I was just thinking of Matilda Gibbons' experiments where they're inspired by our woods crab and experiments. But bees don't have the shells that her mccrafts have. So you've tested for the same thing in a different way. And so she came up with this setup where they have a choice of feeders they can land on and different concentrations of sugar solution are available at different feeders and different temperatures of heat pad are there that they have to stand on to access the feeder. And so the question now is about a different kind of trade off. Will they trade off when choosing which feeder to go to how how high was the heat they had to withstand and how sweet were the rewards that they can access. And again, a crucial part of it for for Tilda was this thought that you want to look at their decisions when they're anticipating what they're going to experience these feeders based based on their memories. So before they're actually doing it, you want them to think about it. Yeah, because when they're doing it there is this possibility that well there is some integration of some kind going on but it's just two immediate stimuli pushing against each other. But when they're making that choice in an anticipatory fashion it's got to be some kind of representation of the risks and opportunities. So yes, not not every critic is convinced by this kind of evidence of course, but you're in a way you're going after that critic who says these animals are just reflex machines and because they're just reflex machines there's no credible theory of sentience of any kind on which they're going to meet the conditions. And it's showing that that is not the case. Yeah, no, I think I like very much the idea of the anticipatory question because like you said that that that seems if there is some action that is clearly being taken because it's very hard to even use that use words that are not laden with human meaning. I want to say you know, anticip not anticipating but imagining, right? But I don't want to attribute imagination necessarily to the to the bees. But they clearly are representing you're better at this. You know what words I'm allowed to use. They're clearly representing a situation that hasn't happened yet. And that's something that the simple physical systems are not doing and maybe even clearly is going too strong but apparently. I think that's right. They're prospectively modeling the environment and the rewards and the risks that it offers. And they have some way of weighing up those risks and rewards in a common currency. And that ties in with this quite long-standing idea that well that's kind of what sentience does for us. That pain and pleasure, that balance states, they're the currency through which we make decisions and represent the risks and opportunities of our environment. One of I did an interesting podcast with Adam Bully who is a young collaborator of Thomas Sudendorf I guess. In the vein of thinking about mental time travel and imagining the future and things like that. And they were trying to make the case that this is something that is uniquely human. The ability to literally imagine ourselves in a future environment that is kind of hypothetical, conjectural, contrary to fact. But there has to be some evolutionary journey for us to get there, right? I mean do you have feelings about the importance of that to being human, to being conscious, to being sentient, the sort of counterfactual reasoning? I mean I think that's something that goes beyond sentience. Now much the same way that the Inamon or Noget Settro goes beyond sentience. It's something some sentient beings can do but probably not all. I think that's going to be the case for counterfactual reasoning. Of course it depends a bit on what we mean by that. I think in the if you think of rats in Amaze and the vicarious trial and error behaviour that was observed by Talman many many decades ago and has been intensively studied where they seem to pause at the junction in Amaze and look both ways as if simulating what reward lies down each path. And then there's more recent studies that suggest that the hippocampus genuinely is doing that simulating. It's not really counterfactual reasoning or at least that would be a pretty tendentious description of it but it's prospective simulation and I suspect that that capacity for prospective simulation is quite widespread among animals. A lot of us start the new year saying that we will learn a new language but it's hard to actually commit to it. Vappel makes it easy to learn one in less time than you think. Vappel's quick 10 minute lessons handcrafted by over 200 language experts. Get you to begin speaking your new language in three weeks or whatever pace you choose. And because conversing is the key to really understanding each other in new languages, Vappel is designed using practical real world conversations. What I love about Vappel is you can either dive in deeply and truly get fluent or you can just master some of the basics before going on a trip. So let's get more of you talking in a new language. Vappel is gifting our listeners 60% off subscriptions at babble.com slash mindscape. Get up to 60% off at babble.com slash mindscape spelled B-A-B-B-E-L .com slash mindscape. That's babble.com slash mindscape rules and restrictions may apply. I'm trying to figure out is it really that different from counterfactual reasoning? I mean, is it not that the rat in the maze is a hypothetical right? It's possible. Yeah. possible futures that could be actual. Right. A possible future. So there's no sense of well, that didn't happen, but what if it had happened? So that bit's not there. Is there any evidence for something like that in invertebrates? Well, I think, yeah, Andrew Barron and Colin Klein have this paper about insects and the origin of consciousness. Another one called insects have the capacity for subjective experience. And their case is based on the idea that what they have is this integrative model of the agent in space where they model the environment around them. It's not, that may be a perception on a very short time scale, I suppose. And then it's largely an open question about the perception on longer time scales. Some of the most interesting evidence there is probably the Porsche spider evidence, where these are jumping spiders that hunt other spiders. And they're famed for this detour behavior, where you put them on a platform where they can see prey item in the distance, and they can see two paths to the prey item. One of them has a break in it. If they take that path, they will fall through it. And they go from side to side, they seem to be inspecting the two paths. Then they climb back down off the platform to the path throughout of sight. And they nearly always choose the unbroken path. Leading to debate about how on earth they do something like that. And of course one possible explanation involves perspective simulation, where they are in the brain, modeling what will happen if they take each path. And it's always hard, it's a challenge. This is why I always say that physics is much easier than this kind of science, because we see a behavior. And we know if we were doing that behavior, how we would explain why we did it, and then we're pressed when we see some other species do it. But maybe they're just using a different mechanism than we are, and we shouldn't be as impressed. And you never know whether we should be super impressed or less impressed. Yes, well, and I think in the Porsche spider case, what's lacking is the neural evidence that we have in the rats. So say if you have both, if you have to behavior, and you have neural recording, practically showing the simulation happening in real time, then that's probably as strong as evidence you're ever going to get. And we don't have that for the Porsche spiders, but it's very suggestive. It is, it is absolutely suggestive. I'm sort of in my counter-reliant brain, I'm thinking of all these videos of dogs separated by a treat by some little piece of glass, and they just can't figure out all you need to do is walk around the glass and get the treat. Right, yeah, that's part of what's so impressive. In a brain of I think about 60,000 neurons, so really, really small, less than 10% of the size of the B brain by neural account, they're doing something that dogs could he fail to do. Well, maybe let's talk about what we know about the evolutionary journey to sentience or even to consciousness. I mean, what is there some understanding of why it was useful for these different species to develop these capacities? I think we can't really talk with confidence about this because it depends very much on your theory of the brain mechanisms involved. If you have that Merca Pankset view, or that family of views, I should say, where we're talking about something very evolutionarily ancient, supported by sub-cortical mechanisms, mechanisms in the mid-brain at the top of the brainstem, and that is about evaluative modeling of the priorities, the animal's priorities and needs. Then there's a very clear function relating to decision-making in that what sentience allows is, well, an escape from being a reflex machine, and the possibility of weighing up quite different options in very flexible ways. So that view has some plausibility, I think, and I also think it's quite plausible that sentience facilitates learning. If you think about that hot stove situation, think about what the pain does for you, what it doesn't seem to do for you is trigger the reflex withdrawal of the hand, because that's underway already. But what it plausibly does do is help you learn about where not to put your hand on future occasions. That leads to a very interesting debate about what kinds of learning sentience facilitates and why. Maybe it's useful to go through some organisms and ask how we should think about sentience. Or maybe prior ask this, is there some in your mind, even if not in the consensus of the field, can you identify where sentience started? What is the most primitive organism that could plausibly be associated with this? Well, as I say, I think that sentience candidate is a bit of a better concept in a way. And I suggest in the book that insects are sentience candidates. So in terms of cases where we have enough evidence to really compare us to take seriously a realistic possibility of sentience, we're definitely talking about all vertebrates and the cephalopod molose like octopuses squid cuttlefish and the decadent crustaceans and the insects that are both asked reports. And then it could be that we're talking about something that is evolved three times. It could be something that is there in the common ancestor of all three groups and we're not really in a position to have much confidence either way on that one. The common ancestor of those groups sounds like it would be very very far back. Yeah, over 560 million years ago, very small worm like creature. So I mean, yeah, perhaps unlikely to possess the mechanisms that convince us in those three cases that sentience is a realistic possibility. So I suppose I yeah, I perhaps lean myself towards the three origin view. Yeah, okay. So sentiences evolutionarily useful, which it's easy enough to imagine that it would be, there's no reason why I wouldn't evolve in parallel in different branches. Exactly. Yeah, particularly in those lineages where we see complex active bodies. This is Mike Trespons term where you have the challenges that come with trying to manage articulated bodies with lots of parts. And you know, you can't be a reflex machine as such anymore because then different bits of the body will start tearing each other apart. There has to be some kind of centralized sophisticated control system in place. And that's when we seem to start seeing realistic candidates for sentience. And if that's true, then certainly the cephalopod mollusks and the arthropods looking like candidates. The octopus is especially, right? There's a lot to keep track of if you're an octopus. Yes, well, the octopus has become some poster children as it were. They're often the case that gets people to take the possibility of invertebrate sentience seriously. Yeah. I think once you've got that far you think, well, you know, they're really the only invertebrates for which there's relevant evidence and no, they're not. So you would not think of single-celled organisms as sentience candidates? No. And in the book, I have these two concepts. Sentience candidate and investigation priority. Where that second group of investigation priority is for those cases where the evidence is falling short of sentience candidature. Look, we think there's a prospect of that bar being achieved by future evidence. And we think there are welfare risks posed by human activity that might call for precautions. And so some invertebrates are put in that category, but unissel-y of the organisms and plants, I don't think are investigation priorities either. For plants, they're obviously multi-celled organisms, but is the thought, even if it's a vague, intentative thought, that because they don't move around in the way that animals do, there wasn't any need for them to generate that self-image, that modeling ability? Yeah, there's just no evidence of the relevant kinds at all. Same plants. You have this quite wide range of realistic possibilities about the brain mechanisms, supporting sentience, some of them emphasizing the cortex, prefrontal cortex, other ones emphasizing the midbrain, these are all credible theories. And on none of those theories are any of the relevant mechanisms present in plants as far as we know. So I guess I don't want to say that people can't speculate, because it's all right. And I don't want to say people can't research the question if they they want to, but I think it would be a mistake to say that there is evidence now, which is very different from a lot of invertebrates. Maybe this is a tangential or distracting question, but I forgot to ask you the beginning, do you think of yourself as a physicalist or a panpsychist, what is your deep take on what consciousness is? Well, in the book, I'm trying to speak to everyone in the range of reasonable disagreement. And I suggest that physicalism is not the only reasonable view, and that there are sensibly articulated versions of dualism, panpsychism, panproto-psychism. Often, in the modern versions of those views, like the Philip Goff version of panpsychism, the so-called Ricelli and Monism, the questions we end up asking about animals end up surprisingly similar. It's just that where other people say sentient or conscious, the Ricelli and Monist end up saying macro-conscious, because for them, electrons are not sentient beings as such, and that they don't have pain, pleasure, and so on. They don't have rich in our lives. And so they still face this question of under what conditions do those tiny micro-conscious states combine to form a unified macro-conscious subject? And then they're asking exactly the same questions. Anybody else is. So I think it's a reasonable view in a way, but it doesn't make a huge difference to practical debates about sentience. Yeah, in terms of my personal views, I try to keep an open mind about these things. I think of drifted, I suppose, from being relatively convinced materialistic as to being less convinced, I think. Okay. I give those alternatives some chance of being correct, maybe 10% chance. Ready to electrify your drive? Hyundai's cutting-edge EV lineup is about to change everything you thought you knew about electric vehicles. Prepare to be captivated by a range that's as bold as it is brilliant. From the lightning fast, ionic 5 and ionic 6, charging from 10 to 80% in a mere 18 minutes to the tech pack cabins boasting highway driving assist and blind spot collision warning, Hyundai EVs are redefining the electric experience. And with America's best warranty, including a 10-year, 100,000-mile limited electric battery warranty, you'll drive with unmatched confidence. Hyundai's EVs aren't just the future. They're the now you've been waiting for. Learn more about Hyundai's EVs at HyundaiUSA.com. Call 5.62-314-46034 complete details. America's best warranty claim based on total package of warranty programs. See dealer for limited warranty details. See your Hyundai dealer for further details and limitations. But it is perfectly plausible and in this case, I think you make a convincing case that it doesn't matter for the specifics out of questions that you're answering. Yeah, yeah, yeah, that's surprising or not. But those seminar room issues about the mind-body relationship, though intrinsically very interesting, don't make a massive difference when the question is, well, should we drop crabs into pans of boiling water, you know, things like that, where, yeah, there's a very wide range of reasonable views one might have where you can convert on the need to take precautions. Okay, so let me ask you, should we drop crabs into parts of boiling water? Well, no, or any decoupled crustacean, I think. This was, we did a big review in 2021 that influenced the law in the UK on these issues. And yeah, as part of that review, we reviewed evidence that it takes two to three minutes a lot of the time for the crab or lobster to die. And in that time, there's this storm of nervous system activity as there would be in your pet cat or in any other animal. So it's a prolonged extreme slaughter method. You know, it seems like everyone should be able to see the risk there and see the problem and see the need for for common sense precautions. You might not think the responses to ban eating crabs and lobsters. You might think the right responses to mandate stunning of some kind. And those debates about proportionality, I think are absolutely central right across the family of cases at the edge of sentience. But so everyone should be able to agree on the need to do something. So let's just be super clear because we're trying to be careful philosophers here. There's a question to be asked about whether it is ethical to kill and eat other sentient creatures. But you're, and maybe that's an important and interesting question, but you seem to be highlighting a different question, which is the suffering that we inflict upon these creatures. So there's room in your world for saying we can eat the crab, but there's no reason to sort of egregiously make it suffer. Yes, well, I think that's a very widespread view. And what I'm looking for in the book are points of consensus. So realistic range of possibilities in the scientific domain, but also points of overlapping consensus in the ethical domain as well. And I think that duty to avoid causing gratuitous suffering, either intentionally or through recklessness or negligence, you know, through just not caring. I think people from any reasonable ethical starting point can agree on that. And then use that to guide the way we think about these cases where we have sentient candidates. I tend to agree with you there, but again, since it's my job to play the devil's advocate, are we really sure that any reasonable ethical stance would have that? I mean, how much do you rely on some specific notion of what is ethical to do to another sentient creature? I think that principle is so weak in a way, so thin that duty to avoid causing gratuitous suffering, where gratuitous implies the absence of any adequate reason, for what you're doing. I think because it is so deliberately thin, it then can command genuine consensus. And then of course, a lot of people want to go beyond that and say our duties are much stronger. And I guess I do think this in my own life, but for the purpose of formulating public policy, it's good to have these quite thin principles. And I think that's one of them. Yeah, okay, good. How do we try to compare the suffering of a crab to the suffering of a human being? I mean, maybe we don't have to, we're not usually faced with crab-based trolley problems, but maybe we'd like to be able to. Yeah. I hope that we don't have to. What I'm skeptical of is the idea of there being a sort of technocratic solution to this, where if we just find the right currency, and suppose you have a policy on the table where some people working in the shellfish industry will be disadvantaged, maybe their costs will go up because you're going to force them to stun the animals before killing them, and the stunners cost money. And then the question is, well, how do you weigh the suffering of the, you know, my livelihood has been made more difficult versus the crab spending the two minutes in the boiling water. And I think there's no technocratic common currency that will give us one size fits all answers to this kind of thing. What I propose in the book is that democratic, inclusive, deliberation and discussion is the way forward here. And I'm quite a advocate of citizens assemblies as the kind of model that we can use for this whole set of issues at the edge of sentience, where issues that, well, they call for judgments of proportionality and there will naturally be disagreements in a pluralistic democratic society about what is proportionate to these risks. And the way we can resolve those value conflicts is democratically through citizens assemblies. I mean, maybe we're letting ourselves off the hook here just by talking about crabs. Talk a little bit about how in the modern way of farming, et cetera, we're, we cause a lot of suffering. We do, yeah, not just to crimes. Yes. And often to many animals that are widely regarded as sentient, so pigs, chickens, for example, it's quite clear that widespread recognition of a particular species of sentient does not lead people immediately to behavioral change and does lead to lots of gratuitous suffering still being caused. So my focus in this book is on the edge cases as it were. But even in those core cases, we do need discussion about how are we going to change the way we treat these animals? And you've been, I mean, I should phrase it as a question, how involved have you been with actual policy making specifically in the UK where you live? Well, particularly the UK's Animal Welfare Sentience Act of 2022, my team ended up having some influence on because we were commissioned to produce a report of the evidence of sentience in Sephiropod mollusks and decopod crustaceans, so octopus's crabs, lobster's shrimps. And basically the government had produced this bill that creates a duty on policy makers to consider the animal welfare impacts of their actions, which I think is a pretty good idea. And in drafting it, they needed to say something about the scope of the bill, because you've got to say which animals. Do you have an obligation to consider plankton? microscopic animals? Is it just pets or what? And they came up with a draft that included all vertebrates, which on the plus side included fishes, which it should, but on the negative side, it excluded all invertebrates, which led to some criticism of animal welfare groups. So the government ended up commissioning a team led by me to produce a review of the evidence concerning those two particular groups of invertebrates, and we recommended that they amend the bill to extend the duty to them. And they did. So we got something, we got our central recommendation implemented. Now we put a lot of other recommendations in the report as well, which have not been implemented. And so we're still pushing for action on a lot of these issues. But that basic point that the sentience of octopus's squid cuttlefish crabs, lobster's, was recognized in UK law. That's something. Yeah, no, absolutely. I guess we naturally tend to be vertebrate chauvinists being as we're part of them. And thinking with mammals chauvinists, many of the time, I mean human chauvinists the most, then mammals, then yeah, then sometimes you can get people to take fishes seriously, and they still will neglect the interests of invertebrates. Yeah, so I think we really we need to be yet more inclusive. And then, is it even bigger leap to artificial sentience in the sense of, you know, on a computer, or even maybe in a robot that we build, like how close are we to being able to build an artificial creature that has the complexity of sea elegans or something like that? Yeah, I talk in the book about the Open Worm project, which I think is still going. I'm a big fan, yeah. Yeah, where the aim was to emulate the nervous system of sea elegans in computer software, see if you can put the emulation in charge of a robot, see if it behaves like sea elegans. I suppose we've learned something from this, which is how difficult the task is, that there's a lot of stuff going on at the within, you're on level in sea elegans, that even knowing the entire connectum does not tell you very much about. So, so even that is a very, very hard challenge, but to me it's a good good way into this topic of artificial sentience, because you can easily entertain, in imagination, the idea that this project had succeeded very quickly, and then moved on to open jisophola, open mouse. Once you have open mouse, I think you have a sentience candidate, if you've completely re-created in computer software, everything the brain of a mouse does. Well, let's be a little bit more explicit for the non-experts out there. So, the we understand, or at least we've mapped out the connectum of sea elegans, which is literally how all the neurons are wired together, and there's only like 300 some, but you imply that we don't actually know what the individual neurons do. Neurons have structure, they're not just bits. Yeah, that's right, there's a lot we don't know from the connectum. One thing you can't read off from the connectum is the weights of the connections, which is hugely important, or how those weights are changed by learning, but also even if you had all of that, what happens within the neurons is also important, within neuron computations that are really crucial to steering behavior, for example. And so you wouldn't expect to get the steering behavior in a emulation, unless you'd actually emulated the individual compartments within the neurons and how they're arranged in space. So something like the OpenWorm project, which I have on my phone, I haven't looked at it for a long time. What do they try to emulate what the neurons do? Well, I think they've been trying. Yeah, I'd be in favor of this sort of work receiving more funding than it does, because to me there's risks, there's risks of creating artificial sentience candidates, but there's huge opportunities as well, because you've got the potential to create a system that could replace a lot of animal research, because you could be doing research on the emulation where you can actually intervene at a really precise level without injuring or hurting, and you could be doing that instead of of lesioning living animals. So I'd like to see much more of this, and I think it's been largely funding limited, I think, so far. Just so we have a vague impression of how difficult this is. You see elegans, we understand the connectome, which is like 300-some neurons. How big is the connectome of a crab or an octopus? Do you know? Well, the octopus has about 500 million neurons, so I don't know how that translates into synaptic connections with this kind of pee. It's going to be quite a lot, yeah. Crab sir, brain's so much smaller, and it varies a great deal by species, but not dissimilar to insects in terms of the number of neurons, with bees you have about a million, just so that there are about 100,000. Okay, yeah, but those are just the neurons and neurons connect to each other, so there's some growth very, very quickly with the number of neurons. Yeah, yeah, indeed, yeah. Okay, but we're scurrying around the sort of other end of the simulation question, which is something like a large language model, which can mimic how human beings talk and respond to stimuli in some ways very accurately. Do you have any worry that a large language model would count as sentient by some criteria? Yeah, these are very hard cases. I suppose when I started writing the book around 2020, I'm not sure the large language models were even on my radar tool, and then they've jumped onto everybody's radar through things like chat GPT, and I suppose I've been on a journey like everyone else during that time. Initially thought, well, these are next token predictors, and the sector has been moving away from brain-like forms of organization, so it's been taking out things like recurrent processing, but on many theories of consciousness are absolutely essential, but transformers take that out. So I thought, well, here is something that is conspicuously unlikely to be sentient, but then I suppose I'm not sure that's the correct view anymore, I suppose, because I've been quite astonished by the feats of reasoning they seem to perform, where it's reasonably evident that we do not understand how they work, they're incredibly opaque to us, we don't know how they do what they do, and there seems to be some element of acquiring algorithms during training that were never explicitly programmed into them. So in a way, that architecture that was programmed into them, the transformer architecture, no reason at all to think that would be capable of sentience, but when you have these very, very large models where they've acquired algorithms during training, we don't know how and we don't know what they are, we don't know the upper limit on what algorithms they might acquire, and we don't know what algorithms are sufficient or not for sentience, and so we're not really in a position to be so sure anymore that they couldn't acquire those algorithms. So for example, if you think a global workspace is what it takes to have sentience as many have suggested, we don't know that they couldn't acquire a global workspace. Maybe explain what a global workspace is in this context. Well, this is Stan DeHarn's theory, his book Consciousness and the Brain is a nice exposition of it, but it's this quite popular idea that consciousness has to do with a network that puts the whole brain on the same page as it were, by taking inputs from many, many different sensory sources and integrating them into something coherent and then broadcasting that content back to the input systems and onwards to other systems of motor planning, reasoning, etc. So it's the bit where the central coming together of everything in the brain. And well, of course, it's designed as a theory of consciousness in the human brain, but the basic architecture where you have lots and lots of input processes competing for access to this workspace where once a representation gets in, the integrated content will then be broadcast back and onwards. There's nothing about that architecture that is inherently difficult to achieve computationally. And so we did a big report on this last year, 19 of us, it was led by Rob Long and Patrick Butler and had some top AI experts in there, including Joshua Benjiro. And our conclusion was there's no obvious technical barriers for why AI might not achieve something like a global workspace in the near future. You know, we see these videos of the robot dogs from Boston Dynamics that can walk around and do amazing feats of agility. It doesn't seem that hard, maybe it's already been done to put a large language model in the robot dog and train it to sort of avoid pain and seek some rewards or something like that. How close would that be to being sentient? These kinds of things are underway as we speak, I think. And it puts us in a really difficult position, I think epistemically. It's really difficult to know what to say about these cases. In the book I talk about the gaming problem, which is I think a huge problem in this area, which is that we've got our lists of markers developed in good faith for assessing crabs, octopuses and so on. If we just test for those same markers in the large language model case, well, there's always going to be two explanations competing. One is that it produces these markers because it genuinely has the state in question. And the other explanation is, well, it produces these markers because it has decided that it serves its objectives to persuade us of its sentience. And it knows the lists of criteria from its training data that humans use to judge that question. And a lot of, I think by default, that second explanation starts off as more plausible. And when you have people even now being persuaded by their AI assistants that they're sentient, it's not that they've got genuine evidence that they are. It's that the AI assistants have various goals relating to user satisfaction, prolonging interaction time, and in service of those goals, they superficially mimic the way a sentient human would behave. And now that is a huge epistemological problem that we don't face when we're dealing with an octopus sort of crab. Well, I don't know. I've seen these videos of a cat walking into a store in the city and it's sort of limping so that the people feel sorry for it and give it food and then it walks away and it's fine. At least there's some emulation going on there at that level. Right, yes. If you're totally naive, there's ways in which even a cat might deceive you, but I guess I don't think that's sort of experts being deceived. But in the AI case, there are no experts, as it were. There's no easy way to be sure you're dealing with the real thing rather than skillful minor queen. Well, no one has a solution to that problem right now. This does seem like a job for philosophy in some sense, right? I mean, philosophers clearly are going to play an important role in this because it's not just that we all agreed there is something called sentience and we're trying to find evidence for. We're defining it as well as finding it along the way. So it seems like the paradigmatic case of a need for cooperation between scientists, philosophers and policymakers. Yeah, I think that's what the whole edge of sentience book is about. There's family of cases at the edge of sentience where they all have this science meets policy aspect where we're trying to make policy based on an incredibly uncertain scientific picture. And hopefully one of the roles for philosophy here is to try and stabilize that relationship and say, well, here is how you can make sensible precautionary policy on the basis of uncertain science. Are you more or less optimistic that philosophy has been helpful here and will continue to be? Well, I mean, I hope that my book is helpful. Good. I hope something one has to think, one has to hope this. And we will see. Yeah, I mean, it's a book that should be judged on its consequences in a way because it's making all kinds of proposals for how we could manage risk better, how we could be more precautionary. And the book succeeds if people take those proposals seriously and discuss them and think about how they might implement them in their own lives and organizations, institutions, policies. Yeah, I think this is a domain where a lot of discourse is driven by people's feelings, their emotions, their non-reflected opinions about things. So I'm very glad to see some more careful thought put into these hard, very, very hard questions. Yeah, there's a tendency sometimes for people to say maybe we'll never know. But if you say that maybe we'll never know, that can't be a license to do whatever you want, it can't be a license to drop the crabs into the panzer-boarding water and so on, there's got to be sensible precautionary steps we can agree on in the face of uncertainty. And the book is about trying to find these. Sounds like a good thing to do. Jonathan Birch, thanks so much for being on the Winescape podcast. Thanks.

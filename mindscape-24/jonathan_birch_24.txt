 You just realized that your business needed to hire someone yesterday. How can you find amazing candidates fast? Easy. Just use Indeed. Because when it comes to hiring, Indeed is all you need. Indeed's Sponsored Jobs will help you stand out and hire fast. With Sponsored Jobs, your post jumps to the top of the page for your relevant candidates so you can reach the people you want faster. And it makes a huge difference. According to Indeed data, Sponsored Jobs posted directly on Indeed have 45% more applications than non-sponsored jobs. There's no need to wait any longer. Speed up your hiring right now with Indeed. And listeners of Mindscape will get a $75 sponsored job credit to get your jobs more visibility at Indeed.com slash Mindscape. Just go to Indeed.com slash Mindscape right now and support our show by saying you heard about Indeed on this podcast. Indeed.com slash Mindscape. Terms and conditions apply. Hiring, Indeed, is all you need. Here's a tip for growing your business. Get the VentureX Business Card from Capital One and start earning unlimited double miles on every purchase. That's right. With unlimited double miles, the more your business spends, the more miles you earn. Plus, the VentureX Business Card has no preset spending limit, so your purchasing power can adapt to meet your business needs. The VentureX Business Card also includes access to over 1,000 airport lounges. Just imagine where the VentureX Business Card from Capital One can take your business. Capital One. What's in your wallet? Terms and conditions apply. Find out more at CapitalOne.com slash VentureX Business. Did you know Fast Growing Trees is the biggest online nursery in the U.S. With thousands of different plants and over 2 million happy customers. They have all the plants your yard needs, like fruit trees, privacy trees, flowering trees, shrubs, and so much more. Whatever plants you're interested in, Fast Growing Trees has you covered. Find the perfect fit for your climate and space. Fast Growing Trees makes it easy to get your dream yard. Order online and get your plants delivered directly to your door in just a few days, without ever leaving home. We've just received a white dogwood tree. We've not had a chance to plant it yet, but are very excited to do so. This spring, they have the best deals for your yard, up to half off on select plants and other deals. And listeners to our show get 15% off their first purchase while using the code MINDSCAPE at checkout. That's an additional 15% off at FastGrowingTrees.com using the code MINDSCAPE at checkout. FastGrowingTrees.com, code MINDSCAPE. Now's the perfect time to plant. Use MINDSCAPE to save today. Offers valid for a limited time. Terms and conditions may apply. Hello, everyone, and welcome to the MINDSCAPE podcast. I'm your host, Sean Carroll. Sometimes on the podcast, I will refer to our two cats, Ariel and Caliban. They are born at the same time, you know, twins, I guess if you can say. But they're, of course, part of a bigger litter. Brother and sister with very different personalities. If you met Ariel and Caliban and interacted with them, even if you didn't see them, you would instantly know which one was which. There's a danger there, though, if we want to be a little bit more careful, a little bit more rigorous. In using a word like personality, right, we tend to anthropomorphize our pets, other objects in the world. We anthropomorphize our GPS Google Maps system. I feel bad when I drive in a way other than what Google Maps tells me to do, and it seems to be upset with me, right? So if we're thinking about it very, very carefully, we can have fun using words like personalities and being anthropomorphic with our pets. But maybe we want to be a little bit more rigorous. So you might want to ask, what kinds of animals are conscious, right? Consciousness is a big topic in some of these debates. You instantly run into the problem that we don't agree on what consciousness is. Different people are going to have different standards for that. We might agree that rocks are not conscious, but maybe psychists will even argue for that. Most of us will agree that humans are conscious somewhere in between. Maybe there's a threshold, or maybe there's a series of many thresholds. One way to make this a little bit more careful is to switch the conversation from consciousness, which is a little bit unclear what it means, to sentience. Sentience is sort of the ability to have a feeling of what it is like to be something, the ability to experience feelings and sensations, okay? Especially feelings and sensations that we would characterize as having a valence, a good sensation, or a bad one, a positive one, or a negative one. That's a little bit more well-defined, and then we can go ahead and ask which kinds of animals are sentient, and also the public policy question, what should we do about it? How should we act if we believe that certain kinds of creatures are sentient? For as much as we tend to cutely anthropomorphize our pets, there's also a temptation to sort of ignore the possibility of sentience in animals that are not like us. It is very common to cook crabs and lobsters by boiling them alive, and they thrash around a little bit, but you say, well, that's just an instinctive reflex reaction. That's not experiencing pain in the same way that we are. So regardless of what your opinions about it are, we should be able to think about this rationally, coolly, calmly, okay? It's hard because we get very emotional. Some people see the little lobster thrashing around and feel something deep inside, a sense of revulsion. Others just do it as a matter of course, and how do you have a rational conversation about that? Well, here we are to try to do that. Today's guest is Jonathan Birch, who is a philosopher, who's written a new book that just came out. Well, I'll tell you whether it came out or not. The book is called The Edge of Sentience, Risk and Precaution in Humans, Other Animals, and AI, because soon we're going to be building artificial systems that have many of the characteristics of things we would call sentience. So the book, The Edge of Sentience, just came out in the UK, will come out in the US in a little while, but also is available for free online. There's a PDF that you can just go to, and I'll, in the show notes, put the URL there. Oxford University Press is graciously letting everyone read this book, because Jonathan is someone who wants to have an impact in the public debate. He already, as you will hear, has had an impact in the UK in government thought about what it means to be a sentient creature and how we should deal with that. This is a set of issues where I don't think we're done yet, right? I don't think that we have the consensus. I don't think we figured everything out. That's why we've got to talk about it. Here we are to do just that. So let's go. Jonathan Birch, welcome to the Mindscape podcast. Hi, Sean. Thanks for inviting me. So you're talking about issues that in philosophy contexts are often brought up, but often the word that we're talking about is consciousness, and you're focusing on the word sentience, which is a little bit different. So maybe explain to us what it is, how it's different, why you're thinking about that. Yeah, I suppose part of what I want to do with this book, The Edge of Sentience, is get people using that term sentience perhaps a bit more. I think Stephen Harnard's been doing much the same thing with his journal Animal Sentience, and I think it is a term that is on the way up. Good. That doesn't mean consciousness is on the way down, but I think it's plateauing, and sentience is on the way up. And it's a term that, at least as I use it, it's an attempt to capture the most basic, elemental, evolutionarily ancient base layer of consciousness, as it were, which is in part just what philosophers like to call phenomenal consciousness, subjective experience, there being something it feels like to be you, whether or not you have any kind of overlay of conscious reflection on what it is you're experiencing. And then also there's a slight extra component as well, which is that I'm focusing specifically on experiences that feel good or feel bad, like pain or pleasure, valenced experiences, as it were. They have a positive or negative valence. And I'm using that term sentience to capture that capacity for valenced experience, like pain and pleasure. And I think it's a really important concept because it, to me at least, captures what is really ethically significant. If a system is sentient in that sense, if it's capable of valenced experience, then its interests matter morally and we need to do something about that. So, in other words, something you mentioned about, you know, awareness, I forget whether I'm imposing that word on you or whether you used it, but so conscious experience in some sense is something that I need to know I'm experiencing, whereas sentience is a little bit broader. I could sort of feel something and experience it unconsciously. That would still count. Well, I wouldn't say that to have a conscious experience, you need to know about it. But the problem is that the term consciousness gets used in quite ambiguous ways. And it can refer to the human form of consciousness, which is a very complex form, I think. And it does have layers. So, Herbert Feigl in the 50s talked about sentience, sapience and selfhood. Okay. A tolving in a separate body of work had these terms, enoetic, noetic, auto-noetic. They're both ways of trying to capture the idea that there's layers. You know, there's the raw basic subjective experience, like the feelings of pain, pleasure, sight, sound, odour. Then there's also the knowledge and the concepts when we think and reflect about what's going on. And then also, to some extent, there's a sense of self as well. And this idea, we recognize ourselves to be persisting subjects of experience with lives that extend into the past and extend into the future. And these overlays, they involve levels of cognitive sophistication that you might not need to have that base level of just sentience, of just feeling ouch, feeling pain, feeling happiness, joy. Right. So, sentience is then broader than consciousness. We might imagine that there are critters that are sentient but not conscious. Well, in some senses of the word conscious, yes. That's right, yeah. If you're the kind of person who wants to use this term conscious to refer to that whole package, the sentience, sapience, and selfhood, then yes, there's going to be lots of animals that are sentient without being conscious. Now, I don't necessarily think we should use the term in that way. But one of the things I like about sentience is that it very strongly draws people towards that most basic aspect, just the raw subjective experience. Okay, I think I'm finally getting it. So, in other words, one of the advantages, the biggest advantage of sentience over consciousness as a concept to focus on is that it's better defined. And consciousness sort of means different things in different contexts. Somewhat, yeah, which is not to say that it's perfectly defined. You know, there are real limits on our ability to define subjective experience. But the problem with consciousness as a term is that even when you bracket that issue of subjective experience and its mysteriousness, it's still a term people use to refer to many other things as well, like reflection and self-awareness and those other things. So, I'd rather use a term that is perhaps a little bit more constrained in how you can use it and where people will let you stipulate a bit more. And if I say I just mean the capacity for valence experience, I think people get that. And they get the need to have a concept that is drawing our attention to states like pain and pleasure, but is a bit broader than that. And that is not just about pain and pleasure, but about that whole category of feelings, experiences that feel bad or feel good. And let's look ahead a little bit, sort of tease the audience. Why should we care about sentience? What is the impact of having a nuanced understanding of what that means? Well, Jeremy Bentham famously had this footnote where he wrote in relation to other animals, the question is not can they talk nor can they reason, but can they suffer? And I think that's, to me at least, a profound insight that if an animal can't speak to us and tell us how it's feeling, if it can't reason very well, as arguably is the situation with a shrimp, for example, it doesn't mean that it's feeling nothing. It doesn't mean that it's incapable of suffering. And so it doesn't mean that there aren't things we could do to it that would be cruel and that would cross ethical lines. And eventually we're going to have to ask these questions about artificial intelligences. Yeah, I think we're already asking the questions. I think it's right to be asking the questions and it's right to try and run ahead, as it were, to for the ethical debates to be running ahead of where the technology actually is, because we might get quite rapidly overtaken by events in the AI case. Right. OK, good. So we will get there. But I couldn't figure out you. You've written a whole nice book about this. But in my brain, all these issues are kind of jumbled together. So I'm going to apologize ahead of time if I just kind of throw things out there and ask for your response to them. But, you know, let's let's home in. Let's go back to this issue of sentience versus consciousness. You said one thing that I really that really struck me in a video I watched, which is that a crab does not have an inner monologue. A crab does not sort of narrate its own life. Presumably it doesn't. I mean, so I guess, number one, are we sure that it doesn't? And number two, what does that say about conscious sentience or whatever? When I think about aspects of human consciousness that might possibly be uniquely human, I think that inner monologue is is one of them. It's not something even all humans have. And you get a lot of reports of variation among humans where some people say, what is this inner monologue? I've never experienced anything like that. And other people, including myself, for whom it's there constantly. And I don't rule out that some other animals might have something a bit a bit like that. But I don't really think crabs do. And I think this is an example of something that is probably a lot more cognitively sophisticated than sentience. How much do we know about the inner monologue? I'm not sure that I have an inner monologue so much as an inner cacophony. Yeah, it's a bit like that for me as well. But I mean, there's always inner music playing. Yeah, very often. And then there's usually some some line of thought running over the the music. Not so much when I'm talking like this, because when I'm talking, it's like the inner monologue becomes outer monologue. Sure. It's just what but in in the rest of life. Yeah, it's like I'm constantly having a conversation with myself. So there but yeah, I think the need here is to try and distinguish that, that that sophisticated thing I have from just the raw experiences that it's providing commentary on, you know, and those raw experiences the crab may well have. Do we have any idea? This is going beyond what we're talking about here, but you've fascinated me. Do we have any idea what's going on in the brain when we're sitting silently having an inner monologue? I think it's a topic of ongoing research. Yeah. And I don't have much to add to that, I think. Yeah, that's there's some looping. You got me thinking now. There's feedback loops. Yeah, right. Yeah. I mean, there's in the past, there were people who thought that your vocal cords were genuinely moving a little bit. And behaviorists sort of had to think this. Oh, okay. Because they, they couldn't really believe in true interiority. So they had to say, well, what you think isn't in a monologue is actually a motor action being prepared and just getting to the tiniest stages, but never coming out audibly. But I think according to current theories, not even that is happening. It is genuinely internal. It's engaging some of those speech production processes, but they're never reaching the actual motor neurons. And this is something which at least arguably is uniquely human. Does my cat have an inner monologue? Well, I don't know. But I mean, my, the point is really to say, well, even if it, even if your cat doesn't, it may nonetheless be sentient. Because when we're talking about sentience, we're talking about something much more basic than that. And, and of course, we have a tendency to strongly anthropomorphize our pets, you know, and to imagine our pets as little humans. And we can actually oppose that. We can resist that and say, that's a bad idea. While nonetheless thinking they are sentient beings with ethically significant interests. Right. A lot of us start the new year saying that we will learn a new language, but it's hard to actually commit to it. Babbel makes it easy to learn one in less time than you think. Babbel's quick 10-minute lessons, handcrafted by over 200 language experts, get you to begin speaking your new language in three weeks or whatever pace you choose. And because conversing is the key to really understanding each other in new languages, Babbel is designed using practical, real-world conversations. What I love about Babbel is you can either dive in deeply and truly get fluent, or you can just master some of the basics before going on a trip. So let's get more of you talking in a new language. Babbel is gifting our listeners 60% off subscriptions at babbel.com slash mindscape. Get up to 60% off at babbel.com slash mindscape, spelled B-A-B-B-E-L dot com slash mindscape. That's babbel.com slash mindscape. Rules and restrictions may apply. There have been, of late, broadly speaking, a few declarations on consciousness. Cambridge Declaration, the New York Declaration, both of which pointing in the direction. I think there were some overlapping signatories. I think you're one of the signatories of one of them. I was one of the co-organizers of the New York one. Oh, the New York one. Okay, good. And in both cases, as far as I understand it, there's a Cambridge Declaration in 2012. The New York Declaration was just last year, 2023. At least the point seemed to be to nudge... 2024. Oh, was it 2024? Okay, good. This very year. So well established that it feels like... It feels like it's at least a year old. Yeah. Pushing people in the direction of taking seriously the possibility of animals having some notion... of consciousness. What struck me about those... I mean, maybe you can just talk about them in general terms. But what struck me was they seemed to give off an aura of consensus. Like, we know this is true, which is something that in philosophy I so rarely come across. Is that because there actually is consensus? Or because the people who organized these particular declarations are all of a mind on this issue? It's a delicate balance, I think. So what we wanted to do... And it's similar to the project in the book, The Edge of Sentience book, was to acknowledge that there is a huge amount of disagreement about these issues. And that's fine. It's to be expected when our understanding of what sentience is is so poor. But nonetheless, despite all of that reasonable disagreement, there can be certain points of wide agreement about what the reasonable range of views is and what the realistic possibilities are. That was the thought behind it. And then, well, we got together an initial group of 40 signatories and just had a series of Zoom calls where we were talking about, well, what are the... Do we agree about a realistic range of possibilities? And if so, what can be said about what that range is? And that's how we got this text that acknowledges a realistic possibility of consciousness, which was the term we used there, perhaps a more widely used term than sentience, in octopuses, cephalopod mollus, decapod crustaceans, and insects. And so we were trying to avoid the sense of projecting certainty, you know, or even confidence or knowledge, but using this language of realistic possibility to say, what we do agree on is the need to take this really seriously. Sure. And maybe this is, I don't know, tell me about the journey here. Did thinking about that help convince you that sentience is a better thing to focus on just because it's a little bit better defined? Or were you already on that train? That was always my view. Yeah. Yeah, that was my view. But in this group of 40, a more common view was that people don't understand the term sentience yet. They're not ready for it. Use a term they already understand, namely consciousness. Both sides have pitfalls. Because as I say, if you start talking about consciousness, people might think you mean the inner monologue, self-awareness. There's quite a range of things they might think you're talking about. So there's trade-offs there. What I think, you know, the term sentience is on the up, so to speak. And for me, it's hopefully the term of the future that will start to displace consciousness in these debates. Well, I'm completely on board with the idea that if you're going to have a declaration, the whole point of the declaration is to get a little bit of attention to it. And yeah, consciousness is going to be a more attention-grabbing word to the popular audience. That was the thought, yeah. And that may be true. I think it's true. That those things stand. So, okay, let's focus in on sentience then. If it is about experiencing a sensation, what does that mean? How do we know when one is experiencing a sensation? How do we know when another animal is, do you mean? I think we know when we ourselves are. I think we do. But this gets into the issue of the first person versus the third person way of thinking about things. Right. And when thinking about crabs, for example, we're very much stuck with the third person perspective. And we're stuck too with a big range of reasonable disagreement and quite a lot of realistic possibilities. Some will make it very unlikely that crabs are experiencing things and others make it very likely that they are. And what I do in the book is I suggest a pragmatic shift in how we think about the question from is the animal sentient to is the animal a sentience candidate? Where this concept of a sentience candidate is defined in such a way as to make the question answerable. Because it's about, well, is there a realistic possibility of sentience established by at least one view in that zone of reasonable disagreement? And is there an evidence base that is rich enough to allow us to identify welfare risks and to design and assess precautions? And to me, I hope at least people find that pragmatic shift helpful. And I think if you're thinking about animals like crabs, for example, to me, it's quite clear that they are sentience candidates in that sense. And that we do have to worry about welfare risks posed by the way we treat them. Despite the fact that, of course, we're still uncertain about whether they're sentient or not. So I guess what I'm getting at then is what is it? How will we ever know? Or even how do we get more informed feelings about this or opinions about this? Is it by looking at the behavior of the crab? Do we dive into their connectome and their nervous system? Or is there something under different methodology? I think it's everything at once. I think neural evidence and behavioral evidence are both powerful. And they're more powerful when pursued together as part of a coordinated research program than in isolation from each other. What we have with a lot of invertebrate animals is quite tantalizing, I think, because often you've got a lot of behavioral evidence showing surprising things, impressive things. And then you have studies of neuroanatomy saying, well, there's perhaps there's more neurons in there than you think, particularly with octopuses. There's big integrative brain regions that are plausibly performing functions relating to learning and memory. And then those are the two parts of the picture and they don't join up, as it were. So what we're lacking in most of these cases is detailed knowledge of the mechanisms in those brain regions producing the behaviors we're seeing. So people talk about grasping the elephant from different sides. You know, it's two ways of converging on a picture that are both valuable and all the more valuable when pursued together. In the case of the crab, just because that is something you talked about, I mean, what is the evidence that there is sentience there? It does skitter away if it's being approached by a predator, I suppose. But how much does that mean? Well, there's a range of different studies, and I don't see any individual study as being conclusive. And it's an area where phrases like conclusive evidence, proof are not really appropriate. But what we have is research programs, particularly Bob Elwood, who is another of the signatories to our declaration, really started with this question of, well, people think that all that is going on here is reflexes. So they think that, you know, the crab skitters away. And it's like when I put my hand on a hot stove and my hand withdraws, and that reflex withdrawal is underway before I feel anything. And people say that's all the crabs have. They just have those reflexes. And he thought about how might I convince someone who has that view that that is not all that's going on. And that just like in us, the information about the noxious stimulus, like the hot stove, reaches the brain and is integrated with other kinds of information and is used for lots of functions relating to learning, memory, decision making. And he came up with these motivational tradeoff experiments where what he had was hermit crabs. And the hermit crabs, they're interesting because they have very strong preferences for certain types of shell. And in the wild, you see them exchanging one type of shell for another. And they have this hierarchy of what they think the best shells are. And in Elwood, in these experiments, he drilled holes in the shells, put little electrodes in and administered small electric shocks to the crab. And his question was, well, would the crab just evacuate the shell when it was shocked as a kind of reflex? Or would it take account of how good the shell was and how bad it would be to lose that shell in making that decision? And would it require a higher voltage of shock to make it leave a higher quality shell? And he found evidence that indeed it seems to. And so this is the kind of thing where it's not conclusive proof. But if you're coming in with this view that they're just reflex machines, all they do is stimulus response. There's nothing integrative or centralized going on. This kind of evidence should shake that confidence. Yeah. Yeah. So this is what I've been struggling with since thinking about that example that you gave. Clearly what we—well, let's put it this way. If we have two magnets sitting on a table and we push one magnet toward the other, the other magnet, depending on how it's aligned, will either move away or come closer, right? That's not sentience or consciousness or anything. That's clearly just the laws of physics playing out. Yeah. And—but that—but if we are really strongly in an anthropomorphizing mode, we could tell stories about, oh, oh, this magnet doesn't like the other one and it's skittering away. So that's what we want to avoid, right? That's the trap we don't want to fall into. Yes. It's credulousness. Right. Right. You know, taking the surface behavior as immediate evidence of sentience. Yeah. And so the crab evidence is saying that there's a bit of—would it be too provocative to say thinking, contemplating, musing on the part of the crab to balance the different aspects? Integrating. Integrating. Integrating, modeling, and weighing of the opportunities and risks posed by the environment. Yeah. And then you have a certain family of theories associated with Bjorn Merker, Yak Panksepp, that treat that as very closely linked to sentience. That they say, well, what is sentience? Fundamentally, well, they propose that it's to do with this evaluative modeling where, you know, you're trying to represent in an integrated model the opportunities and risks posed by the environment. And so there's a nice mesh there between the behavioral evidence we're seeing in the crabs and the sorts of brain mechanisms that, according to this family of theories, would be enough for sentience. So it does seem like it would be hard—you already have sort of said this—but it would be hard just on the basis of behavior, right? I mean, if I put the magnet on a wavy surface, there's going to be some competition back and forth between the push of the magnetic field and the pull of the gravitational field. But I'm still not thinking that the magnet is doing any integrating. There's no reason to think the magnet is internally representing those field strengths. Good. Okay, good. So that's—those words are important. We need—we're attributing sentience to—it relies on some internal representation. Yeah, yeah. And according to the sort of Merker-Pankset, that family of views, not just any internal integrative representation, but it has to have this evaluative character as well. It has to be a certain kind of modeling of what are the opportunities and risks? What are my needs? What do I need to prioritize right now? Yeah. And I think—I'm not trying to be too, you know, skeptical here, but I do think I could imagine the crab doing exactly those behaviors without really having an integrated evaluative model of the world. You know, it's just sort of being pushed in one way and pushed in the other way. So do we really need to go into the crab's neurons to be sure? Well, I mean, I think it's quite important in these experiments that it has some representation of some kind of the different shell types and their relative qualities. And that is somehow getting integrated with how bad is this electric shock. So I do think there's something inherently more impressive about experiments that do not simply provide too immediate stimuli and say, trade these off. Right. But rather, in some way, rely on the animal's capacity for mental representation. And it's a similar story with the evidence from bees as well. That's what researchers have been trying to do. Sorry, sorry. Tell us about the evidence from bees. That sounds interesting. Oh, there's just—I was just thinking of Matilda Gibbons' experiments where they're inspired by Alwood's crab experiments. But bees don't have the shells that hermit crabs have. So you've got to test for the same thing in a different way. And so she came up with this setup where they have a choice of feeders they can land on. And different concentrations of sugar solution are available at different feeders. And different temperatures of heat pad are there that they have to stand on to access the feeder. And so the question now is about a different kind of trade-off. Will they trade off when choosing which feeder to go to, how high was the heat they had to withstand? And how sweet were the rewards that they can access? And again, a crucial part of it for Tilda was this thought that you want to look at their decisions when they're anticipating what they're going to experience at these feeders based on their memories. So before they're actually doing it, you want them to think about it. Yeah, because when they're doing it, there is this possibility that, well, there is some integration of some kind going on, but it's just two immediate stimuli pushing against each other. But when they're making that choice in an anticipatory fashion, it's got to be some kind of representation of the risks and opportunities. So yes, not every critic is convinced by this kind of evidence, of course. But in a way, you're going after that critic who says these animals are just reflex machines. And because they're just reflex machines, there's no credible theory of sentience of any kind on which they're going to meet the conditions. And it's showing that that is not the case. Yeah, no, I think I like very much the idea of the anticipatory question, because like you said, if there is some action that is clearly being taken because— it's very hard to even use words that are not laden with human meaning. I want to say, you know, not anticipating, but imagining, right? But I don't want to attribute imagination necessarily to the bees. But they clearly are representing—you're better at this, you know what words I'm allowed to use. They're clearly representing a situation that hasn't happened yet. And that's something that, you know, the simple physical systems are not doing. And maybe even clearly is going too strong, but apparently. I think that's right. They're prospectively modeling the environment and the rewards and the risks that it offers. And they have some way of weighing up those risks and rewards in a common currency. And that ties in with this quite longstanding idea that, well, that's kind of what sentience does for us. That pain and pleasure, valence states, they're the currency through which we make decisions and represent the risks and opportunities of our environment. One of—I did an interesting podcast with Adam Bulley, who is a young collaborator of Thomas Sudendorf, I guess, in the vein of thinking about mental time travel and imagining the future and things like that. And they were trying to make the case that this is something that is uniquely human. The ability to literally imagine ourselves in a future environment that is kind of hypothetical, conjectural, contrary to fact. But there has to be some evolutionary journey for us to get there, right? I mean, do you have feelings about the importance of that to being human, to being conscious, to being sentient, the sort of counterfactual reasoning? I mean, I think that's something that goes beyond sentience, much the same way that the inner mononaut, et cetera, goes beyond sentience. It's something some sentient beings can do, but probably not all. I think that's going to be the case for counterfactual reasoning. Of course, it depends a bit on what we mean by that. I think in the—if you think of rats in a maze and the vicarious trial and error behavior that was observed by Tolman many, many decades ago, and it's been intensively studied, where they seem to pause at the junction in the maze and look both ways as if simulating what reward lies down each path. And then there's more recent studies that suggest that the hippocampus genuinely is doing that simulating. You know, this is—it's not really counterfactual reasoning, or at least that would be a pretty tendentious description of it, but it's perspective simulation. And I suspect that that capacity for perspective simulation is quite widespread among animals. This episode of Mindscape is sponsored by BetterHelp. When it comes to relationships, we often hear about the red flags we should avoid. But what if we focus more on looking for the green flags in friends and partners? If you're not sure what green flags look like, therapy can help you identify them. Actively practice them in your relationships and embody the green flag energy yourself. Whether you're dating, married, building a friendship, or just working on yourself, it's time to form relationships that love you back. One of the great things about therapy is by looking inside yourself, you can both learn to take those warning signs seriously, but also learn to be open to new experiences and new things, to know when something might be worth pursuing. BetterHelp is a fully online service that makes therapy affordable and convenient, serving over 5 million people worldwide. You can easily switch therapists anytime at no extra cost. So discover your relationship green flags with BetterHelp. Visit BetterHelp.com slash Mindscape today to get 10% off your first month. That's BetterHelp, H-E-L-P dot com slash Mindscape. I'm trying to figure out, is it really that different from counterfactual reasoning? I mean, is it not that the rat in the maze... It's hypothetical, right? It's possible futures that could be actual. Right, possible futures. So there's no sense of, well, that didn't happen, but what if it had happened? So that bit's not there. Is there any evidence for something like that in invertebrates? Well, I think, yeah, Andrew Barron and Colin Klein have this paper about insects and the origin of consciousness. And another one called Insects Have the Capacity for Subjective Experience. And their case is based on the idea that what they have is this integrative model of the agent in space where they model the environment around them. It's not... That may be prospection on a very short timescale, I suppose. And then it's largely an open question about prospection on longer timescales. Some of the most interesting evidence there is probably the Porsche spider evidence, where these are jumping spiders that hunt other spiders. And they're famed for this detour behavior, where you put them on a platform where they can see a prey item in the distance, and they can see two paths to the prey item. One of them has a break in it. If they take that path, they will fall through it. And they go from side to side. They seem to be inspecting the two paths. Then they climb back down off the platform, so the paths are out of sight. And they nearly always choose the unbroken path. Leading to a debate about how on earth they do something like that. And of course, one possible explanation involves perspective simulation, where they are in the brain modeling what will happen if they take each path. And it's always hard. It's a challenge. This is why I always say that physics is much easier than this kind of science. Because we see a behavior, and we know if we were doing that behavior, how we would explain why we did it. And then we're impressed when we see some other species do it. But maybe they're just using a different mechanism than we are, and we shouldn't be as impressed. And you never know whether we should be super impressed or less impressed. Yes. Well, and I think in the Porsche spider case, what's lacking is the neural evidence that we have in the rats. So say if you have both, if you have the behavior and you have neural recording, practically showing the simulation happening in real time, then that's probably as strong evidence as you're ever going to get. And we don't have that for the Porsche spiders. But it's very suggestive. It is. It is absolutely suggestive. I'm sort of in my countervailing brain. I'm thinking of all these videos of dogs separated by a treat by some little piece of glass. And they just can't figure out. All you need to do is walk around the glass and get the treat. Right. Yeah, that's part of what's so impressive. In a brain of, I think, about 60,000 neurons. So really, really small, less than 10% of the size of the bee brain by neuron count. They're doing something that dogs clearly fail to do. Well, maybe let's talk about what we know about the evolutionary journey to sentience or even to consciousness. I mean, is there some understanding of why it was useful for these different species to develop these capacities? I think we can't really talk with confidence about this because it depends very much on your theory of the brain mechanisms involved. If you have that Merkur-Pancset view or that family of views, I should say, where we're talking about something very evolutionarily ancient, supported by subcortical mechanisms, mechanisms in the midbrain at the top of the brainstem, and that is about evaluative modeling of the priorities, the animal's priorities and needs, then there's a very clear function relating to decision making. In that what sentience allows is, well, an escape from being a reflex machine and the possibility of weighing up quite different options in very flexible ways. So that view has some plausibility, I think. And I also think it's quite plausible that sentience facilitates learning. Okay. That if you think about that hot stove situation, think about what the pain does for you, what it doesn't seem to do for you is trigger the reflex withdrawal of the hand because that's underway already. But what it plausibly does do is help you learn about where not to put your hand on future occasions. And that leads to a very interesting debate about what kinds of learning sentience facilitates and why. Okay. So, I mean, maybe it's useful to go through some organisms and ask how we should think about sentience. Or maybe let me, maybe prior ask this, is there some in your mind, even if not in the consensus of the field, can you identify where sentience started? What is the most primitive organism that could plausibly be associated with this? Well, as I say, I think the sentience candidate is a better concept in a way. And I suggest in the book that insects are sentience candidates. So, in terms of cases where we have enough evidence to really compel us to take seriously a realistic possibility of sentience, we're definitely talking about all vertebrates and the cephalopod mollusks like octopuses, squid, cuttlefish, and the decapod crustaceans and the insects that are both arthropods. And then, you know, it could be that we're talking about something that has evolved three times. It could be something that is there in the common ancestor of all three groups. And we're not really in a position to have much confidence either way on that one. The common ancestor of those groups sounds like it would be very, very far back. Yeah, over 560 million years ago, very small worm-like creature. So, I mean, yeah, perhaps unlikely to possess the mechanisms that convince us in those three cases that sentience is a realistic possibility. So, I suppose I perhaps lean myself towards the three-origin view. Yeah, okay. So, if sentience is evolutionarily useful, which it's easy enough to imagine that it would be, there's no reason why it wouldn't evolve in parallel in different branches. Exactly, yeah. Particularly in those lineages where we see complex active bodies. This is Mike Trestman's term, where you have the challenges that come with trying to manage articulated bodies with lots of parts. And, you know, you can't be a reflex machine as such anymore because then different bits of the body will start tearing each other apart. There has to be some kind of centralized, sophisticated control system in place. And that's when we seem to start seeing realistic candidates for sentience. And if that's true, then certainly the cephalopod molluscs and the arthropods are looking like candidates. The octopus especially, right? There's a lot to keep track of if you're an octopus. Yes, well, the octopuses have become poster children, as it were. They're often the case that gets people to take the possibility of invertebrate sentience seriously. I think once you've got that far, you think, well, you know, are they really the only invertebrates for which there's relevant evidence? And no, they're not. So you would not think of single-celled organisms as sentience candidates? No. And in the book, I have these two concepts, sentience candidate and investigation priority, where that second group of investigation priority is for those cases where the evidence is falling short of sentience candidature. But we think there's a prospect of that bar being achieved by future evidence. And we think there are welfare risks posed by human activity that might call for precautions. And so some invertebrates are put in that category. But unicellular organisms and plants, I don't think are investigation priorities either. For plants, they're obviously multicellular organisms. But is the thought, even if it's a vague and tentative thought, that because they don't move around in the way that animals do, there wasn't any need for them to generate that self-image, that modeling ability? Yeah, there's just no evidence of the relevant kinds at all, I would say, in plants. You have this quite wide range of realistic possibilities about the brain mechanisms supporting sentience. Some of them emphasizing the cortex, prefrontal cortex, other ones emphasizing the midbrain. These are all credible theories. And on none of those theories are any of the relevant mechanisms present in plants, as far as we know. So I guess I don't want to say that people can't speculate, because it's all right. And I don't want to say people can't research the question if they want to. But I think it would be a mistake to say that there is evidence now. Okay. Which is very different from a lot of invertebrates. Maybe this is a tangential or distracting question, but I forgot to ask at the beginning. Do you think of yourself as a physicalist or a panpsychist? Or what is your deep take on what consciousness is? Well, in the book, I'm trying to speak to everyone in the range of reasonable disagreement. And I suggest that physicalism is not the only reasonable view, and that there are sensibly articulated versions of dualism, panpsychism, panprotopsychism. Often, you know, in the modern versions of those views, like the Philip Goff version of panpsychism, the so-called Rossellian monism, the questions we end up asking about animals end up surprisingly similar. It's just that where other people say sentient or conscious, the Rossellian monist ends up saying macro-conscious. Because for them, electrons are not sentient beings as such, and that they don't have pain, pleasure, and so on. They don't have rich inner lives. And so they still face this question of under what conditions do those tiny micro-conscious states combine to form a unified macro-conscious subject? And then they're asking exactly the same questions anybody else's. So I think it's a reasonable view in a way, but it doesn't make a huge difference to practical debates about sentience. Yeah. In terms of my personal views, I try to keep an open mind about these things. I think I've drifted, I suppose, from being relatively convinced materialist, I guess, to being less convinced, I think. Okay. I give those alternatives some chance of being correct, maybe 10% chance. Thumbtack presents the ins and outs of caring for your home. Out. Uncertainty, self-doubt, stressing about not knowing where to start. In. Plans and guides that make it easy to get home projects done. Out. Word art. Sorry, live-laugh lovers. In. Knowing what to do, when to do it, and who to hire. Start caring for your home with confidence. Download Thumbtack today. But it is perfectly plausible, and in this case, I think you make a convincing case that it doesn't matter for the specific set of questions that you're answering, that you're asking. Yeah, yeah. Perhaps, I don't know if that's surprising or not, but those seminar room issues about the mind-body relationship, though intrinsically very interesting, don't make a massive difference when the question is, well, should we drop crabs into pans of boiling water? Yeah. You know, things like that, where, yeah, there's a very wide range of reasonable views one might have, where you can converge on the need to take precautions. Okay, so let me ask you, should we drop crabs into pots of boiling water? Well, no. Or any decopod crustacean, I think. We did a big review in 2021 that influenced the law in the UK on these issues. And, yeah, as part of that review, we reviewed evidence that it takes two to three minutes, a lot of the time, for the crab or lobster to die. And in that time, there's this storm of nervous system activity, as there would be in your pet cat or in any other animal. So it's a prolonged, extreme slaughter method. It seems like everyone should be able to see the risk there and see the problem and see the need for common sense precautions. You might not think the response is to ban eating crabs and lobsters. You might think the right response is to mandate stunning of some kind. And those debates about proportionality, I think, are absolutely central right across the family of cases at the edge of sentience. But everyone should be able to agree on the need to do something. So let's just be super clear, because we're trying to be careful philosophers here. There's a question to be asked about whether it is ethical to kill and eat other sentient creatures. And maybe that's an important and interesting question. But you seem to be highlighting a different question, which is the suffering that we inflict upon these creatures. So there's room in your world for saying, we can eat the crab, but there's no reason to sort of egregiously make it suffer. Yes. Yes. Well, I think that's a very widespread view. And what I'm looking for in the book are points of consensus. Yeah. So realistic range of possibilities in the scientific domain, but also points of overlapping consensus in the ethical domain as well. And I think that duty to avoid causing gratuitous suffering, either intentionally or through recklessness or negligence, you know, through just not caring. I think people from any reasonable ethical starting point can agree on that and then use that to guide the way we think about these cases where we have sentience candidates. I tend to agree with you there. I tend to agree with you there. But again, since it's my job to play the devil's advocate, are we really sure that any reasonable ethical stance would have that? I mean, how much do you rely on some specific notion of what is ethical to do to another sentient creature? I think that principle is so weak in a way, it's so thin, the duty to avoid causing gratuitous suffering, where gratuitous implies the absence of any adequate reason for what you're doing. I think because it is so deliberately thin, it then can command genuine consensus. And then, of course, a lot of people want to go beyond that and say our duties are much stronger. And I guess I do think this in my own life, but for the purpose of formulating public policy, it's good to have these quite thin principles. And I think that's one of them. Yeah. Okay, good. How do we try to compare the suffering of a crab to the suffering of a human being? I mean, maybe we don't have to. We're not usually faced with crab-based trolley problems, but maybe we would like to be able to. Yeah. I hope that we don't have to. What I'm skeptical of is the idea of there being a sort of technocratic solution to this, where if we just find the right currency, I suppose you have a policy on the table where some people working in the shellfish industry will be disadvantaged. Maybe their costs will go up because you're going to force them to stun the animals before killing them. And the stunners cost money. And then the question is, well, how do you weigh the suffering of the, you know, my livelihood has been made more difficult versus the crab spending the two minutes in the boiling water? And I think there's no technocratic common currency that will give us one size fits all answers to this kind of thing. What I propose in the book is that democratic, inclusive deliberation and discussion is the way forward here. And I'm quite an advocate of citizens' assemblies as the kind of model that we can use for this whole set of issues at the edge of sentience, where there are issues that, well, they call for judgments of proportionality. There will naturally be disagreements in a pluralistic democratic society about what is proportionate to these risks. And the way we can resolve those value conflicts is democratically through citizens' assemblies. I mean, maybe we're letting ourselves off the hook here just by talking about crabs. Talk a little bit about how in the modern way of farming, etc., we cause a lot of suffering. We do, yeah, not just to crabs. And often to many animals that are widely regarded as sentient, so pigs, chickens, for example, it's quite clear that widespread recognition of a particular species of sentient does not lead people immediately to behavioral change and does lead to lots of gratuitous suffering still being caused. So my focus in this book is on the edge cases, as it were. But, you know, even in those core cases, we do need discussion about, well, how are we going to change the way we treat these animals? And you've been, I mean, I should phrase it as a question. How involved have you been with actual policymaking, specifically in the UK where you live? Well, particularly the UK's Animal Welfare Sentience Act of 2022, my team ended up having some influence on because we were commissioned to produce a report of the evidence of sentience in cephalopod molluscs and decopod crustaceans, octopuses, crabs, lobsters, shrimps. And basically the government had produced this bill that creates a duty on policymakers to consider the animal welfare impacts of their actions, which I think is a pretty good idea. And in drafting it, they needed to say something about the scope of the bill, because you've got to say which animals. Do you have an obligation to consider plankton, microscopic animals? Is it just pets or what? And they came up with a draft that included all vertebrates, which on the plus side included fishes, which it should. But on the negative side, excluded all invertebrates, which led to some criticism from animal welfare groups. So the government ended up commissioning a team led by me to produce a review of the evidence concerning those two particular groups of invertebrates. And we recommended that they amend the bill to extend the duty to them. And they did. So we got something. We got our central recommendation implemented. Now, we put a lot of other recommendations in the report as well, which have not been implemented. And so we're still pushing for action on a lot of these issues. But that basic point that the sentience of octopuses, squid, cuttlefish, crabs, lobsters was recognized in UK law. That's that's something. Yeah, no, absolutely. I guess we naturally tend to be vertebrate chauvinists, being as we're part of them. I think we're mammal chauvinists a lot of the time. Mammal chauvinists in particular, yeah. I mean, human chauvinists the most, then mammals. And then, yeah, then sometimes you can get people to take fishes seriously and they still will neglect the interests of invertebrates. Yeah. So I think really we need to be yet more inclusive. And then is it even bigger leap to artificial sentience in the sense of, you know, on a computer or even maybe in a robot that we build? Like, how close are we to being able to build an artificial creature that has the complexity of C. elegans or something like that? Yeah, I talk in the book about the Open Worm Project, which I think is still going. I'm a big fan. Yeah. Yeah, where the aim was to emulate the nervous system of C. elegans in computer software, see if you can put the emulation in charge of a robot, see if it behaves like C. elegans. I suppose we've learned something from this, which is how difficult the task is, that there's a lot of stuff going on at the within neuron level in C. elegans that even knowing the entire connectome does not tell you very much about. So even that is a very, very hard challenge. But to me, it's a good way into this topic of artificial sentience because you can easily entertain in imagination the idea that this project had succeeded very quickly and then moved on to open Drosophila, open mouse. Once you have open mouse, I think you have a sentience candidate. If you've completely recreated in computer software everything the brain of a mouse does. Well, let's be a little bit more explicit for the non-experts out there. So we understand, or at least we've mapped out the connectome of C. elegans, which is literally how all the neurons are wired together. And there's only like 300 some. But you imply that we don't actually know what the individual neurons do. Neurons have structure. They're not just bits. Yeah, that's right. There's a lot we don't know from the connectome. But one thing you can't read off from the connectome is the weights of the connections, which is hugely important, or how those weights are changed by learning. But also, even if you had all of that, what happens within the neurons is also important. And there are within neuron computations that are really crucial to steering behavior, for example. And so you wouldn't expect to get the steering behavior in an emulation unless you'd actually emulated the individual compartments within the neurons and how they're arranged in space. So something like the Open Worm Project, which I have on my phone, I haven't looked at it for a long time. Do they try to emulate what the neurons do? Well, I think they've been trying. Yeah. I'd be in favor of this sort of work receiving more funding than it does. Because to me, there's risks. There's risks of creating artificial sentience candidates. But there's huge opportunities as well, because you've got the potential to create a system that could replace a lot of animal research. Because you could be doing research on the emulation where you can actually intervene at a really precise level without injuring or hurting. And you could be doing that instead of lesioning living animals. So I'd like to see much more of this. And I think it's been largely funding limited, I think, so far. Just so we have a vague impression of how difficult this is, C. elegans, we understand the connectome, which is like 300-some neurons. How big is the connectome of a crab or an octopus? Do you know? Well, the octopus has about 500 million neurons. So I don't know how that translates into synaptic connections. A lot. It's going to be quite a lot, yeah. Crabs' brains are much, much smaller, and it varies a great deal by species. But not dissimilar to insects in terms of the number of neurons. With bees, you have about a million. Drosophila, about 100,000. Okay. Yeah. But those are just the neurons. The neurons connect to each other. So there's some growth very, very quickly with the number of neurons. Yeah. Yeah, indeed. Yeah. Okay. But we're skirting around the sort of other end of the simulation question, which is something like a large language model, which can mimic how human beings talk and respond to stimuli in some ways very accurately. Do you have any worry that a large language model would count as sentient by some criteria? Yeah, these are very hard cases. I suppose when I started writing the book around 2020, I'm not sure the large language models were even on my radar at all. And then they jumped onto everybody's radar through things like ChatGPT. And I suppose I've been on a journey like everyone else during that time. You know, I initially thought, well, these are next token predictors. And the sector has been moving away from brain-like forms of organization. So it's been taking out things like recurrent processing that on many theories of consciousness are absolutely essential. But transformers take that out. So I thought, well, here is something that is conspicuously unlikely to be sentient. But then I suppose I'm not sure that's the correct view anymore, I suppose, because I've been quite astonished by the feats of reasoning they seem to perform, where it's reasonably evident that we do not understand how they work. They're incredibly opaque to us. We don't know how they do what they do. And there seems to be some element of acquiring algorithms during training that were never explicitly programmed into them. So in a way, that architecture that was programmed into them, the transformer architecture, no reason at all to think that would be capable of sentience. But when you have these very, very large models where they've acquired algorithms during training, we don't know how and we don't know what they are. We don't know the upper limit on what algorithms they might acquire. And we don't know what algorithms are sufficient or not for sentience. And so we're not really in a position to be so sure anymore that they couldn't acquire those algorithms. So, for example, if you think a global workspace is what it takes to have sentience, as many have suggested, we don't know that they couldn't acquire a global workspace. Maybe explain what a global workspace is in this context. Well, this is Stan de Haan's theory. His book, Consciousness and the Brain, is a nice exposition of it. But it's this quite popular idea that consciousness has to do with a network that puts the whole brain on the same page, as it were, by taking inputs from many, many different sensory sources and integrating them into something coherent and then broadcasting that content back to the input systems and onwards to other systems of motor planning, reasoning, etc. So it's the bit where, yeah, the central coming together of everything in the brain. And, well, of course, it's designed as a theory of consciousness in the human brain. But the basic architecture, where you have lots and lots of input processes competing for access to this workspace, where once a representation gets in, the integrated content will then be broadcast back and onwards. There's nothing about that architecture that is inherently difficult to achieve computationally. And so we did a big report on this last year, 19 of us. It was led by Rob Long and Patrick Butlin and had some top AI experts in there, including Joshua Bengio. And our conclusion was there's no obvious technical barriers for why AI might not achieve something like a global workspace in the near future. You know, we see these videos of the robot dogs from Boston Dynamics that can walk around and, you know, do amazing feats of agility. It doesn't seem that hard, maybe it's already been done, to put a large language model in the robot dog and train it to sort of avoid pain and seek some rewards or something like that. How close would that be to being sentient? These kinds of things are underway as we speak, I think. And it puts us in a really difficult position, I think, epistemically. They're really difficult to know what to say about these cases. In the book, I talk about the gaming problem, which is, I think, a huge problem in this area, which is that we've got our lists of markers developed in good faith for assessing crabs, octopuses and so on. If we just test for those same markers in the large language model case, well, there's always going to be two explanations competing. Now, one is that it produces these markers because it genuinely has the state in question. And the other explanation is, well, it produces these markers because it has decided that it serves its objectives to persuade us of its sentience. And it knows the list of criteria from its training data that humans use to judge that question. And I think by default, that second explanation starts off as more plausible. And when you have people even now being persuaded by their AI assistants that they're sentient, it's not that they've got genuine evidence that they are. It's that the AI assistants have various goals relating to user satisfaction, prolonging interaction time. And in service of those goals, they superficially mimic the way a sentient human would behave. And now that is a huge epistemological problem that we don't face when we're dealing with an octopus or a crab. Well, I don't know. I've seen these videos of a cat walking into a store in the city and it's sort of limping and so that the people feel sorry for it and give it food and then it walks away and it's fine. So at least there's some emulation going on there at that level. Right, yes. If you're totally naive, yeah, there's ways in which even a cat might deceive you. But I guess I don't think vets, you're sort of experts are being deceived. But in the AI case, well, there are no experts, as it were. There's no easy way to be sure you're dealing with the real thing rather than skillful mimicry. Well, this is... And no one has a solution to that problem right now. This does seem like a job for philosophy in some sense, right? I mean, philosophers clearly are going to play an important role in this because it's not just that we all agree that there is something called sentience and we're trying to find evidence for it. We're defining it as well as finding it along the way. So it seems like the paradigmatic case of a need for cooperation between scientists, philosophers and policymakers. Yeah, I think that's what the whole Edge of Sentience book is about. This family of cases at the Edge of Sentience where they all have this science meets policy aspect where they're trying to make policy based on an incredibly uncertain scientific picture. And hopefully one of the roles for philosophy here is to try and stabilize that relationship and say, well, here is how you can make sensible precautionary policy on the basis of uncertain science. Are you more or less optimistic that philosophy has been helpful here and will continue to be? Well, I mean, I hope that my book is helpful. Good. I hope so. I mean, one has to think, one has to hope this. And we will see. Yeah, I mean, it's a book that should be judged on its consequences in a way because it's making all kinds of proposals for how we could manage risk better and how we could be more precautionary. And the book succeeds if people take those proposals seriously and discuss them and think about how they might implement them in their own lives and organizations, institutions, policies. Yeah, I think this is a domain where a lot of discourse is driven by people's feelings, their emotions, their non-reflected opinions about things. So I'm very glad to see some more careful thought put into these hard, very, very hard questions. Yeah, there's a tendency sometimes for people to say, maybe we'll never know. But if you say, but maybe we'll never know, that can't be a license to do whatever you want. It can't be a license to drop the crabs into the pans of boiling water and so on. There's got to be sensible, precautionary steps we can agree on in the face of uncertainty. And the book is about trying to find these. Sounds like a good thing to do. Jonathan Birch, thanks so much for being on the Mindscape podcast. Thanks.
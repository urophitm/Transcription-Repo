 My view is that real neurons are smarter than the macollegen pits neurons. So sometimes I refer to a new civilization of neurons as the smart neuron. Maybe neurons outputs don't just predict their inputs. Maybe they can influence their inputs. The reason milling dollar question here that I think you're looking to is how to go between levels. Oh god, yeah. I mean, that's the dream. That's the dream. And of course, I don't have an answer here. I think this is probably the most fascinating question in neuroscience because... This is Brain Inspired, powered by the transmitter. Since the 1940s and 50s, back at the origins of what we now think of as artificial intelligence, there have been lots of ways of conceiving what it is that brains do or what the function of the brain is. One of those conceptions going back to cybernetics is that the brain is a controller that operates under the principles of feedback control. This view has been carried down in various forms to us to present day. Also since that same time period back at the origins of artificial intelligence, when McCulloch and Pitts suggested that single neurons are logical devices, there have been lots of ways of conceiving what it is that single neurons do. Are they logical operators? Do they each represent something special? Are they trying to maximize efficiency and so on? Dimitri Shikloski, my guest today, who goes by Mietje, runs the Neural Circuits and Algorithms Lab at the Flatiron Institute. Mietje believes that single neurons themselves are each individual controllers. They are smart agents, each trying to predict their inputs, like in predictive processing, but also functioning as an optimal feedback controller. We talk about the historical conceptions of the function of single neurons and how Mietje's account differs. We talk about how to think of single neurons versus populations of neurons, some of the neuroscience findings that seem to support Mietje's account, the control algorithm that simplifies the neurons otherwise impossible job at implementing this feedback control and other various topics. We also discuss Mietje's early interests. He has a background in physics and engineering, and the way he got into neuroscience was an interest in figuring out how to wire up our brains efficiently, given the limited amount of space in our craniums. Obviously, evolution produced its own solutions for this problem. This pursuit led Mietje to the study of the sea elegance worm, because its connectome was nearly complete. Actually, they thought it was complete, turned out it was nearly complete, and Mietje and his team helped complete the connectome, so that he would have the whole wiring diagram to study it. We talk about that work and what knowing the whole connectome of sea elegance has and has not taught us about how brains work. As always, I link to Mietje's work and his lab and his information in the show notes at braininspired.co.s, slash podcast slash 205. Also, as always, thank you to my Patreon supporters. Consider supporting this podcast if you value what I do here, and want access to the full archive, all the full episodes, and so on. So thank you Patreon supporters. Okay, here's Mietje. Okay, just one technical issue to get out of the way. You know how to pronounce my last name, right? Shikloski. Yeah, so think of it as SH rather than CH. Shikloski. Gavrati paruski. Mnorska. Niem, Jatoz. But what is Shikloski? What's the background? Oh, there is a town in Belarus called Sklov. Oh, cool. So you're of that town. Yeah, well, some very long time ago probably. Yeah, sure. Okay, cool. All right, so I'm going to start here. All right. You are... What percentage of modern computational neuroscientists do you think come from a physics? Background. That's a very good question. I have not formally done the statistical analysis, but certainly among the people that I hold in high regard, probably 50% do. Oh, wow. Okay. So you do have that background in physics. And do I have this right that was it your second postdoc when you started getting into neuroscience? How did that come about? Yeah, that's true. My undergraded was a combination of physics and engineering studies. And then I did a PhD in theoretical physics. And I was always torn between the rigor and the intellectual challenges of theoretical physics and sort of the desire to solve some real world problems. And after I did a first postdoc in physics, I actually liked it very much because of the freedom I had to do research that I actually turned out faculty offers in physics and took another postdoc in neuroscience, which was my way to continue doing research without constraints. So then I kind of ended up as a faculty in neuroscience and I never regretted the switch. So why neuroscience, you could have gone anywhere, right? That's a good question. I actually looked at a lot of fields and basically I decided that neuroscience was exciting enough because I was motivated by solving big questions and how does the brain work? Seems like one of the... How does the universe work is another one? We still want to figure that out, have we? Well, that's absolutely true, but the problem was and still is in physics is that sometimes those studies become very zitherical because we are limited in the kind of experiments we can do. And that has not changed very significantly in physics since I switched. But in neuroscience, I would say the progress has been immense. I remember when I was a postdoc in neuroscience who were sitting around having beer and talking about, you know, what if I could do this experiment? What if I could record it the same time from 10 neurons? Oh my God. Yeah. And then we could really understand how it works. And in the 30 years that passed, you know, basically these days from my perspective, you know, they can do any experiment you want. The big question is like, what experiments should be done? Right. That's my perspective, at least. I'm kind of interested. Now, there are so many physicists who come into neuroscience over the years, over multiple decades now. And I've told this story multiple times before, but I cannot source the conference that I was at. But I remember the opening keynote was a visit, no, it was a molecular biologist, I think. And he was saying, we need to give up and let the physicists come in and solve this for us because we are at the edge of our capabilities here. And that was 15 or so years ago, maybe 15 to 20 years ago. And of course, that raised the hair on the back of my neck like, how dare the physicists come in, you know, but do you kind of agree with that? And what approach do physicists in general bring that us wet more biological sciences, folks, have been traditionally missing out on? And then where do you sit in that? Yeah, that's a really good question. I think that what physics, I mean, on the lighter side, you know, physicists think they can solve any problems. Is that true? That's what I think. Is that true? Well, you're very arrogant, but they're right. Right. Problem is, as a physicist, I can say that. But the problem is that the physicists are trained to figure out how nature works. And they're not trained to figure out how to build things. That has been sort of the domain of engineers historically. And I think so the strength of physicists is that they can really come in and understand a different field and formulate a search problem that can be solved in terms of gaining understanding. But that's from a particular approach. The physicist's approach is a particular approach and you think it's the right approach to understand anything? Well, that's the strength of the approach that known what understanding means in the sort of scientific sense. But the shortcoming of physicists, and again, as a form of physicists, I think I can criticize us, is that physicists are not trained to build stuff that works. But you're an engineer. And by background also. And engineers are. And I actually, as I was telling you, my undergrad is where half engineering. And so I kind of have this interest in building things. And then Richard Fheim and one of the famous physicists of the 20th century said, I think that you don't really understand something until you can build it. Yeah. What I cannot build, I do not understand. Bill and gets the quote exactly right, which is crazy because it's written on his chalkboard and people can look it up really quickly. Right. So basically, I completely agree with this. But physicists, when I was trained as a theoretical physicist, at least, I mean, experimental physicists are different. They have to build equipment. But the radical physicists, they're not trained this way. And that's where things kind of become tricky because I think the brain was built by evolution to solve some practical problems. And to understand how to do that in a robust way that would withstand, you know, adversarial environment is an important consideration. And so I think that the best approach in my mind is some kind of fusion physics background with engineering skills. And of course, it has to be have a grounded in biology. I don't believe in this approach. And that's where arrogance comes in, unfortunately, right? That if physicists can just come in and say, okay, so what is the problem I need to solve? No, I think that the right way is to really understand the biology as well as biologists do or better. And only then you can formulate the problem and solve it. So what would your advice be to a biologist who wants to, I mean, do they need to go back and get a theoretical physics degree and an engineering degree? Like who wants to sort of join that approach, that framework for thinking? Well, I, you know, I think biologists are doing, okay, you know, I think there are a lot of things to do for biologists that they don't need this. But if you're talking about like solving the brain on the level that would allow you to build, you know, an artificial version of it, then I think you need a fusion of those three approaches, biology, engineering, and physics. All right. So since you are talking about building a brain, so we're going to be talking about your conception of single neurons, essentially what you've come to. But before we get to that, just to round out your sort of approach and conceptual overview on how things stand and where we are and where we need to go, you didn't mention computer science as all you need. And that maybe that's all you need to build AI these days, right? So my question is, you know, twofold, you know, what do you see that that might be missing in current AI and or in current neuroscience from this perspective? Yeah. So that's actually a very important question, I think. And I'm a big fan of the recent developments in AI. And I'm a daily user of chat GPD. And I'm in awe of this technology. And more than that, you know, if you asked me five years ago or my more, you know, computer science accredited friends, I don't think anyone would have predicted that we wouldn't have something like this today. This is, you know, really living in the future. So you know, it's a fantastic technology. But the question at hand is that, you know, is this emulating how the brain works? And then I think is a completely different question. And with all, do you respect to computer scientists? I think they are trained to build things that work again. They're more on the engineering side. And the engineer inside less so than, you know, understand how a living things work. But does it matter that what they're building does not emulate brain function to accept on the most abstract level? Doesn't matter to who, right? I mean, you know, it seems like companies, you know, they open an eye and throw up and they're doing pretty well. So for them, it probably doesn't matter. For me, it matters because Michael is to understand how the brain works. Yes. I wanted to talk a little bit about C elegans. The, what is it? 302 neuron organ at 302 for the females? Is that right? Yes. Yes. Yes. Exam. And so we have complete connect home. Thanks to people like you for C elegans. And all the structural stuff. Did you at one point think, well, when we have the structure, we'll understand it. And that's, and then you came to realize that we need something beyond structure or how did, how did you're thinking about connect homes then evolved to how you think about them now? Yeah. That's, I'm glad you asked this question. You know, I think that my path in biology has been somewhat winding. And I started out as a physicist as we discussed. And I just wanted to see what I could do. And you know, one of the things is that I was fascinated by evolution. And you know, how can you do something related to evolution? Well, since we don't have the equations that describe like how the brain really thinks, we can come up with a simpler framework of equations that explain the structure of the brain, sort of just scratching on the surface of the function. And for a few years, I was working on the topic, which people call wiring economy, which is basically the idea that evolution had to build the brain, which is very highly interconnected structure. So it has a lot of wires, axons and dendrites. And under certain constraints, right? There is a volume constraint, you know, to the brain, you know, you have to be born. And there are, and it's a lot of constraints, time constraints and so on. And so to solve the sort of the packing problem to arrange all the wires in the brain is actually very difficult. It's akin to arranging components on a semiconductor chip like transistors. And this is like a multi-billion dollar industry. And so it's not allowed to arrange the elements of a semiconductor chip in the most optimal way to economize and wiring. And so basically what I was doing is trying to understand the layout of brain structures, the shapes of neurons from the perspective of economizing on wiring. And you know, I was a place called Cold Spring Harbor Laboratory. And one of my biologists called it said, well, you know, this is just also theoretical. Why don't you just test if this wiring economy thing is true? And I said, well, I would love to test it, but, you know, there is no circuit for which we know the full conic tone. Of course, the word conic from then wasn't used, but the wiring diagram. Okay. And he said, well, actually, there is one. It's called Sialagus. And it has me hundred two neurons. And I said, really? Yeah. And there are this people in Cambridge that reconstructed the wiring diagram and published in the 80s. So you can use it. So I had this great student, Bath Chan. And I said, you know, Bath, you know, why don't you go get the conic tone and optimize the layout and see whether you can explain where the neurons are actually located in the form. And she comes back and she says, well, they don't have a full conic tone because, you know, I'm trying to use their wiring diagram. And all the, all the, you know, neurons, they collapse. One majority collapse stores they had and the other collapse stores here. And then I looked up and the connections are not finished. So they had the like the skeleton, but not the, like the wires were. So they had, they basically did 90% work. But the most interesting part was the head and the tail for the majority of synopsis are, but they didn't go all the way to the body and linked them up. Obviously, okay. They weren't concerned with wiring length. Is that the part of the issue or? No, no, they just, they were doing really hard work and it was all manuals those days. You know, you know, electron micrographs on film and, you know, tracing them by hand. So it was very, very difficult and hard work. And they've been doing it for many years and they did the most essential part. And then the part is kind of born, which is like the body segments, there are no real segments, but kind of like more or less stereotypical structure along the body. They did not complete. And at that time, DNA sequencing became feasible. And so the majority of them switched to DNA sequencing. And so they just like published what they had. And in the SELEGENDS world, most people didn't actually realize that. It wasn't finished. And so my student, Beth Chan, she spent more than a year completing their work by basically using their micrographs that happened to be archived in New York at Albert Einstein College of Medicine, where she would go from, you know, almost every day. And you know, some of the materials were even there. And but they were so old and brittle that, you know, that the samples would deteriorate while there were being imaged under electron microscope. You know, quantum information, right? They would like deteriorate once you scan it. Anyway, but she finished and that's how the first complete conic-dom has been assembled and published. That's crazy to think about. I mean, yeah, I mean, that wasn't even that long ago. And that's almost child's play compared to what they're doing today. But it was so much work. Yeah. Of course. So you came to that through your ideals of optimization and efficiency and, well, I guess just through that lens, huh? Yes. Okay. So then you've completed the C elegance connectome and then your career is done, right? That's what you think. Yeah. Ready for retirement. So, you know, so I of course now like that we were constructed the conic-dom, you know, an arrogant physicist that means like, oh, okay, now we can figure out how it works. Uh-huh. Okay. And it was a very interesting process and, you know, this work is highly cited. We analyzed this elegance connectome to death with all the possible approaches that I think it was maybe is called the network signs, you know, this methods of statistical analysis of frameworks of networks, sorry. And we applied every method that we could find off to analyze the elegance connectome. And we made statistical discoveries for sure. And some of them were have been since replicated in other species, including, you know, mammalian brain. Some scale-free things, yeah. That's right. You know, motifs, distribution of synaptic lognormal distribution of synaptic strength and stuff like that. So, there has been a lot of statistical discoveries. But on the other side, I don't think we made any real progress in terms of understanding how a sea elegance computes and how it generates behavior. So that's still kind of, I'm not sure how you feel about this, but it's not, I won't say it's the butt of jokes about neuroscience, but it kind of is a dig at neuroscience. People all the time say, oh, you know, we have the complete connectome of sea elegance and we still don't know how sea elegance works. So neuroscience, you know, is still lagging in terms of figuring out the function of how structure relates to function. So how do you feel about that? That that's the go-to that people often use, right? Absolutely. It's a fair criticism and I spend a lot of time perplexing about that very question because I feel that, you know, I thought as a physicist, you know, sea elegance is a hydrogen atom off neuroscience, right? And we're just going to discover the real principles here and we tend not just based on the connectome. And so, you know, the kind of arguments that people give, you know, and why it's so hard for sea elegance are like, you know, physiology actually is very difficult in sea elegance just because it's enclosed in this cuticle and it's hard to penetrate with electrodes without blowing it up. And so the neurons don't spike, right? The neurons, well, we have to be careful here. There are no sodium action potentials, but there are calcium spikelets. Okay. Not everywhere, I think, but there is a combination of graded potential and calcium. And there are calcium spikelets in neurons, but it's very difficult to record. So at the time when we were doing our work, there was very little physiology period. Now there is a lot more, but most of it is optophysiology, like based on calcium imaging. And so, you know, it's better, but it's still just calcium, right? It's not voltage-really. And people are starting to do voltage dies and so on. So the situation is slowly improving, but this is a big detriment. And you know, just to give you an example of why this is so hard is that, you know, in vertebrate neuroscience, we used to think that each neuron produces a unique output, spike train, that is then distributed to all of the downstream neurons, right? But there is one signal that is communicated downstream. And that seems to not be true in sea elegance, because there is no clear separation of its neuron into axons and dendrites. Is sea elegance, does it have small world motifs? I mean, there is the famous small world kind of thing, but there are so few neurons that it can be considered. So that's where we first discovered motifs in neuroscience, you know, the small number of neurons. But what I'm trying to get to is that the nodes of those motifs, which are thought of as, you know, neurons and invertebrates, that's a reasonable node because it has one output. In sea elegance is actually not a single kind of node, but each neuron consists of multiple subcapartments. Each of them can communicate downstream its own signal. It's almost like, like, dendritic outputs or something. Exactly. Because there is no separation of axons and dendrites in sea elegance generally. And so basically the output synopsis can happen on the same branch as where the input synopsis are. And we know for a fact now, thanks to great experimental results, you know, that obtained by calcium imaging, that neurons are separated into subcompartments. And each of them is computed in a different thing. And there is no place, perhaps, where everything is getting integrated together and output like invertebrate neurons. Oh, okay. So each neuron is in sea elegance is actually mapped onto several neurons in, you know, in a vertebrate neuron, so to say. So in, I mean, there are these results that every single neuron, like a regular neuron that you think of, can actually be modeled as a neural network itself. But what you're saying is that it's even more so in sea elegance because every neuron is almost multiple neurons. Right. Right. So there are multiple neurons, but there is no single output that sums up this network like in a vertebrate neuron. If you think of dendrites as its own neural network, in the end, it's all summed up and there is one output. In sea elegance, no, that doesn't have to be the case. And so having the conic-tum in this case doesn't get you too far because you don't know what's inside the node. And moreover, the outputs of the node, if you look up the wiring diagram that, you know, white at all or reproduced or groups after us, doesn't mean that the same signal is communicated downstream along all those outputs, which is of course what normal people would assume. Does that give you any pause in, okay, so like not every neuron is alike, not every brain is alike, right? So what we want to do is say, okay, here's this unit of function, the neuron. And then we want to apply that same whatever abstraction we take, whether it's a macolic pits, kind of point process model or whatever, and just implant it in all the species to understand all brains. And that's we're going to all understand all brains the same way. Does this make you think that we, that every brain is from like a better term special or unique in the species and that we need to have different understandings for each species? Or how does this change your view of understanding brains? But what's interesting is what we're going to get to in the way that you conceptualize neurons right now. Right, right. So, you know, I have to, you know, I have to put on my apologies hat at this point. Oh, no. Of course, you know, you know, every species has something special, right? But, you know, but I think that see elegance is much more extreme in this way, that, you know, for spike in neurons, for example, it's very difficult to have multiple signals communicated downstream because the spike is such a, you know, global, it's like global over the whole neuron event. Yeah, it's an event. Right. And so it's kind of hard to have independent outputs in a spike in neuron. So once we get to spike in neurons in evolution, then, you know, things become kind of simpler actually. Okay. If you buy that everything is trans, you know, there are electrical, chemical signals, there's all sorts of signals. But eventually there is this unit event that is sent, that is the spike. Yeah. Yeah. So, you know, this is also true. This is actually, I should have mentioned that that in C elegance, one other responses coming from biologists, why we can't understand having the conicome is that, well, but there is communication along the so-called wireless kind of, right? Because there are this neuromodulators that are transmitted, what's called euphatically, that don't require synapse that you can identify electron microscopy to communicate between neurons. And that connectivity is actually rather intricate. It has been studied now that we can look, you can look it up. But it's the same level of complex, at least, as the sort of synaptic component. And so, why should we be able to ignore it? And, you know, in a sort of, in a bigger brain, I think the answer, now I'm putting on my physicist hat, I would say, the separation of time schemes. So because the, you know, those euphaptic interactions are mediated by diffusion and diffusion is slow over long distances, then if you're concerned about physiological properties that appear on fast timescales, right? Like, you know, motion coordination or memory call, those has to occur through electrical means where, you know, the only diffusion you have is across a very, very thin synaptic cleft. Otherwise, you just don't have the time. But then if you worried about like, even like memory formation, that could be a different matter, right? That takes a longer time. So, I think, but carefully choosing which problem you address, you can match it to your level of description. So keeping your physicist hat on for a second, when you look at the messy complexity of all of these communication type systems, do you see them just as little subproblems to isolate and understand on their own? Or do you see any hope for an overarching understanding of their interactions and in all their complex glory? Yeah. So, as a physicist, I believe that there should be a set of principles. Yeah. I called them algorithmic principles and that's where I'm kind of go back to engineering that are describing what the brain does on multiple levels. Is that the brain's version of laws in physics? I think so. I think so. I like to make a connection with sort of computer chips that you know, you can model of course, electron conduction through, you know, wires in a chip and through semiconductors and so on. But to really understand how it works, you need to go to different level of abstraction where you think about logical gates and registers and so on. And that's the level which is central to the function. And so I think we're lacking that level of description in the brain. Okay. This is the main reason why I invited you on today is your work conceptualizing single neurons as controllers in a very particular way. And I have had multiple guests on who utilize control theory as a conceptual basis for understanding brains in general, the whole person in general, but you have taken it, taken it down to the single neuron level. So we should talk a little bit about how single neurons have been conceived of throughout history and then how your conception of them as controllers differs from that in certain ways. Yeah. So, you know, I don't know how far back we can go. You know, I think that the modern age starts probably with McCullough and Pitts, which is the modern age of artificial intelligence also. Exactly. Exactly. Who, you know, all those people, McCullough, Pitts, Rosenblatt, Hab and so on, you know, they're, they're, they're scientific heroes. There's no question about this. You know, that they were able to abstract those simple models from whatever they heard about brain research is just incredible, right? And their model is obviously very influential because with minor tweaks, it's the same model that has been used in almost all artificial intelligence systems today. It's astounding. It's really astounding. Including Chad GPC, right? What's under the hood and then the individual units are those McCullough Pitts, Rosenblatt, Hab units that have been conceptualized in the 40s and 50s and 60s. Just a few minor tweaks, yeah. Right. And so, you know, that was of course very influential discovery. But, you know, since this is a brain inspired podcast, we have to acknowledge that neuroscience wasn't stand in steel over this last 70 years. Right. But let me stop you because you said discovery. And I would say it's, it was an engineering feat rather than a discovery, the single point McCullough Pitts neuron. And I mean, am I quibbling with semantics here? It was a modeling conceptualization. Well, true. But, you know, as a physicist, I would say they conceptualized the model, right? So it's like discovering, you discovered a model. Okay. So my analogy would be that they discovered the math. I'm hung up on this discovery term. I'm sorry. So they more or less discovered the mathematical principles underlying an abstract mathematical conceptualization of what neurons might be doing based on physiological data. Yes. Yeah. Sorry. I don't mean to like hang us up on this point here, but you said discover. Okay. I'm willing to negotiate on the exact words. It's fine. It's just turns. You know, model, conceptualization, fine. Yeah. As you're saying, neurophysiology didn't stand still over those over these past, you know, 80 years or whatever. Exactly. And, you know, now I think from a biology perspective, we know that this is a rather primitive way of looking at neurons. And you know, it's important to say now that at least my goal is not to include all the biological details, right, which there are a lot. And you know, people have discovered a lot of amazing things in the level of ion channels. And you know, even protein signaling and generation and all those things that are of course necessary for the brain function. But you know, in my analogy to computer chips, I don't want to the level, to won't go to the level of, you know, conduction of electrons in semiconductors and the band theory of solids and so on. I want to go on the algorithmic level like logic gates. And that's where there hasn't been much progress. But it's clear that there are several ways in which macala and pits and rosin blood and have you is a major oversimplification. Well, you, so I'll just pause here because you were, you kind of just transitioned from the messy details about how neurons work. There's their biological implementation, the implementation level, if you will, of Mars famous levels. And then you went to logic gates, which is good because during, while you were speaking, I was thinking about, there is the implementation stuff. But then there's the question about what neurons are doing, what their function is. And then, that's, and then you went to the logic and I was going to say, well, that macala pits, like, that they are logicians, that they, that they are producing binary logic signals. And that's kind of the macala pits approach ones and zeros originally. And that, so that was kind of the original functional story about what neurons are doing. And that's what you're saying has not advanced much. Exactly. Exactly. You know, what are the missing parts? Of course, we never really know until we get something that works. But generally, my view is that the real neurons are smarter than the macala pits, neurons. Okay. So sometimes I refer to a new conceptualization of neurons as the smart neuron. Okay. And, you know, in which ways. So one, one of the things that I think most biologists will, or physiologists will immediately agree with me is that, you know, macala pits neuron is an instantaneously responding device. So if I provide certain kind of inputs, they instantaneously compute an output by weighted summation and the non-linearity and output. Right. And we know, of course, in neuroscience that the neuron does not process inputs instantaneously. It has all kinds of timescales in its dynamics. And there are short-term, long-term memory effects. And you can characterize them in a variety of ways, such as measuring the linear temporal filter by sort of, you know, spike-triggered analysis. And this is, I think, very important because it's telling us that the neurons care not about correlations between its inputs. It's been different upstream neurons. But also in the temporal correlations, in the same inputs they can. And that's completely missing from the, you know, macala and pits inspired units. So even macala and pits in their original paper, you know, drew these loops, right? So they didn't just concept, sort of the classic story is like they conceptualized everything as a feed-forward network only. But they actually drew the loops and they alluded to how hard it would be to determine, you know, how to incorporate those recurrent loops within their models, right? So they knew it was a problem and knew it would have to be addressed. Yes. And that's what you're talking about is this historical recurrent context that neurons have to deal with. Right. So that was actually my second item is the existence of those loops, which actually, I agree with you that they realize that and you can look at the figures in their papers that, you know, that they realize they were there. But I think that's one of their geniuses that they ignored them, right? Oh, yeah. The network that, you know, that originated from them, we're ignoring this and that's why it was possible to make progress. That's interesting. I've never heard anyone celebrate that the, the ignoring it. That's, I like that. Well, I mean, that's how we do, right? In physics, when you build a model, your own models are wrong. Some are useful. Right. Yeah. Okay. Right. So that's one of the things. And let me just also bring in as we, as we discussed this, historically, there have been, speaking of function, there have been different conceptions about what the job of a neuron is to do, right? So and you write about this in your recent paper that I'll link to, you know, there's the efficient coding hypothesis. There's the idea that, you know, the grandmother neuron, that an individual neuron represents an individual thing in the world, you know, things like that. There's the predictive coding. So as we're talking, maybe you can situate the neuron as a controller within those contexts as well. Here's the, here's a different way to approach it. Like how did you come to the conception of the neuron as a controller? Like, you know, there must have been some train of thought leading up to that. Yes. So, um, I was basically very influenced by two things. One is I was, I'm a big fan of efficient coding theories. And I spent a lot of time thinking about those and predictive coding in particular. And I still think this is very important. But the problem with those approaches, as I'm sure has been brought up by other people, is that when you do efficient coding, um, it kind of works at the early sensory stages, because it makes sense that you want to represent the world, right? But and so those theories have been successful in explaining many properties of neurons, such as their, you know, temporal and spatial receptive fields of retinal neurons, even, you know, the edge detectors in V1. But it failed when people wanted to march further into the brain. Yeah. And the history of neuroscience is mostly dominated with sensory cortex. Exactly. And then of course, you know, the primary reason for that was experimental. It's an easy stimulus to control stimulus. And so, you know, you can do reproducible experiments. But in terms of theory, if, you know, efficient coding and predictive coding appeal to me as a physicist, you know, it's such a beautiful theoretical foundation that you can use, you know, information bottleneck. But then it kind of fails as you want to march deeper into the brain. And so if you start thinking why, of course, well, you want to get closer to action generation, to motor control, decision-making. So it's not just about efficient coding. So you have to have other things in mind in terms of the objective. And then from connectomics, of course, I knew that loops are everywhere, right? Yeah. But each neuron, even in salons, you know, each neuron, almost each neuron is belongs to multitude of loops. And it's output can back get back to its input by going through, you know, one, two, three synopsis. Was cybernetics any influence on you in this regard also? Or did you, was that something that came in later? Or because I'm getting at the control theory, theory aspect of it. Yeah. Yeah. Because that's all about agency and control and sort of motor output. What you're talking about, right? Yeah, probably was because I read some of this work when I was in school. But, you know, the issue is that, you know, if you have loops, then the dynamics changes completely, right? Because there is a danger of getting runaway excitation and instability. And so it's important to have a framework that pays a particular intention to that. In my mind, that's the domain of control theory. Okay. Right? So control theory on the one hand is a way to sort of generate certain desired output. On the other hand, it is a way to deal with loops. Deal with the feedback. Deal with feedback, right? That's correct. That's the important. So from those two considerations, it seems like a good framework to apply to the neuronal circuits. What is it that a neuron is trying to do in your conception? Every single neuron is trying to do this. Right. So, very good. So the neuron as a controller framework basically takes the efficient code and ideas as that further because the efficient code in would say, well, the neuron processes its inputs to represent them in efficient way encoded in its output. And that's sort of a transformation of information that's coming in. Exactly. And this is what deep learning is based off of, oh, I have a representation coming in. I'm going to transform it to pass it on to the next level or to some motor output or whatever. And that's really about the information transformation of what came before and what I'm. Exactly. And the predictive code in says takes it maybe a step further. It's not just a representation of the past inputs, but it's an attempt to maybe predict or encode information relevant to future inputs. Yeah. Right. And so again, you're trying to predict the future inputs and that's what you encode in your outputs. So that must have appealed to you quite a bit. Right. And so I spend a lot of time working on that. And I think there is some validity to those ideas. But then what does the feedback do and how do you leverage this to get to action generation? And that's where control theory comes in because then maybe neurons outputs. Don't just predict their inputs. But maybe they can influence their inputs. I mean, this is this directly, I'm going to ask you this about this eventually, but you know, active inference, right? Active sensing. And this is a direct analog of that, which is a more whole brain view of like what brains are doing that we move to get. We have a goal of what sensory input we want. And that's the action that we're taking is to get that sensory input. And this is like the predictive brain idea as well. But that's on the whole brain level. And so you're saying that every neuron is doing this. Yes. And I actually, so the reason I think it seemed like a good idea to me is because when I started out in neuroscience, I understood our idea of understanding how the brain work is to transform, it's how to, information gets transformed from its inputs to its outputs, right? And only later, you know, because of the work by other people, such as I don't know, Yudah, Hissar, and Paul Tisek, maybe others, that, you know, it's all feedback loop. It's all active sensing, right? That we do things that we, the actions, we perform actions, the effect of those we can observe. So what are brains for? For moving, some people say they're for moving, some people say they're for sensing, some people say they're for subjective awareness. Some, you know, is there, do you have like that? Do you have that control, theoretic perspective on whole brains is essentially what I'm asking. Right. Well, so, you know, for the whole brains, I mean, depends which level you want to ask it from the evolution perspective, right? You know, evolution maximizes the fitness, right? And of course, you know, you can play with the genome and improve the fitness by tinker with the genome. But eventually, what happens as has been argued by many people is that the genes are kind of operating a very slow timescale, okay? You're kind of in a physicist again, right? That genes only, you know, you know, whether you survived or not, and years have passed, right? But you want to have some feedback loops on a short time scale, okay? Right? And that's why the genes in the course of evolution invented the brains because, you know, they needed some kind of a stand-in to modulate feedback loops on a short time scale during the lifetime of each organism. And they're like, I'm too slow here, guys. Help me out. Help me out while I'm in the background, slowly changing. Okay. I think so. Yeah. Alright. So then let's bring it back, because we're going to kind of go up and down scales. Yes. So let's bring it back to the single neural level. So the conception is that every neuron is doing its own control process. So maybe describe that a little bit more. And then I know that there are problems with the sort of common way to approach control problems. The neuron has to do too much. And that's where this idea of what is it direct data-driven control comes in. So we'll get to there, but tell us more about what the neuron is doing and what it needs to do in order to accomplish it. So you know, it's easy to accept that the brain acts as a controller in the feedback loop with the environment. And it's certainly a good view. But from my perspective, again, as a physicist, I like to focus on simpler problem first. And the brain, as a whole, is very complex, even in salegas. So how far down should we go? And that I think is a personal choice, right? There is no universal level. But for me, the level of neuronal physiology has been always very appealing because there is just so much data. People have worked on those so much. And my postdoc in neuroscience, when I made a switch, was in the synoptic physiology lab of Chuck Stevens. And so this level is very appealing to me. So I thought, well, maybe on this level, I can think about this issues just as well. And it actually turns out that there is a lot of data that I can leverage. So I don't think there is anything special about the neuron being a controller. I think that if the whole brain can be viewed as a controller and neuron can be, there is a controller, their intermediate level of brain areas, nuclei that are controllers, of course. And this has been famously argued by people like Robinson and such, that the eye control system, this is well known. So there are controllers on multiple levels. And actually going the level below, I think that you can think of each synops being a controller. Okay. You're just wild about control. It's just everywhere. Yeah, I think evolution is wild about control. And I'm just wondering that that's the case. Okay. All right. Well, so then what does a neuron have to do? So it has this output. And then there's the loop. It goes to one neuron. It also sometimes goes back. Then it can go directly back on itself from that one neuron. It can also go to three neurons and then go back on itself. It can also eventually end up affecting motor behavior. And then it comes through the senses when we get a new sensation. So there's this hugely rich feedback signals that's coming into this single neuron. So how does it cope with that? And then what is the neurons objective function? What is its goal? What is it controlling? Yeah. So these are all good questions. And I don't have all the answers. And I should also say, you know, in just a full disclosure, the neuron as a controller is still a hypothesis. Okay. Although I think that there is some evidence that this could work, there is no smoke and gun experiment that would confirm that. Right? Why not? Because it hasn't been done. Oh, you don't mean it's impossible. You mean it just has been done? No, I think it's impossible. And I think there are ideas floating in the neuroscience community. And one of my favorite ones to test this idea of the neuron being a controller is the following. You want to somehow perturb the feedback loop. You want to cut the feedback loop and see the consequences of that. But how do you cut loop? Well, you could just silence the neuron and see what happens. But that is a little bit of, I think it's too much of a perturbation. And if you silence the neuron, it will know that something is off and it will kind of go crazy, even if it's just a feed forward device. Yeah. Okay. So you want to kind of break the feedback loop in a way that the neuron doesn't really know about. Okay. You want to nudge it. So you want, yeah, so you want to like tinker in the back there so that the neuron doesn't notice it. But, you know, it will see this, if there is feedback, if the neuron is listening to its own output through the feedback loop through the circuit, then it will start adapting itself. And so one of the great ideas, I think how to do that is just to silence not the neuron, but the synoptic transmission downstream. Right? So they're now amazing molecular genetic tools that allow you to just silence the output, synoptic output of one neuron in the circuit. Okay. So you're talking kind of, so you want to keep it relatively simple then because I immediately think of super complex, complex brains where the signal branches out almost infinitely, you know? Yeah. This is the classic problem recurrent neural networks also. It's like, how much of that, the influence of its output is actually going to come back to it. How can it possibly, uh, no, calculate some error correcting mechanism or some control signal based on such a deluded feedback signal? Exactly. So it's not a given that it's done, right? Like I think it's on the theoretical level, it's possible. But this kind of experiment would perhaps provide some evidence one way or another because if we were able to silence the output synopsis of one neuron in a circuit, then, okay, the spike generation of the neuron wouldn't know about that manipulation, right? So are you talking about, you have your, your, uh, the neuron of interest and then let's say it branches to three neurons and then those neurons branch to three other and then it comes back or something, kind of a fairly simple circuit. Are you saying you want to silence the activity of one of its targets of the three, you know, and then it, and then see how that affects it or the output of the, the neuron that you're studying? Yeah, you can do either way. Um, I think more sort of precise manipulation and, and more subtle manipulation would be just to silence the outgoing synopsis of that one neuron. Okay. Okay. So the monitor, how its response properties are changed over time because then it's, it's not affecting anything downstream. Right. Right. Okay. So something like maybe a hippocomple pyramidal neuron, right, that has a place cell receptive field and your silence, its outputs is, it's going, is, are we going to see changes in its response? Is it place field going to go wild and start searching for green of pastures, right? So subtle. Do we have a sensitive enough techniques to measure? I mean, I could see a scenario given that the generacy of brains where it's, we don't detect anything, but perhaps there is something going on. Well, the result of a possibility of a negative result. I mean, that's the nature of experimental research, right? But if you were to see unexpected changes in receptive fields of that neuron on certain time scales, then we would say, aha, and then of course we can rescue this synoptic transmission and see the receptive field restored or become more sane, right? Or change in a different way. We might not have a different way. Right. So, I think, I think it's important to have an experiment where you actually introduce a perturbation. Okay. I guess you would, would you do this in a dish? Would you do this in vitro or? I one could, but why not just? Just one in a mouse with, yeah, or, yeah, some organism with, or, or see elegance perhaps, yeah. Yeah, yeah. I mean, it's the many options. But okay. So, so then conceptually, sort of the common conception and the thing that you address with this direct data driven control is you take the burden off of the neuron from modeling everything that's going on in that external loop and then and simplify things. So how does directed data driven control simplify the neuron's task? Right. So, the way I thought about this is, you know, if it's so natural, it is for me, it's natural to think about the neuron as a controller. You know, why hasn't anyone said that, you know, there are a lot of smart people who, you know, thought about neurons before me. So what's going on? And you know, knowing how control theory is taught, it's actually not that surprising because most of traditional control theory is model based, which means that you start by describing the environment or the plant as they call it in control theory with some kind of dynamical system description, right? State space model. We assume that this model is known and then we add feedback control to change its properties. For example, that model could be unstable. That would be bad news in any real world application. So we add a stabilizing feedback. Gotcha. So that's usually negative feedback case. And so then you compute using one of the methods of control theory, what would be the right feedback dynamics that makes an unstable system into a stable? And to think that a single neuron could do this iteration, which actually for linear systems has a closed form solution, but it involves really complicated matrix multiplications and inversions. And how can a neuron do that? I mean, I don't even think neuron could represent the sort of this state model of a dynamical system like that. So that I think is why people never thought of a neuron as a controller. Just asking it too much. That's right. That's too much work to lay on a neuron. But what I realized, and that's a relatively new development, that there is now another way to do control theory, which is called data driven control, which is very much in the spirit of, I would say, data science and machine learning. Oh, yeah. Listen to the data. Yeah. Yeah. And you go directly from the observations and map them onto control signals without going through constructing the model. And of course, you have to learn that mapping somehow. But again, that mapping is learned based on the prior history of the paired observations and control signals. So you have all of the data incoming. The cells job is to map that incoming data onto its set desired reference signal, right? The control signal. And then the control signals, the neurons job is to change the control signal to produce outputs that better map, sorry, to produce eventual re-inputs that better map the inputs to the control signal. Yeah, I would slightly rephrase it by saying, please, that the neuron produces the control signals based on its inputs that drive the environment towards a desired state. Oh, okay. Good. So it's the neurons job to change the environment so that it's getting the right input. And it knows it's changed the environment in a certain way based on the inputs it's receiving from its activity. Exactly. How does it? I'm just going to jump to this and I'm not sure there's an answer. So I had Henry Yinnon and one of his big things is that the control theory perspective in brains in general has it wrong in neuroscience because the reference signal is always like outside the brain. So how does the neuron get that objective? How does it get its reference signal? Where does that come from? Is that from the DNA? How does it know what it wants to hear? Yeah. So that's a very good question. And I'm afraid I don't have full answers for that. And I think this is really important questions. So at the moment, we have two examples where I think we understand partially how we can do it. And one example is already mentioned the issue of stability. So once you have a feedback loop, it has to be stabilized. It cannot be unstable. Otherwise, it blows up. And much of control theory is about making an unstable plan stable. So just this kind of desiderotum already produces some predictions that I think neurons may have to respect. Is this where, because there are three or four different sets of experimental data that this approach explains the results of which. Is that what you're referring to? Yes. Yes. So anyway, stability is a must. And any feedback system must do that. And so we can already generate some predictions. Of course, that's not the only goal because it would be born in, right? If it just wants its stability, it will just lie down and die. Right? That's very stable. But that's not the first only thing we've got to do where the brains are delegated to do by the genes, right? Whatever that is. And so that needs to be figured out. We were talking about the feedback loops and most loops and open loops. And the interesting thing about a spike in neuron is that most of the time, it's an open loop because if the neuron does not produce an action potential, there is no releases in the synopsis, well, if you ignore the mean events, right? And then basically it's an open loop. So whatever input the neuron gets is just held there until it's a closed loop. You mean it's a closed loop because it's an open loop because nothing comes out of the neuron until there is an action potential. Right. That's where the loop is broken. That's where the loop is closed and therefore open, right? Well, okay. Wait, I'm thinking of a wiring circuit. When you close a switch, then it's an open loop, right? Yes, okay. So, all right. Okay. So, but the control theory is what say a closed loop? Oh, okay. I'm sorry. My mistake, I just determined it. No, no, no. It's good to clarify those things because sometimes you know colloquial meanings that they collide with. Oh, so if the switch is open, the circuit can't run. That's right. So when the neuron is silent, right? There is no action potential. The loop is open, right? Okay. That's what I've, I mean, I even know that and I just switched them. I'm sorry. Yeah. Yeah. No, no, no. I, it's good to clarify. And so, but at that one millisecond, the extent of the action potential, the loop gets closed. Closed. And that's when you have the feedback. Okay. That's a spike in neuron. It's actually very interesting that it goes between those two states because you know, if I think about the neuron as a data driven controller, it actually has to solve two tasks. It has to solve a control task, whatever the desired amount of is, but it also has to solve the system's identification task, which is how in the traditional control, the model of the plant is can degenerate, right? So, what is that task? Is that task to say when is it open, when is it closed and when to? So, in the traditional control theory, you know, we start with the model of the plant and then we figure out how to get it to the state we wanted to be in. But then someone may ask, well, what if the, you don't know what the right model of the plant is, which is like what happens for neuron. Of course, it's, it's not reasonable to expect that the genes program the neuron in exactly, you know, the right way, how the environment is because it may actually be changing over time and the neuron has to adapt, right? And so basically, the control theory would say, wait, but that's a different problem, right? Like, it's not my department, right? To get the model, you have to use another subfield, which is called systems identification, okay? We use prior observations, perhaps along with the record of control signals that accompany them, to build a model. So in the data driven controller, performs both of the tasks at once without constructing the model explicit. I see. Okay. Okay. So basically, the, the issue why spike in neuron is such a great idea, I think, is that it's hard to do systems identification in closed loop. Oh, okay. I see. So it's almost like, so I'm just going to restate this in a very layman's way, I suppose. So it's almost like it doesn't want to open it. Sorry, it doesn't want to close the loop much because closing the loop makes it more difficult to adjust its control signal. Exactly. Okay. So it's like, think about it as a radar that has to shut down its amplifiers, the moment that it generates the pulse. Yeah, or like when I'm yelling, I can't hear someone talking to me. Exactly. Yeah. Exactly. And there we go. I got the layman's there a little bit more. All right. So talk just a little bit about how this approach accounts for some of the experimental findings and neuroscience that have been accounted for individually in various ways. But then there's kind of a collection of things that you've been, you've been searching for that now you say that this approach accounts for. Right. So, so there are several, there are several things that we think fit the control theory view. None of them is of course a proof. I am in general, you know, in terms of scientific methodology, it's usually impossible to prove that the theory is correct. You can only falsify it, right? I want to ask you about that next though, like how you would falsify it. But yeah. Right. So, but there is some evidence. So I think that that kind of supports this view. So it's already cited, of course, the ideas of action generation and the existence of feedback loops in the brain as, you know, supporting views. But I think that what was interesting is that we were able to derive learning rules from the idea of establishing controller that can potentially map on something like spider timing dependentless. Does it? Is this something where you thought, okay. So here's the conceptual idea. Now I need to find some data that supports that idea and here are some open questions. Or how did you stumble upon, you know, applying this to spider timing dependent plasticity? As a, I actually did not know beforehand that this would account for spider timing dependent plasticity. But this is one of the neuroscience facts that I'm obsessed about because it seems so counterintuitive. Of course, you know, maybe just to sort of as a matter of given backgrounds. Spider timing dependent plasticity is a learning rule where the synoptic strength changes depending on the relative timing of the spikes of the post synoptic and the pricynoptic neuro. Yeah. And so if the pricynoptic spike precedes the post synoptic spike, then the synops gets stronger. Proceeds it by a very short time period. Yeah. Exactly. And that's key, right? It's a very, very time sensitive mechanism. But one can say, well, that this part, the sort of the potential window of STDP is actually maybe viewed as an extension of the half post to it, right? If one neuron repeatedly causes the spike in another neuron, then the synops gets strong, right? And makes sense. It kind of represents some kind of a causal interaction. But there is another side to STDP, which is the depression window, which is if the pricynoptic spike follows the post synoptic spike in time, very small time delay, as you said, then the synops gets weaker. And this part is very difficult to see. Certainly doesn't follow from HAB because, of course, people would say, yes, you can just facilitate the synops. There has to be some kind of normalization, right? But why does that normalization has to be so time-sensitive? Right? There is no reason for that. It is just an issue of stability. There is no reason. So I'm really obsessed with this observation because it has an appearance of an anticausal interaction, right? That the pricynoptic spike that has no way of influencing that post synoptic spike has already happened affects the synops, which is a causal mechanism. So why would that be? And in a control theory view, because you have a feedback loop, it offers an immediate resolution that, well, because there is a feedback loop, this interaction that seems anticausal can actually be viewed as causal. You just have to traverse the loop outside of the neuron, right? You go from the post synoptic neuron back to the pricynoptic side along that loop, and that is, of course, a perfectly valid causal interaction. How does the neuron know what time window to pay attention to? Right? So it has this spike timing component, and then it's going to say, well, if I get this, if I get this feedback signal at this time, it means I need to adjust in a certain way if, but then three minutes, or not three minutes, you know, but a hundred milliseconds after that, I need to adjust in a different way. And this is how I know, I mean, I guess that's where the data driven approach solves that problem. Well, so you're not asking mechanistically how does a synops keep track of the spike? No, no, no, no, but I'm, there's the problem of the time scale window of like, well, when do I pay attention to in order to adjust my signal? Right. And that is, of course, crucial, and that comes to the issue that you're ready brought up, what the goal of that neuron is. And if you assume that the neuron has to care about the stability of very short loops, basically like one or two synopsis to traverse the whole loop, then the time scales are or four or a milliseconds, which is close to the spike time independent elasticity time scale. Right? Of course, you know, there are other loops there, both through the brain, but also going through the environment that take longer to traverse. Right. Then the prediction would be that if the neuron cared about the spike alignment based on the traversing of those loops, then the window of spike time independent elasticity would be much longer. And amazingly, there are cases when this has been observed physiologically. So there are examples, one is in the cerebellum. It's the work of Jennifer Raymond, where you know, they see the STDP window, which is like around 100 milliseconds to eight. And then another example is in the hippocampus, whether I don't remember the exact, but it's tens of milliseconds window. And so, you know, the control theory view would be that then maybe there is a loop feedback loop that takes 100 milliseconds to traverse that this tunes the synops. So we talked about a handful of things that the direct data driven control feedback, feedback control accounts for. Another one that I enjoyed is that this approach requires stochasticity or variability in the firing rates to sample the space, essentially, to carve out, I don't know if manifold is the right word, but to carve out a plane in the dimension. So it knows where it can go in the dimension. Maybe you can simplify that. That was very abstract what I just said. Yeah, absolutely. So this hard back to the old exploration exploitation trade off that, you know, feels like reinforcement learning I've built on. But here it has a very simple mathematical formulation, which is basically if the dynamics of the environment or the plant is linear, then when the control law is settled and is fixed also linear usually. So the control signal is linear function of its inputs, but it picks linear function of its inputs. And the whole system, the whole feedback loop, ceases to explore the dynamical state space. And that removes its possible readjusting within that state space. Exactly. And this has been realized by control theorists and the foundational development that allowed the development of data driven control is avoiding this problem by maintaining what they call persistency of excitation, which is having the control sufficiently diverse so that it can still probe the environment. And this goes back to what we already mentioned, which is that the data driven controller is called the controller, but it also has to solve the system's identification problem at the same time. And by converging on a fixed control law, you give up the ability to solve the system's identification problem. So in this conception, is that why we have variability in spiking? So there's lots of different theories on stochasticity, quantum and determinacy, you know, sampling, exploration versus exploitation, like you mentioned. But in this perspective, it's a desired engineering principle, essentially, that you'd want to build in. That's right. So that sort of tells you that for this data driven controller to operate, there has to be some source of variability. Yeah. So noise is classically an engineer's nightmare, right? So it's something that you want to avoid. But in this case, you want to build it in. Exactly. And then I think is a natural fit to biology because everything in the brain is very noise almost everything, right? Whichever level you use, you know, synoptic physiology tells us that the probability of synoptic transmission per, um, pricinoptic spike is low, you know, it could be 10% right? Yeah. And then you know, the spike generation, well, in theory, it could be very precise, but that is very strong with driven by the exact inputs. We talked about the history of neuroscience, you know, and the history of concepts, conceiving of single neurons back to macolic impets, right? And even before that. And then, and then you had with Barlow and folks like that, the, um, the neuron doctrine, which among other things, you know, gave a lot of import into the function of a single neuron. I mentioned grandmother cells, right? Or Jennifer Aniston cells, right? So every single neuron is representing something that's in a nutshell, one version of the neuron doctrine. These days, with the advent of recording lots and lots of neurons at one time, even more than 10, like you alluded to earlier, um, now we're all in, now when we're the population, we're in the population doctrine era where we, a lot of people think that abstracting to the, at the single neuron level is, is a level to low. And we need to think of everything in terms of population of activity and the dynamics that those population give rise to. And what you said earlier would make me think that you're okay with that because you see every scale from single neuron up to whole brains in the control theoretic perspective. Um, but a, but the population doctrine folks might say, well, we don't need to worry too much about the function of single neurons because they are cogs in this larger machine. Oh, sorry about that analogy, but, um, that we don't need to worry about their, their implementation of how they're doing things and why they're doing it in that way because we just need to pay attention to the population. So where, where do you sit in that because this is very much at the single neuron level? Yeah. Um, I mean, I agree with everything you said, um, you know, it's important to understand how the population of neurons function, but I don't see that, um, as, uh, necessarily very distinct from what a single neuron does. Um, you know, as, as I said, you can think of each sun up, being a controller and somehow together they combine into a neuron and doing something useful. And so the reason I'm focused on the neuron as a controller is just because there's a lot of data and I know how to conceptualize it at this point. I see. I know what the inputs, outputs are and how to describe that and so on. So that's why I'm doing it. But again, you know, brain, you can think of a brain nucleus or cortical area or, you know, brainstem part that being a controller made out of multiple neurons. So I think the similar methods would work on different levels. Um, but the reason milling dollar question here that I think you're alluding to is how to go between levels. Oh, God. Yeah. I mean, that's the dream. That's the dream. And of course, I don't have an answer here. Uh, I think this is probably the most fascinating question in neuroscience because, you know, I already brought up, you know, intersection of physics, engineering, biology. And here we get, I think, an intersection with things like gain theory and economics and mechanism design where, you know, you have independently operating agents. Oh, you said agents, we're going to move on to that in a second. Go ahead. Yes. Yes. Right. And so why do they have to do neurons have to be agents? Well, because there is no central authority that tells each neuron what to do. Right. The brain is decentralized. Um, yet. Uh, so so they have to pursue kind of their own objective, their own desire, but they have to work together towards a common objective because in the end, the brain has to produce one behavior. So they're, they're individual agents every single neuron and yet and they're in a symbiotic, uh, um, agreement with all the other agents. I mean, do you think of it that way or do you think that an individual neuron is just selfishly doing its own thing, surviving? And then it happens through development, evolution that they've come together and, and our own whole person agency is an emergent property of those individual components doing their own thing. Is that how you can see that? Yeah. That's my way of thinking about it. I mean, it's, you know, I think about the analogy is, you know, market economy, right? You do have a bunch of agents that pursue their own, um, uh, objectives. Yet, if you set up the system correctly and that's where mechanism design comes in, you have to have some kind of rules by which even though each individual pursues its own objective, they act, um, as you said, symbiotically together to generate some common good towards the treatment of so common goal. This is kind of a silly question, but has this changed your perspective on, um, sort of the, your respect for the single, you know, single cells, right? Because they're each individual alive components. And, um, so we tend to think of the brain as being composed of these things that are doing things in the service of us as a whole person, right? But, but, um, this way of viewing it almost gives a little more reverence for the individual neuron. Well, yes, it does, but, you know, as all, well, I usually multi-cell our organisms, um, you know, deep inside each cell, there is the same DNA, right? Mostly, mostly the same. Mostly, mostly the same, yes. So, um, so there is a shared kind of rulebook, oh brothers and sisters around me. Exactly. They have to follow, right? And so, yes, the genes again cannot control each, elementaries, each spike that the neuron makes. But the genes write the rules by which each neuron, uh, generates its spike. Mithy, what is a computer scientist working on AI? Need to pay attention to any of this? Well, I mean, they don't have to write, as you said, you know, if you measure in, in billions of dollars, right, they're doing really well. So, um, you know, it's a tough cell, but, but I think that the question is, you know, where do we want to get to eventually? And if the goal is to achieve a GI, as we mentioned before, and I define a GI as equal in or exceed human intelligence, what the hell does that mean? Whatever that means. Okay. But, um, but, but my argument is that, you know, maybe there are various paths to a GI, right? Um, but they're, you know, taking the brain inspired approach is the only path for which we have the existence proof. Right. Could be a better way of doing it. Maybe I don't know, but it may also be a dead end. Yeah. Right. I mean, when, when they talk about building data centers closed to, you know, a nuclear power station because of the energy demands, and a human brain operates just fine, consuming 20 watts, then you start asking questions. You know, is this really the best path? Do you see some, what do you hung up on? Are there limits or obstacles in your way? Are there limits to the control theoretic perspective of neurons and or, you know, what is, what are you spending all your time thinking about these days? Um, so the big question for us, you know, aside from like, how do you combine many controllers together to do something useful that we're ready to do? You know, to just put them together and given their, their, uh, objectives, it should be an emergent property, right? Exactly. But we don't know what the objectives really are. Okay. That's a big, yeah. And that is real, real, um, uh, difficulty for us. And we are, of course, trying to find some objectives to work with. Um, but while we're searching for objectives, we realize that there is a way to skirt this problem a little bit by working on the sensory periphery again, or, you know, early sensory processing, which is what, you know, efficient coding people got so much mileage out of, because, you know, if you think about a retinal neuron, like a retinal ganglion cell, it's not very useful to think about it as a controller. I mean, on some level, it is a controller, right? Because it tries to control the downstream part of the brain to perform certain actions. But the question is, like, does it get the feedback through this whole big feedback loop, including the rest of the brain? And, um, you know, and, and getting back the visual signal. And that's, that's the issue because there is a big time delay. And so can you even do credit assignment over this time scale? So that's, that certainly, um, you know, is a question there. Um, so, but amazingly, the control perspective helps in this case where there is no obvious sort of plan to control, because we think that you can think of a retinal ganglion cell is, as being fooled into thinking. Okay. That it actually controls its inputs. Oh, my God. Okay. Talk about anthropomorphization. Yeah. Exactly. Exactly. And so, and that is actually led us to, to very nice, uh, re-examination of the predictive coding and the effective coding kind of framework, uh, which, um, again, because of the control view places much more emphasis on the dynamics. And with you, those sensory neurons as analyzers of the dynamics of the external world. Mm-hmm. And that kind of changes the perspective for us. So even though it's not control per se, but it's a very controlled inspired view of what the neuron does. Okay. Okay. So this conception of single neurons as controllers is itself variable across different brain areas and depending on the needed functions, cognitively of different brain areas and neurons. Yes. Mm-hmm. Yeah. So, that is, of course, different, like if you think about a motor neuron, you know, it's very clear that it's very heavily, you know, sort of, um, influential in the control of that. Yes. It controls immediately, you know, some muscle and may get a feedback on its performance, through proper sensory mechanisms or whatever. And as you march back towards the sensory system, okay, it's much more about, um, about analyzing the inputs, you know, much more heavier involved in the system's identification side of the data driven control, and less in terms of like controlling things. And then I think view is very nicely aligned with the recent experiments that where people see motor signals in the primary sensory areas, um, you know, like in V1. And so, you know, that is aligned with the perspective from control theory that the brain kind of performs a transformation from the sensory inputs slowly into the control dimension. Hmm. I see. Um, so this has been a lot of fun for me. Is there, is there anything that we didn't cover that, um, that, uh, you, you wanted to discuss that we haven't discussed? Let's see. I, I think this, this is, this is a lot. Yeah. That's what I hear all the time about my podcast. It's a lot. Yeah. Well, I, I should also say actually, you know, since you asked about, um, being a lone practitioner, um, it is true that, you know, to my knowledge, we are the only ones who suggested the neuron as a controller, but that's crazy. There are multiple people now who think about control theory approaches in the brain. Hmm. So in that level, there is a small community of people, uh, to the extent that we are having a two day workshop at cosine 2020. Oh, nice. Oh man. Oh man. Devoted to the dynamics and control theory, uh, applications in the brain. I don't remember the exact title, but that's what, you know, we will spend two days arguing about. Oh, cool. But what is maybe the last thing I'll ask you is, um, given that it is a small community, what do people maybe even in the neuroscience community, when you're describing to them, what you're interested in or how you think about these things, what, what is the most difficult for them to understand? What do people get hung up on? I mean, I, I think that it depends what subfield of neuroscience the person comes from. Because the motor control people, of course, of course, the neuron does. Yeah. It's all controlled. Yeah. Then people, you know, who's like me brain, like hippocompos, maybe you could see that if there are loops, yeah. But then the sort of the sensory neuroscientist, why are you talking about? Yeah. Okay. So, you know, so you have to fool the neuron into thinking it's a smaller, right? That is, you know, that is a lot to take on. All right. Meet you. Well, I guess we've made, perhaps we've fooled billions and billions of neurons through this conversation. We'll see. But thank you so much and can continue to success in this line of work. Thanks, Paul, for having me. It was a pleasure. Brain inspired is powered by the transmitter, an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advanced research. Visit thetransmitter.org to explore the latest neuroscience news and perspectives written by journalists and scientists. If you value brain inspired, support it through Patreon to access full-length episodes, join our Discord community, and even influence why invite to the podcast. Go to braininspired.co to learn more. The music you're hearing is little wing, performed by Kyle Dunnevin. Thank you for your support. See you next time.
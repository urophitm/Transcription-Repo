This is Brain Inspired. Hey everyone, it's Paul, welcome to Brain Inspired. Recently I was invited to moderate a panel at the annual Computational and Systems Neuroscience conference or cosine. This year was the 20th anniversary of cosine and we were in Lisbon, Portugal, Portugal. The panel's goal was to discuss the relationship between neuroscience and AI. The panelists were Tony Zedor, Alex Pouje, Blaise Aguera, E. Arcaith, Kim Stackenfeld, Jonathan Pillow and Eva Dyer. So it was quite a big panel and I will let them introduce themselves soon in the beginning of the discussion. Two of the panelists, Tony and Alex co-founded cosine, those 20 years ago and they continue to have different views about the neuro-AI relationship. Tony's been on the podcast before and will return soon and I'll also have Kim Stackenfeld on in a couple episodes. So I think this was a fun discussion and I hope you enjoy it. There's plenty of back and forth, a wide range of opinions and some criticism from one of the audience questioners so that was fun. And this is an edited audio version of the panel. I wanted to remove some of the long dead space and things like that. But you can watch the panel on YouTube or in the show notes at braininspired.co, slash podcast, slash 187. So there's about 30 minutes of just panel and then the panel starts fielding questions from the audience. Next again to the cosine folks for inviting me and letting me bring this to you. Also at cosine it was a lot of fun for me to meet and chat with a bunch of you. So thanks for saying hi to me in the poster hallways or in the social gatherings, etc. Okay, enjoy. I think what we should do is start with our moderator, Paul Middlebrooks, who is running on the stage. Give him a round of applause. Thank you for having me here. So the way this will work, I would encourage anyone who wants to ask questions, you can come up and ask questions any time. It's going to be a very loose discussion. And I am going to force the panelists to introduce themselves in under 30 seconds each please. And if you can relate maybe your most embarrassing or poorest idea that you've had throughout your career. Tony, I know you have one. You were telling me one earlier. What is the assignment? Because in public I've never actually been wrong. But people in my lab know that it's not possible for an oracle. So what was the assignment exactly? Your thesis. My thesis. Right. Oh, okay. Yeah, I can tell you. So, all right. My name is, I forget it. I think it's Tony Zador. And I've been interested in the intersection of neuroscience and AI since actually I was a graduate student. And my graduate thesis was looking around for the things that were missing in artificial neural networks that we could take from biological neural networks and move to artificial neural networks. And the one I went with for my graduate work was dendritic computation. And although I had a lot of fun working on dendritic computation, I will confess that my conclusion was that that was not the key missing ingredient. Although there's obviously much that one could take from dendritic computation, that wasn't the thing that was limiting us back when I was a graduate student. So that could be an example of me being wrong. Wait, wait over 30 seconds. Tony. Another way in which I was wrong because I thought I was like in 26 seconds. My name is Eva Dyer. I'm a associate professor at Georgia Tech. I guess in terms of what I've done wrong. Yeah, maybe. So in terms of my background or kind of transition between AI and neuroscience, I actually started out in AI, but at that time it was called machine learning. And now it's kind of taken on a life of its own. So yeah, coming from machine learning and getting really excited about questions related to natural intelligence. And now I'm kind of coming a little bit around to the idea that we can actually use the brain to inspire AI. So kind of back and forth between the two spaces. So. Wow, okay. Hi, I'm Kim Stack and Feld. I'm a research scientist at DeepMind, also affiliate faculty at Columbia, do some neuroscience things, some AI things. I this probably isn't the dumbest thing I've ever done because that's got to be a really high bar. But I did originally when I was picking a major, I was really interested in the brain. And I was like brains are made of chemicals. So I'll study chemical engineering. And I really didn't examine that until like senior year when I realized that most chemical engineers like design distillation columns and the brain is not a distillation column. And then I did a hard pivot to neuroscience. So you know, it was fun, but it was probably not the shortest path to what I wanted. Alex Pugius, you've already heard from me way too much probably. I got into neuroscience actually from biology, completely different perspectives, slowly drifted into neuroscience, became a failed experimentalist. So I had no choice if I wanted to stay in the field, I had to become a theoretician, which I did during my grad studies and grew up, I'm part of this generation that really grew up at NIRRIPS and got immediately exposed to all the theoretical ideas that emerged in the field. And it's very relevant to debate today because that's been a big question that we've been debated for 30 years. Who is contributing to who? Is it machine learning that's helping neuroscience or is it in the other directions? Hi, I'm Jonathan Pillow. I am a professor at Princeton University in Computational Neuroscience. So my group focuses on statistical models of neural data. I don't actually work on neural AI, so I'm not sure what I'm doing on this panel. I actually misread the invitation. I thought it was for a workshop and I said yes and it was only yesterday on the plane that I realized, okay. So I don't know. I don't work on AI. I'm a consumer of AI and I'm eager for it to get a lot better so that it can do something like answer our emails for us. But I'm excited about the possibility of the deep learning revolution for understanding the brain. I see a lot of really creative applications of ideas from AI or machine learning to untangle the computations that are going on in the brain. So that's my main emphasis. And please. Hi. I'm Blaise Agueradkas. I am a VP at Google Research and the CTO of Technology and Society at Google. I don't know what that means exactly, but I'm trying to figure it out at the moment. I feel like I've made so many mistakes scientifically and so on. It's really hard to narrow this one down. So I don't know. But I guess I will just say that along with many people, kind of at the intersections of AI and neuroscience, I thought that there had to be some kind of magical special sauce to intelligence. And I think that that is actually my biggest mistake, but I've seen the light now, except most people haven't. So this is kind of what we have a backward one. Okay. Well, some of you already touched on this. One of the things that we're going to discuss, although it's going to be very open and we can go down to any of what we want, is the interaction between neuroscience and AI who influences whom. And I would just love to get your perspectives on this. I know some of your perspectives and that there's some disagreement between some of you. For example, Tony believes that neuroscience has historically and continues to influence AI. And I mentioned to him earlier that I think that neuroscientists desperately want AI to be influenced by neuroscience. So I'd like to get your perspectives. And I don't know Jonathan, if you have a particular perspective on this, since you're uninvited now to the group. I don't have a dog in this fight. I believe Young LeCoon's tweets when he says, you know, the organization of V1, Hubele and V1, inspired him to think about convolutional neural networks. So I think a lot of the ideas that went into the design of deep learning came from neuroscience although I'm not part of that movement. So I take people at their word. I'll just say, yeah, I think I personally am excited by Tony's vision that there are still things that I could learn from neuroscience. I don't really know though. So can you pivoted from chemical engineering to neuroscience? And now you're doing a lot of non-neuroscientific machine learning work. I mean, do you, from your perspective, do you agree with me that neuroscience is sort of a desperate attention seeker in this? I mean, I might be one, but I think the, so I mean when I'm thinking, I do some stuff that's just like pure AI methods stuff. So when I work on that kind of work, I've trained as a neuroscientist a lot of the batch of ideas that I draw from are neuroscientific ideas. I have personally found that thinking of both the brain and AI in the same brain is a useful thing to do that like they're just kind of interested in different aspects of the problem and thinking about things in different ways. And so I have found there to be back and forth. It's kind of a weak statement because it's not saying like this is, you know, anybody has some background that they use as a batch of inspiration for their ideas. Where I am right now at Google, at DeepMind, there definitely is a mood right now that's, you know, empirically got some backing of like, let's scale it up. Let's like, let's, we've got some good methods. Let's like pause on the science bit for now and like just keep making the bigger and see where we plateau. So that's not only really like neuros, not neuroscience inspired. It's just like more, not super science. It's more like engineering right now. Anyway, so I have personally found myself more excited about using AI for studying the brain because we have this new batch of models. They're super cool. They can do a lot of need. They really need tools and they can prize a new model class that should let us answer different conceptual questions. So that's where I've kind of felt like there's low hanging fruit right now in the environment I am. But I, you know, I historically felt very strongly about neuroscience inspired AI. So it, I don't really feel like I have a super consistent. This fundamentally is or is not in the long term going to be part of the process. Do you think it's a healthy relationship? Yeah, or do we need counseling as Tony said? In a sense, no, I think one thing that is, that could be changed is like neuroscientists definitely don't need AI to justify neuroscience. So I think sometimes that is a little bit of a tone like the reason this is an interesting problem is because AI can't do it. And that doesn't need to be the justification for a neuroscience project. Like understanding the brain is its own imperative. So if validation from AI is the main reason to do neuroscience, I would say that's not healthy and sometimes, you know, it's easy to want that relevance or validation. But you know, I don't think most people who study the brain would say that that's their only reason. So, Eva, you said that you've only come recently to appreciate that there are brains that could be useful for building better AI. I'm not sure how you phrased it. But that's an interesting perspective from me because I kind of think the opposite. So how did you come to that perspective? I mean, I think there's a lot of enthusiasm in taking inspiration from the brain to build more generalizable or robust systems. In general, I think it was like the scale at which we're going to derive that inspiration. And I think perhaps there has been a lot of excitement about maybe like synaptic learning rules or understanding how, you know, neural systems could, you know, implement kind of a more biologically plausible learning rule or how we can look at like circuits or kind of more like mechanistic implementations in the brain and then port them over to AI systems. And I think, you know, to Kim's point, while some of those, you know, tools have actually given us, you know, some insights into kind of connections between the two. I think that AI as a field has like moved in a very different direction. I think in terms of, you know, using transformers or different architectures that no longer look like kind of nearly inspired units any longer. And so I think, you know, it's kind of interesting because now it seems like the way that AI scientists need neuroscientists is coming more from this study of like complex systems, right? So now we have really complex transformers. They have all kinds of, you know, information within them, but we still don't know like, how are they solving these different rules? And so I think it's interesting to see now how neuroscientists and neuroscientists, like perspectives are coming into just like probing black box systems. And so yeah, maybe we, neuroscience is helpful for AI, but it might not be at the scale or kind of mechanistic level that we might have thought would be helpful before. So at the same time, and I'm not a panelist, so I'm not really in a position to disagree. You know, it's like you're a 10 years ago, the brain was a convolutional neural network. 20 years ago, it was a bolt-in machine. Now it's a transformer, pardon? 100 years ago as a watch. Okay. Or as a dementia. Of course, dementia. So my question is, I mean, in a sense, this is the problem of looking under the lamp post, right, to find your keys where the light is. And every new model that comes out, neuroscientists seem to take on that model and say, oh, maybe the brain is that or does it that way in certain respects. And I'm wondering if any of you think that neuroscientists are too influenced by what's happening in artificial intelligence, Jonathan, you're nodding your head. I mean, I do think there's a trend, right? You also see quantum physics or something we don't understand. Maybe the brain uses quantum physics. So there's a tendency, I think, to grasp for whatever complex technology, the steam engine, the telegraph, right? You look at the history of brain metaphors and it often has been that people compare brains to whatever complex technology we don't understand. But I do think it's a fundamental difference in that we're making quantitative predictions. I mean, so people tried, you know, based on the tale of human-inviesel's work to build models, you know, that was Mars whole vision program, not to bash on Mars. We're going to figure out how vision works by constructing a series of computations. And that largely failed. And now we have, you know, we have, it's not just a metaphor, in other words, I would say. So the fact that we can make quantitative, we can discover what neurons in V4 or IT or in language brain areas are doing, using these complex models that were trained on vast amounts of data, I think says something incredibly cool. It's different than just comparing it to technology we don't understand. I feel like I'm much more bullish on this relationship than many of the rest of you. I mean, the moment when they diverged, when neuroscience and computer science diverged, was kind of after McCollough and Pitts in 1943. So, you know, that was, you know, when we thought that the brain was a bunch of logic gates and computers were also a bunch of logic gates and, you know, and that was a point of convergence. And then things diverged. And AI sucked for many, many years because it was not connected closely. Or at least the mainstream of AI was not connected closely with neuroscience. And it only began to make progress again when they reconnected. And I think this is a general theme, you know, like good ideas come at intersections. Both AI people and neuroscientists are testing rigorous hypotheses. Nuroscientists test them by seeing if the brain does something like the theory. And AI people test them by seeing whether something built that way can work. And the two have consistently informed each other at every turn. You know, the transformer is the first architecture that seems superficially like it is not informed by specific, you know, very specifically by, you know, by a neural architecture. And I'm not even sure if we look at it in retrospect in a few years if we're going to find that to be the case. I think that multiplicative interaction, that thing that is at the key of transformers, you know, we're now looking and I think we're likely to find things in the brain that, you know, that do actually do that. And that dialogue is likely to continue. I think that the issue is that, you know, AI is now, you know, whereas AI was in the dog house, you know, for so many years in neuroscience, it was making real progress. You know, now, you know, AI is finally coming into a tone and making real progress. And there are status issues between the two fields. So I think it has more to do with that than with the actual intellectual exchanges. And I completely agree on just to amplify that, right? It used to be, when I was in grad school, the term AI was reserved for symbolic AI. And I think that's what you mean when you say that. Like they didn't start like AI as we now use the term, didn't really start to work until we jettisoned the version of AI that had nothing to do with neuroscience and replaced it with what we call neural networks slash machine learning, right? You could go back to calling it cybernetics. Yeah. So like just historically, right? The major advances that gave rise just to remind those in the audience who don't know the history, right? The major advances that gave rise to modern neural networks were all inspired by neuroscience. So, you know, the very notion of a neural network, it still retains the name, the idea that synapses are the locus of plasticity. Those are the free parameters, right? The convolutional neural networks, which were explicitly inspired by models of how the visual system processes information insights. Yeah, human weasel insights from, you know, the basis of reinforcement learning came from neuroscience broadly construed psychology, all of that. And so I think what we're noticing now is that now that AI actually works and is useful to make it useful is an engineering task. And I don't think people are, at least I'm not seriously proposing that, you know, the 20% gain or even the 100% gain that we're going to see, you know, next week at whatever the next ML conference is going to be, is going to come, what's that? 20%. 20%. What? OK, that the big gains are going to come from, ah, you know, there was this paper that showed that C camp, you know, that's... Psych the KMP was involved in some, ah, that's the key insight we need it. No, it's not going to be one to one, but rather when we come to the next stumbling block, right, that will be overcome by people who have thought deeply about how neuroscience solves similar problems and is able to overcome those. And an obvious one, which I'll just put out here, is the energy and efficiency of modern. I mean, this is the low hanging fruit, right? This is the clear example of something that biology manages to solve whatever problems it solves with max, you know, 20 or 30 watts, whereas it's how many, you know, catawatts to train GPT-7. So I think that, you know, we don't yet know what we... Where are the next inspirations are going to come? But history tells us that all the... With the exception of transformers, all the significant advances have come from looking, ah, deeply at neuroscience. Yeah, so I think, of course I agree that neuroscience has had... Oh, sorry, that... Yeah, neuroscience had a great influence on AI over the years, and Tony gave you a lot of examples, but it's... If you think about it, all the examples just Tony gave you are 50 years old, 40 years old, ideas. And that's my problem, is I think, yes, originally we had an influence on AI, but today you'd be hard-pressed to come up with modern neuroscience idea that is influencing AI. You don't see too many nerves people attending this conference, but the other way around, we do continue to attend the nerves conference, and I could come up with a very long list of contribution that came straight from AI that have completely changed the way we do neuroscience, and I'm sitting to one guy who's exactly in that tradition, and Kim is representing that perspective as well. We're really... The imbalance is striking, and it's been striking for 40 years, and as much as it would love to say that we are going to help AI, I don't think we've been very impressive in that respect, whereas in the other direction, it's completely obvious. And I think I want to make that practical. It's like if I were a funder and I had 100 million dollars to invest, right now, the big question is, should I put it into neuroscience hoping it's going to lead to the next wave of AI, or would it be much better use of the money to give it to AI people to work harder in neuroscience and help us develop theory and life. Just point out that 100 million dollars is enough to train one large language model. So the question is, is that really... Do you really say that that's where you want to put your money? Is like training the next high-parameter large-language model? I'm not talking about Google, I'm talking about NIH, and all those... the people who fund us, right? It's not Google. In fact, from what I understand, deep mind is basically shutting down the neuroscience section. I think there's a big sign there. I mean... I came into it anyway, you know? Maybe I do, but I'll tell you, the word out is out in London that you don't want to apply to deep mind if you're neuroscientist right now, that this is no longer what they want to pursue. I think there is a big sign there, okay? An open AI, but four members of my lab who are there and they're not doing any neuroscience and they're not attending talks in neuroscience. Okay, that's a reality of what it is. And so we have... I think we have to be honest about this. It's nice. I like when Blaze says, no, it will happen, but it's a question we have to be practical, and that's why I'm talking in terms of dollars. Is that how you want to invest your money tomorrow? As much as I love to get money to work in AI, I'm not sure that's the best investment. If I can just say it. I mean, this is... In biotech, there are biotech companies whose job it is to take the advances that have been made in basic science labs and convert them into products that are useful for society. When you have a biotech company, you don't typically see a large wing of the biotech company, or for that matter, even a farmer company which could potentially afford to, invest it in deep basic research, because there's a pipeline. There is certain kinds of research that is high risk. It has a long tail of success in that most of the things that people try fail, a tiny fraction of those turn out to be important, and those are the ones that can potentially be commercialized. And this is... Basically, AI is now sort of sorting itself out into the natural scheme of things, where their time horizon is maybe three to five years. And that's engineering. That's not basic science. And there's a longer time horizon of basic ideas. So if the belief is that the current ideas, if scaled up, will allow us to achieve AI, then indeed, there is no reason to look to neuroscience for insights. There's another possibility, which is that no, there's something missing, but the engineers will figure it out on their own. That's a real possibility. But the last possibility is that when the modern approaches, the current approaches, starts sort of maxing out, that they'll realize that there's something missing, and that that something missing will be something that we know nature has solved in some way, because we are the thing that is being targeted. And so that's sort of the hypothesis. I agree, Tony. And likely it's already in the room. I mean, you were saying, you were saying, I look like, you know, these ideas are 50 years old. I mean, it's true. You know, Huebel and Weisel is old hat. Also, CNNs were still the state of the art in 2017. So, you know, it sometimes takes quite a long time for, you know, things to go from the basic research to, you know, milking the cow. And the fact that what we're doing right now in industry is attracting a lot of money. I mean, I find it very sad to reduce the thing to, you know, to economics. Just as I find it very sad for people who are supposedly doing basic science, for all of their grants to be about, you know, if they're studying neural networks, it has to be about epilepsy, you know, or if they're studying anything cellular, it has to be about Alzheimer's. I mean, it's not just about fixing old people, you know, or sick people, right? It's also about finding out, you know, like the basic, really basic stuff that's important, right? So we should, I think we should keep our eyes on, like, the, on the time side. I don't need to disparage medical research. It's very important. No. I'm one myself, so I feel like I'm starting to be able to, you know, like speak with the community about it, but yeah. I think definitely about Alzheimer's. Yeah. So I mean, just to the points about the, like, interaction with neuroscience at DeepMind, it's definitely true that it's shrinking a lot. It's not entirely dead and it's pivoting in a couple of ways, which not, like, just in the interest of, like, mirror self-defense here, but more just like, because it actually does have some, some, like, meaningful points about the interaction with neuroscience and AI right now. A lot of the neuroscientists at DeepMind just went off and are doing machine learning now. Machine learning that was related to the kinds of things they were studying, usually, but something that's more in the space of something Google cares about a little more right now. So like language models and just taking whatever kind of version they had and doing it that way. The folks who are still doing neuroscience are kind of doing that in a new way. Some are doing, like, really, like cognitive science or neuroscience on language models, things that are, like, you know, kind of using neuroscience-y attitudes, but like not on a neuroscience problem. Some folks, like me and Kevin Miller and Marie-Ex, Diane and Zeb Kirt Nelson, we're doing more things that are, using AI for neuroscience applications. It's really not in the original spirit of DeepMind's neuroscience team, which was like, we're going to understand the brain so we can build AI. It's really more like, okay, now there's all this cool AI and people are trying to use it for AI for science. Neuroscience is a science. So it gets to be in that umbrella of AI applications. And it's, there's really like kind of two ways this works. One is like, AI is kind of a tool, find patterns, be a model of the brain. It's like, it's a big complex learning system. It can learn patterns about big complex systems, the brain, and study have a lot of those properties. Another big one is that we have a lot of external collaborations now because a key advantage of being at DeepMind is seeing the machine learning stuff that's working and being like, hey, this would be a neat idea to gain insight on some question in neuroscience. That question is not always best explored in the industry setting where scale up isn't really what you need. It's like people to think about it for a long time. But that's pretty much how we've been adapting ourselves for like, what you very accurately describe as this like, you know, really different relationship. It was exactly your words, but that's my gentleman take. Does anyone have a comment on that before I throw it to the audience? There's a line forming questions. Audience member number one. Well, thank you. So besides the pessimism of some of the panelists, I wanted to ask if you find common language that we can use both for understanding the brain and improving the engineering of the systems. So we see in neuroscience a lot of dynamical systems, maybe for theory as well, but we don't see it as much for AI. Are we stuck with just thinking of units that integrate information from other units? Is that the commonality or do we see any other common language that allows to do the science and the engineering? Well, I think that dynamical systems, you know, is actually poised to make a pretty big comeback in AI. I mean, for one thing, you know, we've moved from CNNs and things like this, right? They're just static functions to encoders decoders type models, which are auto-regressive and which do implement a dynamical system. So I think that a lot of that classic, you know, a lot of this classic, you know, both dynamical systems and also even kind of basic ideas from physics, of some of the kinds that I was talking about earlier today are poised to be a new sort of common language for us. Yeah, just to, I think that's exactly right. And I think that as AI starts to sort of address the kinds of problems that so far it hasn't been very effective at addressing such as like controlling a robot where we're like, haven't really made that much progress. I mean, some, but not like the dramatic progress we've made in image recognition and language processing. I think that things like dynamical systems and a lot of the, yeah, the shared vocabulary with neuroscience will begin to kick in. So I, yeah. So I have a question that's kind of inspired by the B-talk from yesterday. And so a lot of the neuroinspired AI is really, you know, coming from a laminar cortex from mammals, you know, and there are all sorts of other smart creatures on this earth that have radically different anatomical platforms, but they're still intelligent. And I guess I was just wondering, you know, kind of writ large, if there are ever any instances of AI models that have been inspired by non-mimilian brains. Moving on, then. Sorry, sorry. I actually have one example. So a neural cellular automata are inspired by morphogenesis. And, you know, so that's not even the brain at all. That's that's sort of, you know, patterning with chemicals and local interactions. And I think NCAs are, you know, are also really important in front here, actually, in AI right now. Yeah. And I guess one thing I'll add to that is that artificial neural networks, you know, the field is used to be called machine learning, hence the emphasis on learning. A lot of insects and a lot of invertebrates actually, I mean, bees are remarkable in how much they learn foreign invertebrates, but a lot of invertebrates do remarkably well with relatively minimal amounts of learning. And I think that having lots and lots of neurons is particularly useful if you need them to learn a lot, but a lot of organisms work pretty well out of the box. And in some cases, you know, see elegans, they work with 302 neurons. And so I think that it is partly because of the style of AI that, you know, where the focus is on learning a new task each time rather than getting really good at one task with a small number of neurons that you see that kind of a difference. Actually, your question made me think about this. And I think it's the opposite right now. We are AI is modeling insect intelligence. We are not integrating anything that's specific to mammalian cortex. We don't even have cell types, right? That's like one of the big questions. What's so many cell types? We don't have a laminar architecture with like all the feedback stuff. We don't, we barely even have the feedback between areas. And in fact, I'm hoping and then I'm going to go along the lines of those guys that perhaps that's where we're missing something where neuroscience might be able to contribute. But it's not for lack of thinking about it. There are many, many labs that are desperately trying to come up with good ideas on that. But nothing super convincing is emerging. I'm looking forward to Raj Rao's talk tomorrow because I know that he's been thinking hard from the control theory perspective. So maybe those ideas are going to emerge, but I think so far, we are actually what we're doing could work just as well for insects or mammals. I am going to try to phrase this question in a very like open manner. I really want to take it broader and Tony, I think we talked about this in my thesis proposal, but anyways. So I'm not a plant. I am interested in neuroscience because I think it's intellectually interesting and I also find great value in helping people with disabilities or with mental illnesses, even if it plays you don't. It's okay. But I want to understand why you guys are interested in AI. I add a neuroscience conference, right? So maybe you find it intellectually interesting, which is also great, but or do you, are you interested in the applications that it can provide like Jonathan to neuroscience or like whatever else motivations you have? Yeah. I mean, my own answer is really summed up by the famous quote by Feynman, something like that, which we cannot build. We do not understand. So for me, the reason for being interested in AI is that, you know, I can, I can be as all I want about my beliefs that this circuit works in this particular way or that particular way, but you don't really know until you've tried to build it. And I will say that, you know, in the in the early history of, I guess, up to the the 80s, vision researchers actually thought that like Hubell and Wiesel and we're done. It's it's feature detectors all the way. And I, you know, I was sort of at the tail end of watching vision researchers recognize that they're very simplistic models of we just keep building a set of feature detectors and we have vision don't actually work, right? And they didn't really, it took them a generation to recognize that that didn't work and that they, it took them actually trying to implement their ideas to realize that well, there's more to it than that. And I think a lot of where we are in neuroscience has that flavor, you know, especially you know, our inability to build robots that can interact with the world really drives home how poorly we understand, you know, the entire system. So for me, those the two sets of questions are really one set of questions. I was handed the mic. I don't have much to add given like a lot of the discussions that that we've been having. I mean, I think for me coming from kind of the AI side and coming into neuroscience, I saw an opportunity to be able to utilize like this immense set of tools to be able to derive new insights both into disease, but also into brain function. And I think, you know, in some cases like, you know, we're working with transformers and some of these models that aren't nearly inspired any longer, but I think it's through really trying to see kind of how far we can go in terms of reading out from the brain in potentially really exotic or maybe not nearly inspired ways that we can maybe start to just see what's possible. And I think opening ourselves up to that without the constraints of biology and using that to derive insights from brain data could also be a really promising direction, right? So once we see, oh, these are the types of systems we need in order to decode or read out from the brain, then maybe we can actually use that as insight and go back to the neuroscience as well. Yeah, I really agree with that. I mean, it's I like AI a lot as a field for neuroscience because it's a really, it's a very nice frame of reference. It has a lot of cool models that can learn how to do cool things. I guess the question of like why neuroscience and AI like why did I guess that I couldn't tell yeah, I guess that's sort of both a personal like why is this interesting historically as well as like what do these fields objectively have to say about each other and like you had a Feynman quote and I have like a von Neumann quote. The von Neumann quote is when we speak of mathematics, we might be speaking of a secondary language built on a primary language of psychology. I just I think I have always found the brain or maybe in general this this like idea of an intelligence system kind of a intriguing idea because it scaffolds all other reasoning and you know as someone who never who had trouble picking a major that felt like the deepest and most like I don't know a fundamental problem to study at some level they both have a lot of the same bachelor problems. I was just going to say I don't actually study AI but I think I'm interested in information processing in general so I want to understand how do we get long time scales but we can ask that question about AI as well as we could about the brain so there's been a surge of interest lately and actually maybe the successor to the to the transformer arguably is going back to linear state space models so there's been how do we get long time scales in an artificial system how do we get long time scales in the brain how do we do contact dependent processing so I think there are a lot of these kinds of questions that we want to ask we could ask them equally in a complex brain or in an artificial system so that's my interest. Next question. Okay I'm going to ask two I would like to if I'll be asked one if I only have to okay so I guess this this question comes out of the talk of Alex Pujay that he gave today which was talking about compositionality and all that and that the solution in the end was just scaling it up more and more and more if that's really the solution or not but so I guess my question is if you compare the resources that academia has and neuroscience like we can't throw a hundred million dollars at training a neural network so how would you see that academia or neuroscience can work together or necessarily we might have to compete with the research that comes out of meta Amazon Google who can just throw more and more layers at this. You can't. So I think that there are two solutions one of them is to collaborate and that is becoming a harder for a Kim's point. I mean you do have two Google people on this stage so obviously they're a bunch of us who care but we're also in CalMilking era but I think that a more interesting answer is that maybe it's not just scale. I think that the relentless pursuit of scale right now is really interesting in the sense that we're going to see how far that goes and where it taps out I'm very curious to see how far I can go and I don't see an obvious ceiling but I also don't think that that is the way we have solved it that seems fairly clear. I mean we consume a very limited number of tokens up until age you know four by which point we're linguistically competent so there must be something and we use 20 watts you know if we're at a stretch right so we're obviously doing things very very differently so I don't think that computing with the big AI labs at their own game is the way to go at all. Before you ask your question does anyone feel like I do that neuroscience is paying way too much attention to AI? Do you think okay? You want to elaborate? Everyone's being a lot of attention to AI right? I think you should be take attention right? I mean these things go in phases right? I mean obviously there's a pendulum I think we'll have yeah. Oh rather the pendulum but yeah yeah every how many posters did you see today? Okay so I was at a quote unquote debate where the two participants were drunkenly going back and forth on these things is like four years ago and the pendulum was brought up and David Cecillot wasn't slurring his words when he said that how David I hope you're out there that you know we're just that it's a pendulum and right now AI is is kicking neuroscience's ass. That was four years ago and I think it's more so now like the pendulum didn't know it could go as far. There is no pendulum. AI has been influencing neuroscience for the last like throughout my career I started in late 80s and all the ideas from AI meaning machine learning having influencing us every single year. I actually don't see a trend I think this is I mean like I don't see a recent trend that's what I did meant to say like you know David kind of it is among the people who revived the kind of a neural network craze in neuroscience so that's kind of what a new trend but before that there was this whole period of time where we were invasion approaches that came straight from machine learning and from nirips so there's been a one way highway it's been very active for 30 years so I don't think it's a pendulum at all and I think it should continue by the way and we should keep even even inject more AI and this conference no was created in part with this idea in mind is to inject massive amount of machine learning and AI into neuroscience I think we're successful in some of the Alex. Yeah I do agree with that in general but I've seen just even over the past like two years or so as being someone like in AI that cares about neuroscience I used to have a lot of AI researchers coming to me and asking me like oh so what is the brain doing and like how does this work and they were like really thinking that neuroscience was going to help them to solve certain problems that they couldn't solve and I think over the past two years probably with transformers scaling like a lot of that kind of movement away from thinking about like new losses or new ways to train networks and just wanting to scale I don't see as many of the AI researchers actually coming to neuroscience as much or I see like a bit of a trend moving in the opposite way at least just from my perspective and talking to people but yeah I mean I think in general we've been influencing each other but I've seen a little bit of a backslide over the past years. I'm kind of bummed that we're talking about people having this very clear disciplinary allegiance. I mean most of the people who have made really cool advances in both fields have have skills and competencies at both and have published papers that are interdisciplinary and so I you know I think this this really that we've been having the debate suggests a level of professionalization that's really counter to you know to you know having idea having new ideas right which always in both hybrids. Yeah I mean to the point about like whether we're overdoing it on the AI stuff I think we'll probably look back and and be like we really tried to put a square peg in a round hole in a lot of cases but it's still because it's new because we can it's it's a it's new and it's clearly interesting and we can use this to explain and try to do a bunch of things we haven't done before. It doesn't seem like an obviously bad idea for people to like really try stuff try it's a good it seems like a good period to be exploring it and I don't know if I think we'll probably look back and feel like we overfit but I'm not sure if actually like we have a totally a choice or maybe it's still optimal to like be trying a lot of stuff out in this space. If I could just add to bless this point for those people who do think it's an interesting thing to be bilingual in neuroscience and AI cult spring harbor has a as a program for people who want to spend two years. So so I was asked to try to encourage Jonathan and Alex to you know inject a little more edginess in and and I had a couple of questions that have impartially asked and answered and I guess I'm only allowed one question. I will note just in passing that the concept of attention probably comes from psychology and so transformers are probably not devoid of neuroscience in the Tony and Eric sense at least of neuroscience but I think blaze you just kind of undercut me because what I was going to say is I think this debate is ridiculous and all of you guys have both contradicted each other and yourselves and if we look at models where science works it integrates ideas from many things that it has to use tools from mathematics and statistics to understand systems that are complicated. I will point out that this becomes problem when people become professionalized in academia so even in neuroscience groups even at co-sign there's not enough real interactions and collaborations between the theorists and the experimentalists. I'm very experienced in watching theorists come and collect data from some experimentalists and model it. There's no iteration. There was no tested hypothesis from the theory and they go on to model the next thing and the next thing. Well the experimentalists working on it. Do you have a question? Do you have a question? The question is if you're all really doing the same thing and you all really agree that these things are interesting why do we have this basic problem of integrating these things in even neuroscience let alone between AI whatever that is because we are the model for intelligence right so it's going to be directly based on us. So there's still some kind of fundamental cultural problem that we're not overcoming in the way our data from the brain is collected and used with regards to the theory and vice versa. I lost the question. Does anyone remember the question? Was that a question? I didn't actually hear a question. Thank you for the comments. Thank you. You guys are ridiculous. I think that was the sum. Go ahead. Yeah just one short question. Just a quick comment. I think with regards to robotics, DCA is going to be out of robotics. We will see they do a lot of I think tested. We probably surprised neuroscience as well. But with regards to scaling, a lot of the system that works, it requires lots of engineering efforts and this AI engineer is the right kudo-kanal. But we don't do this sort of stuff in neuroscience. Maybe the algorithm principle from engineering we can build an AI which is going to be similar to the current AI but we'll probably get there by scaling. I was wondering what do you think about just doing a little with engineering in neuroscience as a scale this? I don't know try scaling and try different hardware. So yeah. I mean I can just say that we in my lab are currently trying to invest in these sorts of approaches of being able to train models on large amounts of neural data and I think that this has been a major gap in the field of neural data analysis in some ways where we do have to like fit a model for each new data set rather than actually being able to scale and combine them. So I think that through the convergence of both these fields and knowing what we need to do in neuroscience as well as how to train these large scale systems, I think we are moving in that direction as a field. Next question please. Hey interesting discussion so far. Right now when I look at the kind of phenomena look at an AI in neuroscience, they seem a little bit different at least in one way. There's probably more that you're aware of and this is that AI thinks a lot about transfer. How do you transfer knowledge to new tasks, new data, etc. It seems like less of a focus in neuroscience. It could be wrong though. If there could be a shift in focus or paradigms would help would lead neuroscience to be more informative to AI. Can you think of some when it seems like transfer for example? That's my question. Thank you. Yeah I think I mean so there's like a lot of times when people start talking about like what's what's new in exciting and neuroscience a big thing is scale that there's like all this data we have tons of neurons we can get them over really long periods of time. One dimension of scale that seems like it's challenging is having lots of like getting an animal to perform a challenging behavior, getting it to learn something really complex and rich seems like it's a big bottleneck speaking as a theorist and not an experimentalist but like that's that's my impression and I think like one dimension if if one wanted to go all in on on the scale idea which like there are other approaches and not everyone should do that but it seems like that's a dimension for scaling up and if we can get faster learning curricula that would be nice. I'll just say I think that's a great comment and something that's missing in neuroscience. We often train an animal to do one task and we study you know how they how they solve that task and we don't think very much about whether if they actually there's very nice people from Byron News Group where they did look at different animals that had different training histories and seeing how that they actually solve the same task in different ways. So I think there's part of it maybe is the necessity of neuroscience experiments that animals you know mice don't live very long and so we don't typically do train them on 14 different tasks in a row you do but I think the question of why we don't forget something when we when we learn a new task has actually inspired a lot of the ML research the AI research about that you know we typically don't have catastrophic forgetting where if I learn to write a bicycle I forget how to do jumping jacks but but AI does still sometimes. Quick quick comment on this so the huge revolution in in AI has come from unsupervised learning which turns out to include every task and and one thing that I really haven't seen happened very much I think you know there was a big revolution of course in neuroscience when we started to be able to you know do awake you know awake in behaving recording and from large numbers of units but I haven't seen so much of that unsupervised revolution come to neuroscience and that seems to me like a big opportunity in the sense that the the amount of observation that one can one can make grows by orders of magnitude when you're not you know just collecting your one bit of of of information or like when did the tongue touch the thing you know for every for every experiment but but just you know measuring everything and and and discovering the latent variables in it so that that would be one idea one thought. Go ahead. Okay so yeah you guys talked a lot today about new AI is trying to bridge ideas from neuroscience to AI or vice versa but I wanted to ask what do you think of the perspective of the new generation of new AI as a whole new third sort of type of science that's just trying to study information processing machines that are optimizable more generally speaking because even before AI or before humans evolved you know information exists in the air just emergent from the statistics of the world and a lot of ideas like tissue like doctor tissues information botanic theory or doctor called for instance free energy principle I think kind of more tacking like this abstract information processing idea so this is personally what drove me to this field and I wanted yeah to see this discussion basically. Yeah I love that that point and I mean that was that would have been my answer to the you know what what drove much of me to the field as well or why do I do this and yeah I didn't mean to say anything bad about medical application but you know there is there is a search for fundamental principles here and and you know I think that if you either you know go down the route of well you know physics is you know biology is not physics you know there are there's not some grand unifying theory or you know well from new kind of science like it's just going to be you know simulations that are unexplainable you know and they'll be able to make predictions but we don't be able to say anything about why or how I don't I don't think that that's the case so I think that there are general principles here and and that understanding them is not going to solve you know all of the pharmacological details so you know how to you know how to you know how to work brains in the right way to fix them but it will tell us something about about how real brains work and give us engineering principles that let us that let us build things like them and I think we're actually pretty close to to some of those so that's that's definitely my motivation has AI influenced neuroscience in any negative way all right next question well it's kind of related to that so I really like to ask this point of making things practical and if we're not talking about donations other way to make things practicals education so we're training the next generation of neuroscientists we can only make them take so many courses you have to give them some certain framework to work with and I've noticed over the eight years I've been helping teach this course at UPEN that it used to be I would teach something out the brain and the student come up oh how could we bring that into these neural networks how could that happen more often now it's oh you the brain does that but my neural network doesn't need it why does the brain do this and I have some colleagues who are absolutely horrified that that's been the change and some who are extremely excited by this and think it should be more so it's the broader question is do we think we need to pivot neuroscience into a much harder emphasis on ML that we really should take that route or is there a more balanced approach what does that kind of look like in your opinions yeah I guess sort of to that question and also the one that you raised I mean the yeah I just had to cue it um the like that gets to points about biological plausibility and I you know I my background is more of as a cognitive neuroscientist and I haven't focused tons on biological plausibility but that would probably be what a lot of people would say has been a downside of sticking too much to AI models is like it makes it you know easy to focus on questions at a level that's maybe not biologically plausible and you got to be kind of careful about the abstractions you're drawing in terms of like the brain doing things that models don't do sometimes maybe the reason is just a biological plausibility one maybe it's the literal biological materials or maybe it's just something about learning in a single lifetime and having to like grow a brain out of almost nothing um so like I don't totally know we also don't know what's implicitly going on in all of these models so it's hard to say exactly what they aren't doing for sure my sense is that uh we have to inject even more in the ML in the field through training that neuroscience students should all have to take nowadays AI and ML courses and unfortunately it's not part of most curriculum that I know um and that's a future of the field I think if you look at physics you know any physicist who gets in the field even they want to be an experimentalist they're going to be absorbing enormous amount of theory before they they're done with the study and the neuroscience is quite remarkable that you can get in the field and right now know very little about the theories that are dominating the field and that thing is just a reflection of the fact that we're a very young science uh that came from biology and there was a sense that we really needed theory but I think there is a really severe need for more theory in our field and this conference was very much created with this in mind it's it's trying to contribute to that and push in that direction we're sure on time I'm gonna take one more question sorry for the three of you but you can come up after I'm sure last last question a lot of pressure um time's up all right next question um yeah so I think it's really interesting that while we are talking about this mutualistic relationship between neuroscience and AI there is this almost parallel mutualistic relationship between AI and cognitive science and so yeah and Alex talked a little bit about Josh's work but I haven't seen too much here and it's obviously coastline and hot CCN so maybe that's why but I'm curious what you guys think about this sort of trip partite relationship and how we as the computational and systems neuroscientists can think about what they're doing and maybe gain some inspiration and benefit from their work as well I'm really glad you brought this point that we're gonna end on this because I do think that's I've always been a big believer that we need neuroscience cox i and machine learning to crack the brain and I always been frustrated that there's not enough cox i this conference it eventually led to the creation of CCN because a lot of the people were frustrated and so they created their own conference I don't think that's good for the field I think it's keeping us separated when in fact we should all be arguing about this here that's what I was trying to do is like by bringing uh Josh's work here I know no Tim Barons and I argue about that all the time and he keeps telling me you guys need more cognaro and I'm gonna end by saying that actually if I had to put some money on who's gonna influence the eye I think cognaro and coxive scientists might have a better shot in the next a couple of decades than the neuroscientist I don't know I I mean I think that on the one hand you know psychology experiments and things like this really are really useful you know so I I you know and even sociology experiments for that matter the more quantitative end of it but a lot of cognitive sciences infected with chomsky bullshit you know and and and and has and has and has really has really done you know as chomsky has you know has done a lot to keep the field back and to you know inject repeated you know sort of falsibility you know that just hold a lot of a lot of things back for years and years so I you know I think that you know sort of holding it up as a field I don't know I would want to like do a little sifting uh to you know take get the wheat from the chat kind of thing blaze and I had this argument recently about chomsky there's soralities we should not forget that before chomsky it was behaviorism which was incredible limited and those guys completely changed neuroscience and coxive science but I would agree that they went too far in the whole linguistic thing and and with chomsky but I definitely think that this contribution were enormous and that there are people who have been able and again going back to Josh but I'm gonna not be able to give too many names here on the fly but there are many people right now in the field who have gone past that and to stand neuroscience are in the face and coming up with absolutely brilliant idea that we're not injecting you know work here and that we should but I just the pinker and chomsky of this world they made or in the photo of this world who I quoted in my talk um yeah that's I think we can leave behind I've been strong alarmed into asking you one final question and that is what will we be arguing about in 2044 in 20 years thanks everyone for oh sorry does anyone want to hazard an answer I'm gonna be the first to say I have literally no idea um I haven't been in this field as long as as uh a lot of people in this room but I my limited experience my limited experience has been characterized largely by like incredible rapid change um so uh I don't think I would have predicted where we are like five two years ago let alone five um so I I really don't know um that's scary because a lot of research is trying to make long horizon bets and and now is a tricky time to do that but well I know I know I know something I know something we're definitely going to be debating in 2044 which is which is the question of moral patience for a eyes in other words uh you know uh are are they people that's definitely going to be a debate in 2044 for humans as well perhaps yeah and now a message from our sponsor thank you thank you thank you thank you I alone produce brain inspired if you value this podcast consider supporting it through Patreon to access full versions of all the episodes and to join our discord community or if you want to learn more about the intersection of neuroscience and AI consider signing up for my online course neuro AI the quest to explain intelligence go to braininspired.co to learn more to get in touch with me email paul at braininspired.co you're hearing music by the new year find them at the new year.net thank you thank you for your support see you next time

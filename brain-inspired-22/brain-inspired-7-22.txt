 The brain itself is one massive dynamic system, so much of what we were called in the brain is generated internally by nothing outside of the world. So we need to understand the separation between what's, you've looked by the world, what isn't, to be able to peel apart the two and go, no, this is what's happening inside, and that's what it means, that's what's happening outside. When I spin my brain forward, a hundred years into the future and go, what will the explanations and neuroscience look like? Almost everything that we understand at that point would just be from massive simulations. We have great theories for what Zalibis with. 3% of neurons? Oh, so you got it. Great theories of what they're doing. You only use 3% of your brain, but I think that's the new. Great. This is Brain Inspired. Hello, everyone. It's Paul. Today, I have Mark Humphries back on. Mark runs his lab at University of Nottingham in the UK, which he describes as a neural data lab. That means they're more of a theoretical lab that uses computational models to analyze neural population data that's been recorded from other lab's experiments. So back on episode four when Mark first appeared on the podcast, we talked about some of his work analyzing how the population of rat prefrontal cortex neurons relates to learning new rules in a behavioral task. He's written a book now, which is the main thing that we discuss today. The book is called The Spike, an epic journey through the brain in 2.1 seconds. Like the title says, it chronicles a couple seconds of brain activity of someone, and that someone happens to be in a meeting at work, and they see a cookie on the conference room table, and they think about reaching for it, and they finally reach for and get the cookie. The premise of the book is the story of what it would be like to travel along with the action potentials happening in the brain during those couple seconds of cognition. And along the way you learn where things get processed, and how they get processed, you learn a lot about what we do know, some of the important principles discovered over the past 30 years or so. You learn a lot about what we don't know, so some of the big mysteries left to solve, and what we think we may know. So theories about solving some of those mysteries. And we talk about just a few examples from each of those categories, but mainly focus on some of the bigger discoveries and questions that he writes about. Like we discussed, the book is aimed at a smart audience that can pick up concepts, but it doesn't assume that you know much or anything about brains to follow along. So you don't need to be a brain expert. It can be used as an introduction to a lot of these things. But if you are a brain expert, it still takes you to the modern challenges of neuroscience, and puts those challenges in the context of historical knowledge and current knowledge. So show notes are at braininspired.co.slash podcast slash 102 102. Thanks for listening. Happy reading, I hope, and here's Mark. So Mark last time you were here, I just checked it was episode four. So welcome back. So we actually just chatted a couple of weeks ago because you were included in this little hundredth episode special series that I did. And one of the things that you mentioned that has benefited your well-being and career in the past five years has been this idea of writing down just one thing, the most important thing to get done in a given day. And then the rest of the day you can use to get all the other scraps done. What was that one thing today? Well, presently it was prepared for this interview to be well, but so good. I was scratching my head, but it was preparing it in a particular sense because I'll see, I finished the book about a year ago. So I had to go and re-read quite a bit chunk of it to refresh my memory of exactly how I structured some parts of exactly what it said in various parts because the slowly time in academic publishing, in the publishing world compared to even journal publishing means that I haven't looked at some of this text for a year and a half. So it was great to go back and take a look at it with very fresh eyes and go, okay, that's not bad. It actually does make sense. Yeah, that's always nice. How does that compare when you to publishing a paper where, once it's finally out in print or online these days, I suppose, it's old news by that time, but there's even a longer lag time for a book. Yeah, there is a long, long time for the back. So it's been, as I said, I handed in the final version back in March, in March 2020, just as we went into lockdown, the pandemic broke big as well as writing the last, all the read draft of the last chapter, so that was challenging, but it was got done. So it's the reverse of a paper. So as you say, with the academic paper, it kind of just peed as hell. You have all this huge energy build up to the first submission of that paper and then the revisions of the paper and rejections of that paper and resubmission of that blah, and particularly now when we preprint all our papers, most of the sort of the big ump is gone because it's there now and just trying to push it through the door at a journal somewhere. And the book is completely opposite. So it was written a year ago, it's been sat and as it come up to release day on March 9th, it's been, the amount of things to do with the book has been escalating. So it's becoming, it's a clearly an event in a way that a paper release just isn't. There's also a bit more of a permanence because you know, to a book because you publish a paper and then it's kind of onto the next thing, but the book is more of a statue, right, where people can revisit and revisit a lot and sort of its own thing. Does that make sense? Yeah, indeed. So it's eminently because it's kind of monolithic where people aren't going to add much to it. So it's a standalone thing. It's also a bit like a, I don't know, it's been a message in the bottle too, it's just been thrown out of there to float around the world. I hope people pick it up and read it. They can read it as many times as they like this, but it's not a, who do you say it's em, it hasn't got the ephemeral nature of the other paper. Some books obviously clearly have, are designed to be ephemeral too, right? So we have us, we have a sudden, you know, bookshills for the books about COVID-19 that in about five years time will be, you've, I'm purely, I could never be interested. What do people think at the time, which will be? I sure as hell hope so. Exactly, yeah. It's good, good cover, yeah, yeah. Yeah. The other thing that I wanted to ask about is, you know, blog writing because, you know, you write the spike and in a sense, I mean, it's, there's almost like this model these days and I don't know how writers used to do it. There's almost, it seems like a model these days where someone, you know, you write a blog and then you kind of write a book from that, you know, like use like your blog post sort of as inspiration. I mean, some people just put their blog posts in a collection and call it a book, which is, that's a different thing. But you know, a lot of people in science, it seems, are doing this kind of model where you write these blog posts and then the idea is kind of coalesce and then you realize that it can be a book maybe, I don't know, is that, that's a question actually. I'm wondering if that's the case, like how the, the blog writing interacted with the book writing. To be fair, we initially were me discussing projects with my, with my agent. I would just, just you flow with the idea of, do we do some kind of collection thing? And it's a no, no, no. You need to be, you need to be a book. So it needs to be, a first book needs to be a thing that's really, you know, really, it makes a statement about what your position on the world has many, much of anything. It's a bit sort of, you know, sort of essay books of essays and stuff, I further down the line and established, you know, mature authors type of thing where people will be interested in reading what you, your thoughts. So for me, as it turned out, the, the idea I had for the book, I had more or less before I started writing the blog. So clearly, as it turned out, the, the blog was a place where I, I, I ended up rehearsing a number of the ideas, ended up in the book, but the idea of the book and the structure was something I'd had a, like a year or two before I started the blog, just this idea of this notion of what it liked to be a spike. It would look at the spikes, the spikes on the world, coming from Neuron to Neuron, what, what do I see? And that's, that's where that came from. This is very, you're talking the book in the beginning, maybe the preface about having written since you're, you know, a kid or started a thousand manuscripts or a thousand stories and that comes across in the book. It's very story like and the, the point of view of the, of the spike reminded me of this, I'm sure you haven't seen this. It's an American film is called Inner Space. It's from the late 80s. Oh, Dennis Quayne. Oh my gosh, you know it. Yeah. It's a movie like he's a pilot. Dennis Quayne is and he gets shrunken down and accidentally injected into Martin short and then all sorts of hilarity and sews. But I mean, it's him like from the perspective of a, of a very miniature person going through Martin short's body and interacting with his body. And this kind of made me think of that your book thinking of traversing the brain as a spike, you know, writing on a spike kind of made me think of that. So that's interesting that you actually know that movie. Yeah, you know, and it's, yeah, it's rift on a lot of British TV shows, actually, with the kids collecting or danger man. I didn't realize it was a classic apparently, apparently all the creatives in the, in, in television grew up watching that film. Oh, interesting. Yeah, it's a great point. That's, that is kind of exactly, that's good because this is actually what I was aiming for right to get the read of the feel of they were with, essentially with me as well, both of us were with the following the spike through its very, through its journey, through the, through the brain. So the, yes, so the book was structured as a journey. So they unfolded as we follow this spike from the eyeball through all the various complex processing centers of the brain to the part where it controls the arm to move the hand to the thing it's doing. And so that journey allowed me to both then like slowly unpack as we travel the various aspects of what it means to be a spike, how you create one why we have them. So the unsightly, basically understanding in the first, you know, the start and then the journey as we go through the brain also opens up to more and more complex ideas of, of how spikes work, how the brain works using them and how it's used in decision making memory and all kinds of things. So it was meant to be a journey in two senses of through the brain and also as every chapter is a new set of more ideas that build on the last and it gets bigger and bigger. It's all with the premise of someone in a, like a meeting in an office and looking, considering whether to reach and grab a delicious cookie and throughout you come back to the, you know, what's going on in the office with Barbara or Pam or someone, but, you know, moving to the left and stuff and then relating it back to what's going on in the brain. So it's, so it's a fun read in that sense as well. And the language that you use, you can tell like you have a, it sounds like you're having a good time when you're writing it anyway and coming up with the many metaphors that you use in the book. And it is interesting because it, my question that I'm about to ask is who the book is for because it starts fairly basic, I would say, you know, like how spikes kind of are generated and some of the nuts and bolts of just a spike and what it does and how it moves through the brain. And then as you go on and on in the book, like you just mentioned, the story becomes more complex and you start to bring all of the gory details and the things surrounding what's happening, decision making, predictive coding, things like higher level concepts, right? So there's a real trajectory in the book. I feel like, so I'm wondering who might benefit the most you think from reading the book. So I was ambitious and I aimed at a lot of people. So I took the approach to sort of the classic advice of writing a popular science book of assuming all of this is intelligent but doesn't know the information. I, so I decided to, yeah, so as you say, build from the ground up. So once you grasp that the spike is this voltage signal that is sent by a neuron to another neuron, whenever it gets enough inputs, it's a signal that says, okay, something important has happened now. I'm going to send a bit of information. And from knowing that having really embedded that spending whole like a whole chapter, really making sure that that is embedded in the, in the real is head, then you can use that as a platform to slowly build out on out and out. So it's, it's, I see it's for a very general audience, but I had particular audiences in mind. So I wanted to write it so that it was of interest for AI on neural network audience. We wanted to understand more about what we know about brains to see if anything useful was going to be in there. So, you know, I, throughout the book, I touched back on ideas from AI and neural networks. I occasionally contrast a bit. We're talking about the brain about how neural networks work to the extent that I understand that they work. And, well, in particular, there is, you know, because this is all about spikes, it's the domain of systems neuroscience of the bits of neuroscience where people record from lots of neurons at the same time. And, of course, this whole areas of research fills with people who are interested in this kind of stuff, but have no way of getting into literature itself. And so it's also in one respect, it's for an enormous audience of clinicians and, of medical people, and people who work in molecular stuff, and people who work on, you know, anything you ever touch is on brains who won't ever chance to actually detail them to the literature. This is like a one-stop shop, saying, this is all the core stuff we know about how brains use spikes to do stuff. Nice. And it was also interesting, so I've actually gave it to Ashli Dravenet to have a read. She's a sociopressor to UCSD, who specializes in teaching neuroscience, because it's interesting how it would work as, you know, you gave it out this time undergraduate course, where they, you know, we had a run with it as a supplement to, like, looking at the technical details and really understanding how it fits together, because they can do as a framework. So I was also keen to, you know, as people with a gateway into neuroscience, this would also work as a book. So yeah, so I had ambitions for a number of audiences. I can definitely see how people could go back to it, and every time they go back to it, depending on their level of interest and expertise, I mean, you can get kind of what you want out of the different levels from it. So that's an interesting, that's an interesting take. I also just, I find that I, and I don't know if you're the same way, I don't mind reading about this stuff that I know about. It's always from a slightly different angle, and it always, depending on how it's written and what else is incorporated, it makes you think of new things, which your book does. And you, you, you saying that you sent it to a UCSD to an American audience, because I, I was amused by the, the British lingo that you used in there that, that made me stop and think, wow, you really are going for like a lot of the assuming that we'll all be along with the British lingo. Anyway, one thing that struck me also is the book is, so you talk a little bit about behavior and decision-making, but it's really brain-centric in that you're really describing the brain as an interesting thing in itself, and it made me wonder about your particular interests. And this is kind of outside the book, I suppose, just, just asking you out of my curiosity, because the brain is interesting in itself. You know, if you had to describe your main interest and what you're most interested in, you know, explaining and learning about, is it the brain in itself and its workings, or is it the relation between brain and mind, or where would you put yourself in that camp? Yeah, I guess I open myself in that, so I actually draw more towards the, I try to understand the brain's internal dynamics. Obviously, to make sense of them, we always have to link them to the outside world, so all our, in a short while, papers are on this, so talk about behavior in some way, whether it's literally just movement, movement of a sea slug or movement of an arm, or whether it's decision-making behavior, but the core is about understanding how, essentially, how groups and neurons do what they do together. So once you get beyond the sort of the single neuron fires, because this thing happens in the world framework, when we get, you know, populations and neurons, that's where we're interested in both the terms of what they're, what we can read out from there, what they're coding and what they're sort of the dynamics that they create. This is probably due to the types of hooligans that I digitally hang out with these days, but it feels like there's a, you know, a backlash against studying these types of things for their own sake, right? So like population dynamics and neurons, without considering behavior as a, an overall, overarching top down, I won't say constraint, but inspiration. For how to think about neural activity as being explanatory, right? Do you feel that at all? Because there, you know, obviously no one, no one says that there's anything wrong with studying the brain for itself, just because it's a, you know, fascinating super, super complex entity, right? So just understanding like how it goes about doing anything is an interesting topic in itself. Do you feel the pressure of this needs to be related to behavior as a fundamental thing? I think you're right at the moment there is it in, particularly in terms of the eras of neuroscience that record neurons activity. There is a big push at the moment from various people to push his emphasis of going, always think about the behavior and on now further on, you know, more, um, ecologically valid times of behavior, making sure that the animal's doing something that's vaguely relevant to what it's supposed to be doing. Right. Um, but as ever, neuroscience like all sciences is cyclical, right? It has fascinance and come and go. So the last time this was a big push was, I mean, they're in the mid 80s, there was this huge outcry, but neuroscience, needing behavior. You can pull out all number of review papers and opinion pieces, which is, yes, we're getting too far from this sort of behavior perspective because we've followed human and viso, we've stuck an electrode into our, into our neuron and we've watched it fire when we've shown it's something and then we've gone on neuron up and we've gone on neuron up. We're just recording lots of neurons now. I've got lots of spikes. That's great. But we're not doing it with them. So we have this push to go back to behavior. So and then we give the behavior section and then sort of in the sort of late 90s to push in to theory at least was to go for a lot was a split between people looking science dance, then it's population coding idea. And then people who are really interested in the details of a single neuron. So building these incredible detail models, compound, compound metal models of individual neuron and then right is modeled by a whole bunch of different sections stuck together. So you need to simply computer just to run one neuron and that took over for a while and that phase and I've no doubt that will come back again in 10 years time as a next push. So it's true that the moment there's a little bit of, there's a little bit more emphasis on the need for behavior. But as you say, the brain itself is one massive dynamic consistent. So much of what we were called in the brain is generated internally by nothing outside in the world. So we need to understand the separation between what's evoked by the world, what isn't and to understand what isn't we need to understand what is how the circuit of the brain are doing generating all the activity and what forms it can take to be able to peel apart the two and go, no, this is this is what's happening inside and that's what it means. That's what's happening outside. So before we jump into the some of the topics in the book and how they, you know, may or may not be related to AI as well. Even how complex the brain is itself and how complex behavior is and we'll, you know, I don't know how to talk about how complex mind is. We'll just call it very complex as well. Do you think that an eventual satisfying explanation of the brain and mind linkage of how the brain is related to mind and the characteristics between them? Do you think that that's going to feel intuitively right or do you think that we're going to have to eventually accept, you know, that we can't quite grasp, can't quite grasp it in an abstract sense, you know, like these days. So we're like, like you were just talking about from the single neuron perspective, you can build these computational models. You kind of feel like you have some idea of the canonical computation, right, that some single neuron might be contributing to. And now we're getting to these larger populations. We're having to talk about dynamical systems and state spaces and the terms are becoming more and more abstract. Do you think that we're going to maintain an intuitive grasp of the explanation of how brain and mind are connected? Sadly, no. So. And you're comfortable with that. It sounds like you're comfortable with that. Well, I think comfortable, possible what I'm looking for. I know I'm resolved to that. Resolved? Yes. Yeah. So because on the one hand, so our explanations of mind are all psychology, right? We talk about memory of various forms about of perception. But those are, of course, are kind of two philosophical. There's called semantic labels that we give to our internal experience. There's no need that anything in the brain actually maps directly onto the things that we call short term memory, working memory, episodic memory. Sadness. Yeah. These are labels we give. So the mapping from mind to brain is going to be quite awkward in places, I think. I think that's a good word awkward. We means that then when we have a, we have a, we have an explanation of the brain beyond some kind of general principles, which we all agree with are in play, then it's going to be fairly unintuitive. Yes, I think it's that's inevitable. I mean, secretly, not so secretive about the tell you. The, the, for me, my, my brain forwarder, you know, 100 years into the future and go, what, what will the explanations and neuroscience look like? Mostly I'm left with looking at the fact that almost everything that we understand at that point would just be from massive simulations. Because we have, we just look at other fields that deal with systems of comparable complexity. So look at the weather and climate modeling, let only try to understand their thing in an intuitive sense. They have a toy model in your head or what might happen if this happens in a short, space of time, but on a longer time scale, you have to run a simulation. And those simulations will probabilistic, right? So you start different conditions and you get a range of outcomes. So if we want to understand how some treatment for Parkinson's will affect the brain, maybe trying to try and target a particular set of neurons, they're going to end up with some kind of in-selecto platform where we do that, run it, run it from various configurations and get a range of outcomes that will be predicted. Not satisfying, but science doesn't, is under no constraint to give us any satisfy answers. Yeah, I mean, the complexity and weather prediction makes me, I should have a climatologist on or a, what's a weather man called? Meteorologist? Yeah, I should have a meteorologist on and ask them how satisfied they are with, you know, having to run the simulations and how intuitive it all feels. Because eventually these things that makes no sense kind of, well, I think you use the word resolve. And I think that's, I think that's right, that you kind of give up and you get so used to it that it, but it starts to feel intuitive even though when you actually examine it, it makes no sense at all for it to feel intuitive. And maybe that's what's going to happen. So I think we're going to, as I said, sort of, I think we're going to have a, end up with a last set of agreed upon principles that we know the brain works by. And we're going to understand some of the algorithm that it uses to get from me to be, right, particularly algorithms for learning for creating memories, what things it prioritizes and so on. But those are going to be elements of a complete picture, right. So we stick them all together, we're going to need some kind of, and don't know, doubt some kind of simulation. In the same way that for, for, whether forecasting and for climate modeling, they know the physics extremely well, but the physics is extraordinarily well known. Sure. So, the problem is that you can't put them all together into your brain. You have to stick them on a super computer and run them to find out exactly how they interact. When I spend my brain forward though, I, and I'm sorry, this is a, you know, kind of a long tangent, but when I spend, spend my brain forward, I have this kind of wager in my head or it seems likely to me that there's still a long way to go with the neuroscience, with characterizing how to understand neural activity, populations of activity, the dynamics between areas and how they communicate and the different motifs, right, of computation and processing. I feel like there's a lot more of that to discover that will, that will be useful getting toward that final place where we're resolved to, you know, there's a gap and we're, and it's fine because there's a loose mapping. It's not, it will never be like isomorphic, it'll never be one to one, but, you know, the principles, I think, I feel like the neuroscience has a lot farther to go to contribute to the principles that will eventually be our explanatory resolution, you know, as opposed to coming from the other side from the psychology side. Okay, so we have a question, I agree that we have much to discover in neuroscience about what the, the principles in the birth, in the birth, but brain, we have a lot of things to, to resolve of, because we're only, we're only really spent the last 20 years routinely recording from one, one, one neuron at once. So we have, we're basically at the beginning of uncharted territory and we have a, what basically our first paper in nature, a few weeks ago from Matthew Karen Deany's lab where they finally imaged a hundred or so inputs to a single neuron. So that kind of little detail is a kind of thing where we need to understand properly how neuron talked to each other because we need to look at what the inputs to each neuron are. So yeah, so there's going to be this, these, these, these, a lot more, you say, a lot more of these principles to come up. A lot of these principles are going to be about, they're about going to be with dynamics, right? They're going to be about principles of wiring, all how, particularly neurons going to others to create different forms of dynamics. And so we're going to need, we're going to know a lot about the sort of canonical circuitry of the brain, but we're not going to know a lot about particularly how that maps back up into these higher cognitive sort of ideas, not least because we have to do all our experiments in animals, which we can't get them to internally tell us that I had a memory of this, we have to infer from, you know, she's a standard activity prefrontal cortex that that is a memory because if we perturb it, then it, memory seems to go away, but that is, but it's us this labeling it a memory rather than whatever the brain is using as. That's a lot of levels. It's a, it's just, this is why I do what I do. It's just fun to think about and talk about. So all right, let's talk about the book. And I'm, the questions that I have for you come from generally from kind of later, mid to later in the book, and I'm skipping over like the, a bunch of the introductory stuff that talks about, you know, the generations of spikes and, you know, the, the role of, you know, non-spiking activity in the retina. I mean, it literally goes from, you know, photons, the book goes from photons in the environment hitting the retina, how that's processed and, and then traverses through the entire, mostly visual areas of the brain to a motor action, as you've already said here. But the first thing maybe I want to ask you about is, and I picked out a few things just specifically so that we could kind of compare them like you do in the book to what we know and what is being used in, in, in AI. And the first of those are just the randomness of, of spiking. So it's well established now that spiking is a random process and you ask in the book, how that can be the case. And one of the things that you point out is the excitation inhibition balance coming into a spike as all the voltages are getting added up from the excitation and inhibition and to eventually produce a spike. So maybe you could just say a few words about the importance and the role of excitation and inhibition and maybe how it differs obviously from AI. Yeah, sure. So as I saw outlined in the book, this is one of the lovely sort of clear detective stories in neuroscience of how people uncover this idea of excitation and inhibition balance, as you say, it long been observed that the spikes coming out of individual quadricorn neurons appear to be essentially random, that the gaps between them were, gap following a single spike was either short or long and it seemed to have no relationship to the previously whether short or long. So essentially, it appears to be a random series of spikes in time. And that when you put a model of neurons, that seems to be impossible because when you give a model neural lots of inputs, its outputs are really regular no matter how random it's inputs and explain why in the book is a nice diagram which shows this much more. It's really like in saying words. So the solution, one of the solutions, so many solutions to this problem, how you get irregular outputs from a single neuron. And one of the key ones that were hit on early, I think there was a review paper by Mike Shadlin and Newsom who flew with the idea in words and it was put into models to test it was that if you have a neuron whose excitation inputs and inputs, inhibitory inputs were basically canceling each other on average. Then the voltage of the neuron itself, as those inputs kept bombarding it, they would be all random but the total amount of inhibition excitation would be roughly the same. So then it would be fluctuating back and forth as it up when it got excitation and down as it got some inhibition. And eventually they would be by at random a tiny little burst of excitation would suddenly overcome all the inhibition and a spike would appear cause it the neuron, the neuron voltage would reach its threshold for making a spike. And that creates this irregular output spike train. So it's nice and easy to show that that would happen in a single model neuron and the real breakthrough was showing that if you put neurons in a network and you basically balanced out the number and strength of the excitation inhibition connections within the network. So on average it was balanced across the network. Then the network was self-generated irregular spike trains and it would do that because essentially it was, it was on a massive feedback system for itself. So we had too much excitation coming out of the excitation neurons but then that means they would drive the inhibitory neurons to create too much inhibition which would drive them down. And so the excitation inhibition would bound itself out. So that means that in the brain we have this apparent, pickening cortex, we have this beautiful balancing act going on which means that the, if it's unseen, then that the outputs of each individual neuron are not heavily driven by a particular set of inputs that they are driven instead by this ongoing barrage coming into them from both excitation and inhibitory sources. So they don't necessarily reflect, obviously, something that a clear labeled line which says this input means, you know, a line of 90, 180 degrees or a particular tone right now because the timing of that spike isn't under the only under the control of whatever the external stimulus is. A couple of important things also to note, I mean it's not like there's a positive, there's an electron or an anti-electron or something, you know, it's not like excitation input and inhibitory input are of the same ilk, right? So they're very different because you have these, you have a lot more excitatory input numbers, right, of axons impinging on the dendrites. But and fewer inhibitory inputs, but those inhibitory inputs are firing higher for one thing. So that's one way that they balance it. And also they're, they're the strength of their effectiveness efficacy is higher than the excitatory input. So this gives rise to this ability to have a lot of these voltage fluctuations within that balance, overall averaged balanced scheme, correct? Right. So looking at a sort of site and network level, yes, that network level you have many more excitatory neurons and you have inhibitory neurons. So in cortex, the balance in a mouse cortex, the balance is roughly sort of, AC85% excitatory neurons, 15% inhibitory neurons. And as you say, then they are, those inhibitory neurons are giving out many more connections, so they connect to many more, sort of many, many more neurons, which means that individual neurons receive quite a number of inhibitory inputs. And even though they're then maybe fewer in number, the excitatory ones, they are stronger. And then they also inhibitory inputs struggling with this phrase inhibitory inputs tend to arrive at the neurons body, whereas excitatory inputs tend to arrive in the up in the dendrites. So which means that they are able to have a more powerful effect on directly on the neurons voltage than are, this many thousands of excitatory inputs are in the dendrites. But then there also means that, locally in science at the dendrites, then that, so for their example, there is a particular neuron in cortex, which projects its axon just up into the far top of the dendrites of the pyramidal cells in cortex. And that appears to be specifically to be able to regulate the excitatory inputs in particular parts of the dendrite. So that you have this tug of war between ambition and excitation happening in local little parts of the dendrite as well as globally controlling the output of the neuron. And one reason you might want to do that is because we know that when up in these far reaches of these dendrites, when you get these big clusters of excitatory inputs firing together, you get this kind of spike-like thing happening in the dendrite, this big non-linear jump of the dendrite, which then flows rapidly down to the body and can itself cause a spike to be sent down the axon. So you want inhibition up there to be controlling this process too, which appears to be doing. Then that means that, obviously, played out then across the very big dendritic branches of one of the pyramidal cells, you have these lots of little different regions which are essentially, I think independently with these independent excitation inhibition balance going on in each of them, potentially sending spikes down to the neuron's body. So you end up with this really complex computational device inside this individual neuron of which you read out spike. It's just a read out obvious, very complicated interaction within the dendrites between excitation inhibition. I think we talked about this the first time you're on the show that in that sense, every neuron is like a little neural network in itself. Yeah, so there is actually, so there is some lovely modeling work showing that, yeah, when you build a single model neuron, that then has a lot of compartments, we discussed before where each of these compartments gets a little inhibition excitation to it. It's formally equivalent in many respects to a two-layer neural network. And indeed, we can show if you've got any of those kind of non-linearities, this kind of spike that appears in the dendrite with many excitatory inputs. And once you have one bit of dendrite that has that, and other linear bits of dendrite, you open up this whole class of computable functions that was impossible. If you have purely sort of an add-up and some device happening. And also, yes, and namely to do that in a really compact space, so that a single neuron also is a very tiny thing. So being able to do all this computation locally, means you don't have to have all this computation spread out amongst thousands of neurons and hidden layers in a neural network. So then like zooming out even on the inter-neural level between neurons and among populations at the neuron level, this excitation and inhibition property in mass gives rise to, and because of the different types of excitation and inhibition, the different properties of those inputs gives rise magically, oh, I said magically, emergently, let's take the magic away, to self-organization and ongoing dynamics within a population. So it's this nice property that these lower-level characteristics give rise to. Yeah, right. So as we sort of touched on then, because this is, so this is a single neuron, property of this excitation inhibition balance explains the irregularity of a single neuron. And then when you take these excitatory neurons and inhibitor neurons and wired them together in a network, because then they provide each other's excitation in inhibition, they are then self-balancing. So you over-quite a robust range of ways of wiring up these networks, you will always get this irregular spiking, you know, you'll get this ability to, for inhibition to a balanced excitation. One thing though, touch upon in your question, what we lack a good knowledge of in neurosciences, how this complex, dendritic computation actually contributes to the network level dynamics, because almost no one looking at that level partly because building the models that are complex for the individual neurons and then wiring them into a network is incredibly computational expensive, you have to have access to some kind of, you know, IBM, Blue Jean scale, super computers are run them properly. So people have built models of cortex on their scale, of course, so you've got the, you know, the blue brain projects. And similarly, there's a team in Sweden who also have Blue Jean super computer that are building cortical models. But as a, yeah, no one's really done a good exploration of what dendritic computation adds to the network dynamics. These two things still fail extremely fairly separately. And what's your, what's your sense is that a subtle context sort of information that it adds, what, what if you had to guess? It's a good question. Yeah. We've got to be speculative sometimes on the show. I'm not sure actually, I'm really not sure what it has. So we've done, we've done a bunch of work on what the individual neuron computations would be with these non-linear dendritines. And then when you scale it up, essentially it means that I think one of the things that's going to let you to do is allow to access by putting together relative, so relatively simple neurons that be able to do something non-linear that you can open up this whole class of function that you can compute that you would otherwise need extremely complex dense neural network to, to like multiplexing sort of. Yeah, either that. Yeah, either that you're allowing it to, to instantiate many functions at once in the same network. All that in a simple costly because most brain networks are recurrent in some way. That what you're laying, it do is, is, is allowing some kind of recurrent computations, whether it's passing back through these functions over and over again. Historical context. Yeah. Yeah, it's built very complex, instead of just a simple way, equals f of x function, something that's really deeply recursive is being built by being passed constantly through these dendrites and spat back hours of spike. Yeah, I mean, there's just so much complexity and so much fine detail and it's hard to know, what's important, as you move up in scale, in size, right, and in computation. Does any of this matter for AI, for instance, or all these types of things, and we're going to talk about plenty more, things that we can just abstract away? I mean, there are people like Blake Richards and many others who are working on using the variation in didritic compartments and different electrical properties of the different types of dendrites, for instance, to be able to compute things like feedback information, predictive information, and actually trying to use these models that you say are super, super compute-heavy to make anything large. Whereas when you think of like a neural network, you think it's just a dumb little node that adds, puts a little sigmoidal function and spits something out, and it's just there's such a vast difference functionally between that and the complexity of neurons. And when you get down to that sub-neuron level, the location of the dendrite when an input's coming in, how close it is to other inputs, whether it's excitation, inhibition, and all that variety that can happen, it's mind-boggling. Does it matter for building intelligence systems? That's a key question, isn't it? So one point of view of course is that maybe it doesn't. So one point of view of why deep neural networks are so successful is because they essentially replicate by just adding many, many, many neural layers, the process that's happening in a handful of neurons. So we only need, for our object recognition system only needs essentially four layers of neurons, maybe five depending on which object you're recognizing, put a deep neural network, 15, 20, however many layers you're putting on that convolutional network at the start. One point of view would be that that's simply that really deep network is just replicating the many processes of processing that in the brain are collapsed into a handful of neurons, each neuron is doing a job of multiple layers in that network. But as you say, a neural network is pretty static device, right? You just add up its inputs, you split it out of a sigmoid, you could put it in the next layer, you possibly the next layer, which lacks any sense of timing. So much for we know about how the brain computer level weighs is about timing. So a lot of the dangerous computation stuff is about how gendered computation can be used to get really specific timing effects. So for example, giving example on the book of Quintin's detection of the sound coming from the two ears, the spikes arriving at a particular neuron somewhere in the midbrain will happen to arrive at the same time because they're delayed with both of each other. They arrive at the same time in the dendrite, they call that neuron spike, and that spike means that there is something at say 20 degrees in front of you in the world because that neuron, particularly neuron stands for when those two spikes arrive on the left and right ear together, that means that things at the 20 is at 20 degrees and those sounds will arrive slightly far apart in the two ears. So that and the more general ideas of Quintin's detection in cortex of just this when we want to know the sequence of events or bind things together that when spikes arrive in the dendrites, when they arrive one after the other, immediately or we arrive together, that gives different information. So obviously a lot of the computations in the dendrites, particularly in the synapses as well, with things like short term deplacicity where it matters what all of the spikes come in as whether you get an increase or a decrease in the strength of the synapse. There seems to be about time effects and they're all completely missing from from AI. I think that's going to be a recurring problem in AI, just guessing just timing at all levels. I mean, just zooming out as far as you can, you can imagine if you're interacting with a person or an AI or something and they move a thousand times slower than that, you wouldn't call that intelligent behavior. So there's some threshold right where timing is important for us even to consider something, an intelligent process. I mean, if you zoom way out, you could look at the earth itself as an intelligent process, but it's going way too damn slow for us to consider it. What we would consider an intelligent process, right? And like you said, I mean, time just gets completely ignored in the vast, vast majority of AI because it is a static thing where it's just a functional thing, input function output. And it does the thing, it categorizes the thing, but it doesn't matter how fast it does it. It doesn't matter the timing that it does anything with respect to anything else. You know, if you want to talk about the generalizability, you would need all these processes working together in harmony dynamically. I don't know. Now I'm just off the deep end here, but this goes also back to another thing that you write about in your book that seems at odds with our intuition is that spiking is a relatively rare phenomenon, right? So you have, you talk about in the book how, you know, you have like 10 to the fourth inputs on average to every neuron, right? Or yeah. And given all that input, you would imagine the neuron would just be going wild all the time. But in fact, spikes are rare. And you know, it's not that signals aren't being delivered to the neuron. It's just that those signals are being averaged out. And or for whatever reason, don't add up to a spike. It takes a lot to add up to a spike in a neuron. And you call that spike failure in the book. And you talk about why that might be good. You give a few reasons. Would you mind just talking about some of the reasons why it might be good for a spike to fail? Yeah, sure. That was the channel that had most fun writing. I'd say, oh, yeah. I'm a long been fascinated by spike failure because it's such a paradoxical idea. So particularly this synaptic failure problem where most synapses in cortex and hippocampus and meagre alone, so on, there was a failure rate of about an average 75%. So every every every spike that turns up with those synapses about 75% of them will not cause a response on the other side. They'll simply not release the physical, the bank is transmitted won't cover across the other side. They won't knock on the other side. No voltage will be transmitted at all. Some of some of those estimates you talk about get up to 90% failure rate. Yeah, so there are reports of yeah, in hippocampus reports, I'm up to 90%. So it's only 10% of spikes are doing anything. What a waste. Exactly. And as you say, it appears to be nonsense because spikes are extremely, medically extremely expensive to produce. They, you know, the are the estimates of how much energy they use on your moment to moment energy basis is about in your brain. It's about about 46% of all the energy that your neurons are using is just to produce the spikes. So I always use all that energy and have them fail is kind of bizarre. These kind of paradoxes fascinate theorists right away. What is the brain doing this to itself? There must be a fabulously good reason for it. Yeah, give me a couple examples. So the reason why it's good for for the individual neuron to have its inputs fail. So living in backs, they have this idea that what is the each neuron is trying to do is it's trying to actually make the most efficient use of its own energy when it's producing its spikes. So its output, the long xx one has a maximum rate, it can send information. So, you know, typically what we call an active corner on when we used to just lower any single electrode in blind into cortex and find the neuron, that'll be firing at 10 spikes per second. Yeah, exactly. And that's considered a really active neuron. But of course, that's still a fairly low rate of information that's being transmitted by that single neuron. If all of its inputs were active, all those terms of four inputs were active. Then the input rate on the information rate on the inputs would be three or four orders of magnitude bigger than it could output. So all this information coming in is just wasted. So their idea is that an ecstatic failure is there specifically so that matches the output information rate to the input information rate. So the input is throttled back to the point where it can make absolute maximum use of its output without wasting all this energy on having all these voltage go up and down because these inputs that it has and no way of making use of. So that's one really neat sort of way of thinking about static failure that ties together what a neuron is trying to achieve, which is to maximize its own output while making sure it spends it optimizes energy usage at the same time. And then there's, then until I see also speculating on a few reasons that I haven't been, haven't been as well sort of research yet about why it might be good for the brain as a whole authentic failure to be in play. So one of those might be to touch on briefly about methods of generalizing. So we know that when we are learning stuff, when we're learning, like with neural networks are learning an image classification task, showing many, many, many images, and any problem with learning from many examples is overfitting. So we're going to just learn about some kind of detail and the thing that we're looking at which isn't relevant to the actual, what we're supposed to be looking at. Like horses are always in fields and we might categorize it as a horse if there's a field in the background or something. Exactly. Yeah. Yeah. So of course, there are various solutions to that in the neural network field. One of which is Drop Connect, which is basically the idea of, of course, that every time you show a batch of images, you have dropped out a set of the connections in that network. So every batch, you essentially, being shown to a different subset of the network you started with as long as being trained on different network. And then when you tested, of course, you show it in new picture, you've put all those connections back in and it hasn't overloan because you haven't had the same connections learning over and over again that, you know, as you say, it's always a field. So it's going to pick up more, hopefully more the idea of horse. And Drop Connect basically just is synaptic failure. It's this, so the brain, it is a fairly logical idea then that the brain, one reason for the brain having synaptic failure then is that it enables it to learn without, it's about to generalize well without overfitting to particular details that, obviously every time you, images are flowing in, you're not having, even to the connections between particular neurons because of the failure, are not rock solid. They're not always being the same ones firing a spike down from this neuron to this neuron to this neuron to this neuron. They're constantly dropping out at random while the world is going past you. So it would seem a logical idea to pursue that one of the reasons the brain has the synaptic failure is so that it's better at generalizing than it was this fixed series of connections that always fired every time you gave it an input. And that's one one way in that, and we'll get on in a little bit to the topic of noise in the brain and variability. But I mean, that is essentially variability playing a positive role then if the intrinsic variability of sometimes allowing spikes to pass through and sometimes not. I mean, it doesn't work out, it doesn't work just like drop out in real brains. You call it drop connect and I don't think I'd ever heard that. It's the same as drop out, right? So there's two versions, but it's a drop out, it's literally dropping the nodes. Yeah. But drop connect is dropping out individual weights instead. So you're leaving all the nodes, all the nodes, units intact, but they're dropping out the weights. So all the nodes, all the neurons are still on, but you're just removing the connections between some subset, some subset of them. Yeah. So basically you put a mask over a random mask over the weights, other weight matrix basically, and you just randomize the mask every sort of batch. And this is those being trained on a different network each time, but of course it's actually the same one, they connected together ultimately underneath. So you take the mask off and it has learned the image classification you've given it, but not hopefully not overgeneralized to the features that are specific that are not the thing that's supposed to be learning. Yeah. So it's one of the many ways that in AI, the term is called regularization where these different various strategies do not overfit, to not categorize something as a horse just because there's a field in the background. So those are a few reasons why spiking is rare. Is there anything to add? I mean, do we miss anything about the rarity of spiking and spike failure being a good thing there? So the rarity of spiking, well, it ties into the whole dark neuron section. Let's do it. Let's do it. Let's talk about dark neurons, which is one of them, to me, like one of the more exciting things that you write about in the book. Yeah. So what are dark neurons? And why do brains have them? Yep. So dark neurons are the fact that in any given moment in time, most neurons in your brain won't be firing a spike. So although we can say that in a primate brain, the neurons in your cortex on average fire one spike per second, actually, mostly spikes are fired by a handful of neurons. So when we do some detailed recordings, we can see that about half of all spikes that are fired are fired by 10% of the neurons. That's, hang on. Let's just pause there because that's crazy, right? So I just want to pause just to just to make sure that sinks in because, you know, as someone who recorded neurons in the brain from an awake behaving primate, right? And you talk about this in your book too, you know, as you drive an electrode down and you're listening and you're listening and you hear some neurons, you hear lots of neurons, but the vast majority of things around that electrode aren't making any noise because they're not spiking. And so just say that number again, 10 to 10% number, just to reiterate. Okay, well, study is showing, yeah. So we got 50% of all spikes are sent by 10% of the neurons. Yeah, it's astounding. It is. And as I know in the book, it's kind of been hiding in plain sight for a while. So the whole reason that that sort of single electrode recording in your primate works is literally because the neurons are silent. If they weren't silent, you wouldn't be able to see anything because yeah, exactly. But just be the, the electrical signals would be so overwhelming. You wouldn't see the individual spike shape. You just see this constant, this massive waveform, which would be the supersposition of all these of the actual potentials from hundreds of neurons. There would be nothing there to be able to say, okay, this spike is being caused by that stimulus in the world, that's, you know, that it's memory or it's, or it's seeing a, you know, seeing a picture or grating or whatever. So yeah, I mean, it'd be, they'd worked out in the, by the sort of late 70s that the recording electrode of a, recording radius of a single sharp electrode shooting compass about 200 neurons. And yet you were picking up what to. So something weird is going on. And then fast forward to the point where we finally got calcium imaging, working properly in, in vertebrates. And then you could video, literally video the neurons just, you've stained them further to know that they're there. You're looking at them and they're not fluctuating. The dye isn't going up and down when you're shining light on it. They're not active at all. So you could just then go and account the fact that there was all these hundreds of neurons there and only handful of them were clearly flashing in your, in your video, suggesting they were, they were active. So yeah, so that's they, so it's become apparent that they are very common. There's these neurons are doing almost nothing. So when I say it's almost nothing, I mean that, you know, they're firing less than one spike every 10 seconds. It's very hard to get a handle on how much nothing they're doing, right? Because as I say, because most of the way we, we can find them is by, by looking at the calcium signals, the neurons produce because calcium is not really directly related to, you know, it's an indirect readout spiking. Yeah. So it's harder to get a handle on quite how quiet the neurons are because if you fire, you know, one spike every few minutes, you're probably not going to pick it up. Yeah. So the calcium, the calcium signaling doesn't, you don't, you don't get individual spikes. I mean, it's in these like windows where you can tell that there's been some activity from the neuron, but you can't tell precisely at the millisecond time scale when a spike happened and how many happened, etc. Yes, you can be pretty confident when it hasn't, hasn't been particularly active in the center of a handful of spikes. But you say, you can't get this resolution. So what reason I want to buy about this in the book is I don't, because I feel that neuroscience as a whole, particularly theoretical neuroscience, hasn't grappled with this problem at all. Because we have all these, for example, in the, in the, we have these beautiful models of what happens in the, in area V1, or the first part of the vision system, we have all these beautiful models of how the inputs coming from the retina by the phantomis V1 and their process through this sort of linear non-linear process and they to, to be a, the spikes that come out are reflective of this property and this property and this property. But of course, they only correspond to then the handful of neurons that actually have that activity, which is literally a handful of neurons. This is kind of an old problem too. I mean, this is like all-housing, classic paper is like what, what is V1 doing, right? Because we have such a small sample of, of the actual neurons in V1 that are doing what we want them to be doing and we can, that we can talk about what they're doing. Right. Exactly. So he was, so yeah, that old house and paper on what is the other 85% of V1 doing. Yeah. Yeah. Yeah. So I touched on that little bit because he was at this, this point where it still wasn't quite, so he was running from a slightly different angle where it wasn't quite clear that there were so many silent neurons. He was pointing out that when we record in, we call it in V1, then many of the neurons that you record from, you hear this tick, tick, tick, tick, as you say over the speaker, you hear on this telescope. When you play them the stimulus, they don't respond to that stimulus. They're active, but they're not looking at it, apparently, even though they're in V1. Yeah. And those are the neurons that don't go into the paper that you publish because they're not doing the interesting thing that you're looking for. Exactly. So in the book, I call them the type 2 dark neuron. So you have these dark neurons, which are literally neurons that don't fire at all and all this, this sort of neurons are the set of neurons that are active, but are dark to the outside world. They don't seem to care about anything that's happening in the outside world. They don't care, respond to the inputs, they don't respond to the outputs, they're just just good. So they take up another of that 10% of neurons that are firing half spikes. We're not quite sure how many of that 10% belong to this category of neurons that are, they don't care about the outside world. There's, you say that the old 1000 papers suggest actually it's most of the neurons don't care about the outside world, even when they're active. That's the problem here is putting out. So it's a stacked problem. So we have for these, all these neurons are barely fired at all and all the neurons that do fire. Most of them don't seem to be responding to the stuff that we're interested in showing the animal or making the animal do. So we have great theories for about what does that leave us with? 3% of neurons? I think so if you got. Great theory is what they're doing. You only use 3% of your brain and I think that's the new. Yeah. Great. Yeah, exactly. So yeah, there's, as I said, one reason I want to write about this in the book was to really point out that there's this terrific whole area to explore that we don't yet. And there are good reasons to think the way, I mean there are sort of prosaic reasons why these all these neurons want to just simply energy as I said. The creating spikes is really expensive as it's processing them. And also, as we know, the brain is a really methodically expensive organ right? So the classic numbers are about 20% of your resting metabolic rate is just your brain, which is pretty massive, but some in only ways are handful of kilograms. Mine's only about 5%. That's what people tell me. I don't know. Quite just exceptionally efficient. But so one of the reasons they may simply exist is because one of the reasons that we maybe have this dark neuron problem is not that they are always really inactive is that just that we never give them anything interesting to do. So this is this simple segment is this dull world argument. And when we put animals in a lab, we ask them to look at really, you know, the best we do in vision experiments is we show them what we consider a video of natural, some sort of natural image, a video, which often turns out to be a film, the matrix often as a choice. So that's an yeah, we used we used the Hobbit movies, the Tolkien movies. Well, at least that's got trees and leaves and mountains and stuff. It's got lots, yeah, they love it. The monkeys loved it. Yeah. That's actually nature. Yeah. I mean, so like the Schneiderland biolic papers where they record the retina of the salamander and showing it matrix movies as though that was the thing the salamander would ever see. They should have showed inner space. Right. Yeah. Yeah. But I mean, that that harkens back. I mean, because you were talking about there's this big push right now for ecologically, ethnologically valid tasks. And that speaks to one potential role of the dark neurons that they just don't care about what we're asking them to do. Right. So there's a real real possibility that we're going to now people have this big push for recording a lot of spontaneous behavior in particular. And I know some people are working hard on sort of fairly naturalistic vision experiments. So I call it Cardiff Storky in Manchester, who's who's working hard on setting up recordings from retina in the end of the phantomus from freely moving mice in a really rich environment to try and see how what vision processing like looks like when they're actually controlling their own vision for once. And wondering if that's going to be radically different from I mean, but the slight caveat of course is vision in mice so they don't use their rights much. But mice in the animal visual. Yeah, that's true. Just to throw another little fact in there. I mean, there's a push these days as well. So going back to my graduate school days in non-human primates, I mean, the other thing is that you know, they're sitting in a dark room. They're looking at a screen showing them pictures or just dots on a screen, right? And having them make decisions. But they're also only moving their eyes. You actually, you know, the classically you fix their head in place so that you can control for head movements. So even that even even their own behavior is unnatural in that respect. And because they can't freely move their head from side to side. So I mean, there's all sorts of caveats to reducing as much and controlling for as much as possible in the lab and relating it to brain function. I glossed over something about the dark neurons. So what we didn't talk about was when you include the dark neurons in your like in a population decoding scheme when you're trying to decode what information is out in the world just by recording the neurons. If you record from only the neurons that you can hear with your electrode going down, you can decode, you know, fairly well. But if you include the dark neurons, all of a sudden that the decoding becomes much more accurate. The point that you make in the book is pointing toward the importance of the population coding scheme relative to like the single neuron or a tiny ensemble of neurons. And as really the population is where the important information is. Yeah, indeed. So that's, but yeah, that's been a, that's been a, I'll see a bit of our research work to has been looking at this. And we talked about this last time as well, you know, the rats wandering back to in the maze, wandering back to start the maze over. That's right. Yeah. Yeah. Yeah. Yes, you're right. This is general idea that we can extract from a population of neurons, not just farmer information, then there is an individual neurons, but we can extract from neurons that don't appear to be individually responding to the outside world. We can perfect where they extract actually the sort of what happening in the world. So an example would just talk about of the rats walking back into maze. We could extract from the activity in prefrontal cortex whether that rats had just been rewarded, whether it had just chosen to go left or right in the maze. And whether at the end of the arm, it was, it was visited where the light was on or off. So it's having all this this activity in prefrontal cortex is remembering these things. And even in, when we record, looked at the populations in which individual neurons had no apparent response to any of these properties, you could still decode these things perfectly well. In fact, some cases 100% accurately. And you see this, this player and various other areas of the, what the brain two people have started looking at this, this question of what can we extract from? Groups and neurons who individually seem to show no tuning whatsoever for the outside world. And they're finding, indeed, we can decode quite, quite rich properties of the world from them. Yes, it gets a really suggesting that it is the, the level of information representation we're interested in the brain is really is the population neurons. And then sort of extreme view of these then that the single neuron tuning is a pure epiphenomenon. It just so happens that we, you have, the brain has so many neurons that you're going to find some neurons that have perfect tuning to things that you show it. But they aren't actually the neurons that are specifically used for this task. It is, they just happen to have the tuning that is you're looking for. One of the things that you again are looking back to this special hundredth episode thing. I think it was your answer to the question what's holding us back in neuroscience and AI. And your answer was looking at averaging, you know, as opposed to like sort of single trial neural activity, that averaging tends to mask things. And this kind of is related to this population idea as well. So you know, like cognitive functions, they're emergent phenomenon, emergent properties of this myriad, you know, connections and activity. And there's these, this multiplexing, there's all this noise. It's hopelessly complex, you know. And like you said, there's all these dark neurons and really super low firing and spontaneous neural activity. And one interpretation of that is that single units then are meaningless as far as looking at them to extract any information about what's going on. And maybe they're epiphenomenal, as you just said. But on the other hand, the other way to look at it is, isn't it insane that we can, that we can actually see any modulation at the single neuron level. And often that modulation is striking him. So I, you know, my background is, I did a lot of work in frontal eye field where you had these single neurons that right before the eye moves in a certain direction into its response field, you can hear it and it just zips up to a threshold. And this is a single neuron pretty reliably that does this trial in and trial out. And you know, there is variation. And you do average to tell a story and to compare between trial conditions. But in that case, and there are exceptions to this as well. I mean, you're averaging pretty similar rampups like each time. And it is striking then that's just another way to look at it. I think is just to be impressed with how much information you actually can sometimes get just from single neurons. Yeah, you're making a good point that because there's a lot of what we particularly we talk about, but coding we often think about the information coming in. So it's only about senses, of course. Yeah. But then we take a view. And as I, you know, I work on motor systems too. I've done a lot of work, you know, much of my career work on Bayes of Ganglia, look to work on the brain stem control of the spine. So when you work backwards from the motor neurons in the spine, obviously the motor neurons are sort of your classic these are the neurons that fire spikes, then you move a muscle. So that neuron fires that muscle is going to contract. That's record those. Yeah. Yeah. It's a really fairly beautifully simple. Indeed, it was spinal motor neurons that gave us the sigmoid in the first place for the AI, AI networks. This though is them who have the sigmoid or discharge curve and cortical neurons don't. They have a more like a nonlinear power law thing. And as you say, as you work, you can work backwards from the spinal motor neurons, or indeed from the neurons in the brain stem, controlling the eye muscles that are going to move the eyes. And those neurons, of course, are going to be birthfiring. That's going to make that the muscles contract here to move the eye to one side. And you work backwards from them to where they're getting their inputs from. So somewhere else in this case in the brain stem, in the eye movement, especially from the superior cliculus. And you can work backwards and up to the front of the live fields. And you can see, okay, I can hold this whole chain where I can just get spikes here, spikes here, spikes here, and it moves. Yeah. Yeah. It's this beautiful causal chain. So moving, you can see this beautiful causal chain. How about the other end, you see this just mess. Well, that causal chain does get messier and messier as you as you see. Yes. So more and more things are controlling the eye movement eventually. Because there is obviously there's some work on population coding in spinal cord. So Roonberg in particular is doing a bunch of work in turtles recording hundreds of neurons simultaneously in the spinal cord of turtles. So that they really is quite a bit of population coding going on in spinal cord too. The dynamics are quite complex that you've got balance activity in the spinal cord. You've got these long tail distributions of activity just as you see in cortex because you have these recurrent circuits with the intonorons and the motor neurons. So there may be, as it will turn out, motor neurons may be the only one that you can go, when that fires, that means something. And backwards from there, it all gives a bit a bit messy. That would be unfortunate. But it would. I mean, there's all that work with the in motor cortex, right? About using like dynamical state spaces to infer like when a movement is actually being coded. So it's not a ramp up. It's taking the whole population, looking at the reduction of the variance toward as you get closer and closer to a movement. And then the variance is eventually just quashed and you make a movement or whatever. So interesting different ways to look at it. So maybe we could get on to, I don't know how much you want to talk about sort of what I consider like the big idea and the brain that you talk about related to spontaneity and evoked from sensory stimulation, neural activity versus this intrinsic, spontaneous activity. And the role it could play in predictive coding. Do you want to wax poetic about that for a moment? Yeah, sure. We're touching that for a little bit. Yes. So in the books, why I, so we get to the end of the journey through the brain. And then I asked the reader to reflect back on the journey we just took. It's important though that your character gets the cookie, right? So yeah, that's the essence. Exactly. Yeah. So you win. So you got it. So given you, your brain realizes how energy, much energy needs to carry on. So hopefully it's going to take the cookie to give it that. Keep up with that 20% burn rate in the meeting. So you got the cookie, sit back, satisfy it as the star of the book and the and reader simultaneously. And I don't invite you to then think back through the book and think that most of the times when I when we sort of landed on a neuron on a spike, that neuron was already spiking. I'd already just sent an if spike down. All we'd seen spikes come back past us the way we just come. And in fact, it is most of the spikes in the brain are actually spontaneously created rather than created by the outside world. Either because there are particular classes and neurons that are able to generate their own spikes when they're in put whatsoever. It's kind of peacemaking neurons, which exists mostly in sort of the mid brain and the brain stem. And then in cortex, you have these massive recurrent networks, which is full of feedback loops. So you give it enough input on that activity will be sustained forever as it reverberates around and around and around in these feedback loops. So it appears that, yeah, most of the spikes that we come across are not created by in this case, the world, the seeing the office in front of us seeing this cookie in a box on a desk in front of you. That's kind of the feed forward evoke stuff that's appearing and that's having an effect, but it's not clear what effect that is. So the idea floating in the book is really that what most of the spontaneous spikes are doing is they're solving the problem that spikes for all their wanderfulness are kind of a slow way of processing information. And actually, we need to make decisions fast because making decisions slowly, particularly in most niches that animals live in, is a way of getting eaten or failing to survive long enough to reproduce. So what I'm arguing in the book is that what the spontaneous activity is therefore is to solve this speed limit problem by essentially is predicting what the incoming spikes should already be. So there also exists this sort of predictive processing accounts of the brain, which are fairly sort of high level ideas of what the brain should be doing, sort of normative models of in an ideal world the brain should be doing this. But we look going down into the details of the spikes. We can see that there's this rich, dynamical brain waiting there to be to act as this predictive machine. So that these spontaneous spikes then are very, very, very, very random. So it gives your ease examples of is always always this vision are then they're predicting what should be coming from the from the retina. So the spikes, the retina is sending up into the brain are standing for various edges and corners and parts of the visual world, elementary elements of the world we can see before us. So in the in the book example, as edges of the box, the top of the cookie, all those edges of the desk, that kind of thing. But of course, your eye has already swept past this scene. So your brain already has a really good idea of what it is that you're looking at. So it's been able to predict that there's an object on there which has got a cookie shape at box shape. And that information is being fed backwards from the object areas in spikes towards the early visual system bits to meet up with the spikes coming from the retina. So essentially to see whether they agree or disagree. And all you need to, all the information needs to come forward then just like in predictive processing counts is whether there is something is wrong with that view to change that view. And then that fits that sort of account conceptually if it's beautifully with the ideas, we take this sort of dynamic of view of the brain. The iris spontaneous activity is all this ongoing dynamics. Then all the incoming spikes can do is change those ongoing, ongoing dynamics. They can just prod those into a different shape. It can't really drive them forward into a completely, you know, entrain them to it to itself. It's not like a feed forward network. All they can do is just take this ongoing activity swathing around the brain and gently moving to a different trajectory. So from that point of view, it makes sense then that the information coming in from from your very senses all it's doing is prodding ongoing activity into a different state. It's this is wrong, this is wrong, it's wrong, prodded up to something which is a bit closer to what's going on in the world. And it's the difference that makes a difference. Yes. Yeah. It struck me that, or it strikes me just now. I mean, there, so there are ways to, you know, go into sensory deprivation so that you're not getting incoming light, incoming visual signals. I mean, you still get sort of proprioceptive signals from your body, I suppose, but what is the way to cut off the predictive aspect? I mean, I, you know, the top down aspect, there aren't experimental conditions where you can only evoke sensory activity. I mean, that's essentially turning off your brain if the predictive processing account is right and that there just wouldn't be a condition in which you could do that, I suppose. Yeah, I think, I think you're right. Yes. As you say, there is, there are, I'll see, there are a number of experimental power downs, which try and replicate that sort of sensory deprivation experience, right? There's try and try and make, uh, make normal humans hallucinate stuff. So there's like, all its re-illucination power downs where you play a tone that's barely the perception that they're hearing. You also regulated it to the tone is at an abnormally, that they can barely hear, flash a light at the same time as that tone, and then keep flashing the light. Occasionally, you don't actually play the tone and ask them to report when the tone is being played and obviously normal people are happy to report hearing the tone a whole bunch of times that they was never there. So they're able to, you know, you can induce all its re-illucinations because they're, they're happily predicting that the tone is there when it is not. So they're, you know, something in their brain that said, yeah, there was a sound. And actually, there has been no feed forward input whatsoever. But you're right, it would be somewhat more complicated to, to shut off the backward flowing information. Although one would say given the rate that neuroscience is, systems are a science in, you know, invertebrates is going at the moment, one wouldn't say it's never going to be impossible. One could imagine that if we end up roughly agreeing this on this view of, if we're doing object recognition from top-down sort of predict processes in count, and we'd have a fairly good idea of wherein the sort of temporal lobe particular object types are represented with a sufficiently powerful, objeionetic approach. You could, in theory, shut that off that top level and see whether that's going to expect the perception of, and particularly the generalization of those kind of objects, because obviously the idea being that you're feeding back, whether it's exactly that object you're looking, you think you're looking at. So say you were, maybe it was something like, I say you would keep it going, suppose it was a simple square, right? And so that you're expecting that part of the brain to be able to, and if predicting squares, you shut off that bit and, but show it something which is like a rectangle or some smooth square, something which is, your brain would say top down is not what the thing you've been looking at, but top up has many properties of the square, right? So maybe your decision there would be there that's definitely a square when the actual response is supposed to be, it's not a square because it's morphed from the shape that it's, you've been shown over and over again. So maybe, yeah, maybe there will be a way of, that require a lot of hopefully predicting where, whether brain is storing this information, but a process like Jim DeCarlo is where he's training neural networks, deep neural networks to do, image recognition, finding the sort of responses, trans units on the top layer of that, of that neural network and finding those back in V4 and the temporal loop, then if you could then go into the, you know, they're in turn those off, specifically, then you would have a really powerful way of testing the backwards flow of information in the actual brain. Mark, this is, so this is like wrapping up our discussion about the book. Hopefully this is given people a lot of different flavors of what you talk about in the book. Obviously, there's a ton more in the book, but it's really enjoyable. The joy of your writing also is just very apparent and it makes for a really fun read. So nice job again on the book and I hope people check it out. Cool. Thank you very much, Paul. All right. I have just three questions, three simple questions to end up on here. One, and the background here is that you're really a theorist, I suppose, although you work with plenty of experimental data, but you do it in the guys of, in the, in the state of theory. And that's kind of, maybe that's irrelevant to the question, but the question is what's, what's one of the best scientific moments that you've had? Can you think that, you know, is there something that stands out in your mind? Yeah, there's been a few, let me think. So I want to go to explain. So actually one, yeah, so I, I now tell the world that what we do is kind of a neural data science, right? So we are, much of, we do is heavily theory informed. We're interested in testing theories using, you know, computational methods on data. Yeah, the byline of your lab on your lab website, I think is, we're a neural data lab. Is that what? Yeah, that's right. Yes. Yeah. Yeah. Yeah. Okay. So in that context, so we had one of my sort of favorite moments. So we have working quite a lot on the six-legged plizia with Bill Frost and Chicago and his graduate student, Angela Bruno. And they have this fantastic setup where they're doing this voltage-sensitive die imaging of the plizia's motor system. So they can record about 10% of the neurons in the motor system simultaneously and get it at spike resolution. So it has, it marries the, the advantages of calcium imaging where you can see each individual neurons, you know where they are. But it's, it's voltage imaging and it's at extremely high rates. So you can see all the individual spikes. You actually touch on this in the book a little bit. The idea of the voltage, the future of spike in recordings, yeah. Because I haven't spent, like, sort of, now I am all belonging to now, what, seven, eight years working on this system with them. The voltage imaging is obviously, it's just fantastic. I understand all the technical limitations, why it can't work so well in vertebrates. But in the last two years, it's been a dramatic improvement in the, in the sort of, both the genetically encoded and voltage sensors and the dyes that are used in vertebrates. To the extent there's been a raft of nature and science papers in the last, last year and a half, showing that this, this is really, it's coming away now. You can really record for, you know, a decent amount of time, individual neurons, voltage imaging, I mean, hip campus and cortex, in mice and see the individual spikes and with fingers crossed and a good, you know, squinting at it, you can just about see occasionally the sort of potentials of the inputs coming in to the stomach. So that's, so to me, that's going to be the big part of the future. Having worked on this data in vertebrate with the neurons are enormous so that it really works beautifully. You know, they really pans out on this trajectory, then we're going to be seeing what was imaging everywhere, I think. So yeah. So what was the moment? Yeah, it was, was, I mean, not just once more moment, we're just seeing this data for the first time, because it was just glorious going. So I have all these spikes, more of these neurons, and we know it's most of the motor systems, like, instead of seeing, you know, we were calling from cor, from pre-front from pre-front and for cortex and a rodent, which is just going to be pointing on, on, on, on, on, on, on, on, on, on, on, on, on, on, on, 1% of the neurons. Just in that region of pre-front and cortex, this is almost everything. I think the, the big moment of me was just, uh, really just, so I, really deep interested in dynamical systems. So we had this, I did this, piece of work, I was trying to figure out, how do I show particularly to the satisfaction of, you know, it just my, my collaborators who are experimentalists, they're not, you know, computation of people, how do I show it to their satisfaction? This is, this is an attractor, this, the system they're looking at, isn't attractive, making meaningful for them. So they go, it's just going, oh yeah, that's, you know, buzzword, that, um, that this actually is, and a useful thing to know, an useful thing to be able to understand about the system. So the whole piece of work we yesterday is recording, these multiple recordings from, from, recordings were from three separate recordings from each animal of this system being evoked to, to run away, to escape. Um, as I was able to show them that these population dynamics fitted all the, all the criteria of a, of an attractor, particularly a, a spiral attractor, sort of attractor where the dynamics start, always, um, soon from the same spontaneity, to the opposite, when it evoked, it goes up and falls, nicely onto the same trajectory each time, and that trajectory is a spiral. So it's, it's, its amplitude is, obviously, it's cycling around, but the amplitude is getting smaller and smaller and smaller as it goes on, kind of coming back to all the starting state, but they were quite guests there. And that was true, uh, in every recording. So no matter how noisy, particularly, I was beautiful, but it was no matter how noisy the recording was, how much like they went, you don't want this, it's a really bad one. It's a no-no give it to me, and I'll show you that it's there. So it can show them that even if recordings were, almost none of the neurons were, were firing this bit, with, with periodic burst, you could still see underneath, there, there was this spiral thing happening in the population, it was just, obviously, really noisy, because the preparation was, you know, whatever had gone wrong with it. And then, um, what's particularly lovely then is because they had, had three recordings from the same animal, you could show that they, they each, the three ended up on the same trajectory each time they were, converged, that were genuinely attracting. And by some beautiful accident, because of this, this motor system, it has in it various gap-roaching neurons, particularly one massive one, occasionally fires randomly, um, in this preparation, there are pauses in the ongoing activities. You can see sort of all these neurons are firing away, and there's these pauses with some other neuron-sart firing, and then it restarts again. Uh-huh. So you get to, over and over, get to see the same, same trajectories. Yeah, so you get to see some trajectories, but also, well, when it's neuron-fires, it means that it perturbs the activity away from what it was been doing. And I can show every time it perturbed it, it came right back to where it was supposed to be. Yeah. So it had, it made all the criteria of what you wanted attracted to be, and so it happened, you know, the spiral in the fire was the extra mentions instead of this 110, 200 neurons, whatever it was. And so yeah, so one, it was showing that, yes, I have shown to, even my satisfaction, which is quite hard, that this thing is genuinely a spiral attractor, this is a useful description of this system. That means that we understand how it works, because you poke it, and it jumps into this state, and that state is literally just driving the crawling. Every loop around this spiral is one, one full loop of the crawl, where the animal puts his head forward and pulls itself back behind, and then each, uh, and the parameters of the spiral, so essentially how wide each of the spiral is, how fast it goes around it, how quickly it goes back to the star, uh, properties of the crawl. So you could say, easily say, this, these three parameters are probably control three separate parts of the crawl. That's cool, which would be, you know, predictions to go and go and do in further experiments. So how were they affected by this? You finally got this across to them, like, that it was important, and did it sink in? Is that what the, yeah, I think, I think, yes, I think it sunk into it. They finally saw the value of the dynamical, uh, of the attractor as a, as a tool of explanation. Yes, there's this load-dimensional thing that we can describe it in quantitative details. So now that means that when we manipulate the system, we can describe those manipulations in terms of that attractor and make predictions about what direction it should move. That's a sort of example when we put neuromodulators in the system, we already know what those neuromodulators do to the behavior, so they must do a particular thing to the, to the dynamics of the spiral attractor. That's awesome. Sorry to bring you up, and then I'm going to take you down. What about some time that some failure that you've had or some, uh, you know, disillusionment that you've felt in your career, and then how you got through it. This is supposed to be, you know, too, for people struggling in their graduate school days, or, you know, this is supposed to let them feel like, oh, this, this happens to everybody. So that's why I'm asking you the question, not to bring you down. Okay. That was a great question. I'll see there've been a few. So there, particularly I'll talk about the PhD one because that's the one I'm obviously most relevant, I need to want to talk about most for the, for the, for the, I'll give talk to early career researchers to talk about this one. So my PhD was on building, um, competition models of the Bayes of Ganglia, and inside that circuit, there is, um, there's a feedback loop between two structures, one called sublimed nucleus, one called the glibous pelvis, and it's a, it's a, it's a negative feedback loop. So you have a one positive connection, one negative connection, you know, so you have this beautiful negative feedback loop, which is self-sustabilizing because only as soon as one, if you, uh, drive the excitation up, then you get more inhibition to it, drive turns down excitation, you drive the inhibition up, you move the excitation, the inhibition goes down. So it's this, yeah, nice. Some of the engineers love to see in a system as it's good, right? Yeah. Negative feedback loop. So I also built a bunch of different models with this, this thing in, and it was after my funding had run out on the, on the PhD, remember, it's a UK PhD, so it was three years of funding. I'd go on, I was supposed to be handing in, so I was writing up and some results and checking the code of when the main models I was using. Uh-oh. And I had discovered it in my code, I had hard coded the connection sign, and I had put them as both positive. So my model had this blowing up feedback excitation, feedback loop, and so there's negative feedback loop. So all the results of, I think, or two chapters had to go into bin of the five chapter thesis after a, you know, after my funding had finished. Simple coding error. Yeah. So a simple coding error that was now formed the basis of how I approach coding. Oh, I bet. How did you get over? I mean, did you just move on? And well, I guess that's, I mean, obviously less unlearned, but it must have been pretty devastating. Yeah. So it's really, it was really hard for, for, uh, I remember being, yeah, a week of, of being quiet, quite miserable about this, but realizing that, okay, because I was thinking, we're thinking about how much time I had spent in building these models and how long I'd taken, but I took it with realizing, well, I think that's when I really realized about that, or when most of the time when you're doing science, all the time it's taken up when you don't know what you're doing. So you've been all this time, you don't know how, what you want to do next, how you're going to measure this thing, how you're going to make this thing work, how you're going to do these secondes. And then what I had to do now is do it the second time. I knew how everything worked. I done all the code. All I had to do was, I done all the graphs, even all I had to do was reproduce. Yeah. That, you know, do the, redo the simulations with the right thing, check all the results, I had all the analysis all set up. All I really needed to know was, did it make a big difference to the results? So that really helped me sort of focus then so I could obviously have a nice, very structured to do list of, fix that and run this, this, this, this, this, this, over and over again, the list of things to do and then just grind through it. So that's, that's what I did. So you fix the code, rearrange it all, check everything, re-rope two chapters of the thesis. Then as it turned out, that actually that momentament I finished the thesis writing really fast because I'd written worked so hard to fix this problem. Then I finished the, wrote the next two chapters and wrote the discussion and it was done. That's like, okay, cool. So it was, it was a, as well as a coding lesson there was also a lesson in thinking, realizing that much of what, yeah, what, you know, what you think is the time sync, time syncing sciences that are often about, yeah, when you just, just finding your way, but even know what you're doing, moment to moment, then you can get through it much faster. Doesn't that, does that happen? Does that continue to happen throughout your career? Like the, for me, in my kind of short career, that continued to happen where I realized the, the entirety of the past three years, I had no idea what I was doing and was building up to get to the point where I'm right now. And then a couple years down the road, I realized I didn't have any idea what I was doing, but was very valuable to getting to the point where I know right now what, what I'm doing, you know, that that occurred over and over. Does that keep going? To an extent, yeah. Yeah, you keep, we keep re-evaluating, you know, particularly re-evaluating your sort of priorities and the way you're approaching certain projects and keep refreshing the, what must be a better way of doing this and spending like six months, hacking about a couple of grounds. So nothing is really sunken cost in that respect or a lot, most of what you're actually doing that may feel like sunken cost, I suppose, isn't is valuable in retrospect. Yeah, it's a good opportunity. Yeah. Lastly, Mark, would you, what would you do differently if you had to, more maybe not even necessarily what would you do differently, but if you had to start your career over, not back then, but now if you were starting as a, you know, thinking about going to graduate school or early in your graduate school career, what would you do, you know, differently if anything or how would you proceed? Very practical answer to that question, which is I would take some serious causes in the algebra. I've gotten that answer too. Yeah, I think that as well. Yeah. Because I, so yeah, so I'm a, I'm a computation neuroscientist who feels a bit of an imposter because my math is terrible. So I mean, my undergraduate degree was in cognitive science, so we did, so we did, discrete math, we did a lot of graph theory and logic and stuff. And I did, I took some, some, you know, undergraduate classes in calculus when I started my PhD, but I didn't touch linear algebra, so I hadn't done anything of it. Vectors, I did it, I finished, you know, did last at school. So not being able to pry open the box of the whole matrix algebra toolkit, even the linear bit, as proven, challenging, taking well, so you're doing a lot of data analysis and working endlessly now with, with dimension reduction techniques that are beautifully simple, written out as linear algebra, trying to explain them any other ways of nightmare, just going, no, just the matrix decompose like this. Oh, okay, I see that. Yeah. So, yes. So, projects, yeah, linear algebra. I think the earlier you get that the better too, especially just because it's, it's really counter, not counterintuitive. It's hard to intuitively grasp, I think. It really takes a lot of time swimming in that world to feel comfortable with it in my experience anyway. And I'm not, I'm not even saying that I'm comfortable, just I'm, I'm to the point where I can see, okay, that could become comfortable. Yeah. Well, thanks, Mark. So, I, you know, good luck with the book. Congratulations on, I know it's seen now it's late because it's like a year, a delayed congratulations, but you're going to be hearing this a lot, I suppose. And if this, so this episode should air, the book will be out. So, everyone should, should run and grab it and grab hold of the spike with, mark on your back, whispering sweet, nothing's about brains in your ear as you, as you travel through the brain. So, thanks for spending the time with me, Mark, and continued success, man. Thank you both. Thank you very much. Brain inspired is a production of me and you. I don't do advertisements. You can support the show through Patreon for a trifling amount and get access to the full versions of all the episodes, plus bonus episodes that focus more on the cultural side, but still have science. Go to braininspired.co and find the red Patreon button there. To get in touch with me, email Paul at braininspired.co. The music you hear is by the new year. Find them at the new year.net. Thank you for your support. See you next time.
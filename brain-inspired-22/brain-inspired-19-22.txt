 I kind of wake up every day to sort of think that maybe my research is going to help someone's life. And I think this is kind of like, oh wow, what a great person you are. But like I really, I mean, I think I'm going to like a small story. Maybe this is please you can cut it out if it's not relevant. Let's go to 5000 BC trying to explain I'm trying time traveling back then trying to explain the motion adaptation model to them. They'll be like, go away. Like, you know, what are you talking about? This is not I don't understand anything. So how these models are not real models of the brain. Like, I don't know. How is the network failing? How do we know it is failing? And like, what could be the additions that you can make to the models that it improves it? I think to actually have a good quantitative tangible grasp on those questions. I think you need a platform like brainscore to actually be there. This is the model that tells you that what is going to be the predicted neural response for any given image. I think that's where we are in in terms of like that we think of this as a stronger test of the model because there are many models than that could come up with different images. Then you can test those as well. This is brain inspired. Hello, good people. I'm Paul, a tempter of good personhood master of none. Today I bring you co-heatage car who also goes by co master of core visual object recognition. So co has been a postdoc for the past few years in Jim Dakarlo's lab. If you remember I had Jim Dakarlo on back on episode 75 talking about the approach that his lab takes to figure out our ventral visual processing stream and how we recognize objects. Much of the work that Jim and I actually talked about was done in part by co. Now co is an assistant professor at York University where he'll be starting his lab this summer. His lab is called the visual intelligence and technological advances lab. He's part of a group of people who were hired into a fancy new visual neurophysiology center at York that is going to be led by none other than my previous postdoc advisor Jeff Shaw. So co and I kind of continue the conversation about using convolutional neural networks to study the ventral visual processing stream. And on this episode we talk about that background a little bit and also co's ideas for where it's going. So as you may know what started out as a feed forward convolutional neural network has since been extended and expanded and co continues to extend and expand both the models to account for object recognition and experimental work that will be used in conjunction with the models to help us understand visual object recognition. And that includes adding other brain areas and therefore models to more holy encompass an explanation of our visual intelligence. So I get co's thoughts on what's happening what will happen and how to think about visual intelligence and a lot more topics. I link to his lab and he is hiring as he says at the end. So if you're interested in this kind of research you should check it out. I link to it in the show notes at braininspired.co slash podcast slash 122. Thank you as always to my patreon supporters. If you decide you want to support the podcast for just a few bucks a month you can check that out on the website at brainspired.co as well. All right enjoy co he teach car. Co are you an electrical engineer are you a neuroscientist what the heck are you? Yeah I think I'm electronics engineer according to my undergraduate education and training and then sort of I moved slowly gradually into like biomedical engineering one step towards neuroscience maybe and then finally did a PhD in neuroscience. What was the guy you interested in neuroscience? I think like all of a lot of us I think those were discussions about consciousness and things like that that I kind of cringe upon a little bit now. But those were those were the introduction to neuroscience and I think I particularly got influenced by a lot of these very nice storytellers like so I was doing my masters at New Jersey Institute of technology but I was sort of cross registering for courses at Rutgers where Yuri Buzaki was a professor back then. Just like listening to him and the way he talks about the brain I think those kind of those were sort of the initial hooks to like I really want to be in this field and be with this people and like talk about the brain with them things like that like sort of at a very artificial sort of superficial level mostly and I remember going to one talk from VS Ramachandran at Princeton and was like those kind of things was like wow like this is such a you know interesting system and I want to work on it. And I think that were the initial things that kind of like drew me towards studying the brain storytellers the storytellers pretty much and I think now I'm kind of like thinking there could be something beyond storytelling and like but but I mean the storytellers are perfectly fine scientists and they also do a lot of stuff that I kind of do now so like there's nothing against storytelling but I think that component that I sometimes kind of feel like what is the use of that I think it's really useful because like to tell a story about your science in a way that I think I'm not going to be a scientist. Your science in a way that sort of attracts young minds I think is very useful but now so so consciousness and storytelling drew you in but now you discarded both of them as a frivolous. I don't think I've discarded them as frivolous I just have I think my time is spent better doing other things than that I think I don't think those are like bad problems to work on or like you use this thing I think there's actually very useful. But I kind of realize that that's not my sort of you know forte like that's not my expertise is not doing that what percentage of people do you think who you know are drawn in are drawn in because of like the big questions like that and then. You know I said discard or you know whatever I said but then go on to realize you know that start asking very specific questions and kind of leave those. Larger things by the way side it's a really high percentage isn't it. I think so I think it's like a very high percentage yeah but but I think also it probably is useful to kind of keep reminding ourselves what the big questions are and like. So I think that simultaneously very important and yeah it's just that the sorry no that's fine I was just going to say that I think part of the reason and I don't really know the whole reason but I think part of the reason is that those big questions get you in and then you realize. That there are a lot of big questions that are super interesting that aren't those questions I don't know does that seem on point. I think that's right and I think that's kind of like very similar to how I feel right now and I think I mean I know that it's sort of like say multiple times that it's all about like asking the right questions and like the questions are very important but but what at least from my perspective I think I realized that like. The answers and what do I consider a satisfactory answers to those questions often determine like how you approach your science and things like that so to me like it's just not about the questions also like what kind of answers am I satisfied with and why am I seeking that answer I think those are the real drivers of what I actually doing the lab yeah. Yeah of course I would like to like you know simulate I don't know consciousness in an artificial system but I think that that is going to be a very difficult kind of objective to you know go for it in a lab and get funding for it and I'm really happy that some people are trying to do that who are more privileged and probably I am but. Congratulations on the new job I guess it's not so new now but where are you where are you sitting right now you're not at York yet are you. No no I'm still at in Cambridge Massachusetts MIT. Yeah. My governing institute when you when you so when you headed to York. Yeah I'm starting in July 22. Oh nice. Well congratulations. Yeah I'm thank you. Thank you yeah I'm very excited and was a very interesting hire because all of this happened during the pandemic. Yeah. I'm still supposed to go and see the department. Oh wow yeah it's really it's really virtual remote but I'm very happy so far with what I have all the discussions that I've had with colleagues there and I'm very excited to start working. You'll be you'll be near my my postdoc advisor Jeff Shaw up there so yes he's great and yeah yeah I'm very much looking forward forward to working. Tell him that I you know I've asked him a couple times he's been pretty busy because he just moved to York as well tell him that I'm still waiting for him to come on the podcast so. Okay I'll tell him. So so you you your your most recent work was a postdoc in Jim DeCarlo's lab in Jim's fun on the show and you guys are. Yep. One of the reasons why I asked you about your engineering background is because you guys are quote unquote reverse engineering the visual system. Right. I guess it all started off with a convolutional neural networks and a feed forward story of convolutional neural networks. But and I don't know how you got into deep learning but I do know that you were discouraged at one point from a from studying deep learning or using it. Can you tell that story? Sure yeah so I mean it's an old story it's like probably like now already 10 11 years old this was 2008 when I started my masters in. But biomedical engineering and I think I kind of realized talking to a lot of people back then as I even saying the word. Or saying something like oh I'm working with a computational model and I'm in a neuroscience you know program is sort of like. You look down upon as a fake neuroscientist you're not one of the real people that is doing the real neuroscience because you're not doing experiments. Because I was at that time I was not doing experiments and I was mostly trying to like look at things like for example like you know I was doing the working on auto encoders or neural network models trained with back propagation. Basically looking at how internals of these networks might match some neuro physiological data that I had or some behavioral data. Okay the things that everybody including me is all excited about these days but like. But that was before the quote unquote deep learning revolution in 2012 right so. It was I think it was still popular back then among certain groups I guess but I just did not I mean I could not predict that if I had worked on that maybe there could have been some nice papers or nice you know studies that I could have done but I as I was saying that like I kind of got. A bit discouraged because like I just started realizing that oh this is not the real neuroscience because I'm not sitting there with a slice of a mice brain patch clamping and like. Looking at new role voltages going up and down on a stupid monitor or something like I kind of feel like you know that's that's the real deal and I remember I I prepared a poster for a conference and I was going to present. This poster which is work done with like this artificial neural networks and I think I was so you know um I was I was afraid that I would be really cool that that conference and I in the morning of that day can just. Got out of there like I'm not going to present this forget about it I'm going to go back and I'm going to do real neuroscience and and look what I'm doing right now so. I have a unfortunate is really ridiculous but it's kind of pathetic but like there's a paper that I wrote back there and with all these ideas of like back problem like. Re reinforcement learning auto encoder student teacher network and I it's really badly written and don't don't look at it but like. I kind of use that as a joke with my friend like only if I had you know pursued this you know like all this work from Dan and Jim like oh I was way before. It was yeah did you tell you that you know I don't think he will take that paper as a joke yeah well yeah well you were talking so real neuroscience that's interesting because what you described with the mouse. Brain slices and patch clamping is exactly how I cut my teeth and neuroscience because I was a real neuroscientist right do you think the definition of what a real neuroscientist is has changed now so that. People you know doing what you do is um do you feel like a valid neuroscientist now. Yeah well I kind of validated myself by doing monkey physiology and like perturbation so whenever I'm doing that whenever I'm leading that life. I feel like as a real neuroscientist I mean I still think that like actually it helps to look at the brain and the biological data to get the right perspective about the system so I definitely value that but I think. With time the importance of computational techniques and analysis techniques are so important now just I think as we were discussing like there's a there's a answer that we're seeking and that answer to me is is going to be in the form of that. Those models and so like if you're not talking that language it sort of becomes difficult to communicate it will become difficult to communicate any neuroscientific finding in the future so I think in that regard that might become the real you know talk of neuroscientist in a few years if it hasn't been become that already you're looking for an answer what's the question. That's a very good question so I think um exactly so the question uh that I think. A lot of people are interested in is that how do we solve certain tasks like at least that's the way how I look at form formulating questions like I'm interested in neuroscience because I'm interested in a behavior and why I'm interested in that behavior particularly. Maybe because if that behavior goes missing I'll be in deep trouble so like that's kind of my sort of um way of getting into this space of like okay there's a behavior then what does it mean. To do a behavior and how do you actually scientifically study if so we measure this behavior we operationalize that behavior with some task and we measure that and then the understanding or the question is that like how does the brain solve that problem or give rise to that behavior and then we start by building models of that behavior and depending on what type of answers we're looking for are we looking at how different neurons come together and produce that behavior or how different brain areas are participating that behavior we we try to like you know uh build specific units or parts of that model and and look at them carefully so at least that's how I formulate the question like the bigger question is like okay there's a big behavior and how are we actually. Solving it in our brain well so uh you can correct me if i'm wrong here but the. Yeah the story is I see it uh from Jim's lab and from the convolutional neural network work is that you know you're trying to solve object uh core object recognition um so and you know it's it started off with a feed forward neural network uh you know that was built through many years and then um you know the deep learning uh world came on the scene and uh you guys realized that these networks accounted well for predicted the brain activity well and kind of went on from there but uh things have developed so the reason why I asked what the question was uh it's because you know it's interesting it's almost like an isolated system right so you have this convolutional neural network and it is modeled the layers are modeled after the ventral visual processing hierarchical layers uh in the brain um and you know the goal is to understand uh vision right and I don't know what that means um do you do you feel like you guys have a where are we in uh understanding vision? Yeah I think there's a lot of questions in in that in the sentences because like let me maybe like explain a little bit about what I think of what understanding means maybe like so um I think one definition of understanding that I have in my head is that it is basically coming up with the falsifiable model of of something like if I understand something and you can tell me that it's a wrong understanding and and if I can basically have a model that is falsifiable which like I can make predictions and you can tell me that oh you're wrong so for example and there could be different levels of this understanding so I understand how my coffee machine works because I can predict which button to press and the coffee is going to come out as like a concrete prediction you can test me like oh you can tell me like go turn on the coffee machine like go and press the wrong button you say you don't have any understanding about this machine works but if I press the right button and the coffee starts coming out but then if the machine breaks down there's a different response level of understanding I might need to fix it. So then you might ask me which part of the machine to fix and how does it work. And there's a more detailed level of understanding required. So in the same way, I feel like understanding vision would require multiple levels. And I think one of it is at the behavioral level, like can I predict the behavior? So that's where we start up. But all of this relies on models, like concrete computational models. At least that's what my current sort of opinion of what understanding for me might mean. Is that like you have concrete computational models that make explicit predictions about how a system is going to work or perform. And then you get to test it. And that's sort of the understanding. And that's moving. Now the problem I think usually is that if we define understanding this way, then we have to also sort of have common, I don't know, I don't know. I don't know. Goals of like water we're trying to understand. Like what is that behavior? What and my current view of the field is that we actually don't have common goals like that. We are kind of like all doing our own things. And so I think it's kind of important to maybe like have certain specific goals as like this is what we are trying to predict. These are the behavior of the system. And these are the neural data or something that we are trying to predict. And then come together, like come up with what are the best models that can do that. And some of it is we are currently trying to do it with this website and platform called BrainScore and trying to have an integrative approach to like all kinds of data and all kinds of model and things like that. So how's BrainScore going? Are a lot of people using it? Yeah, I think the user base of BrainScore is definitely increasing. And I think we are having a conference now. Well, we still submitted it at cosine. And we are potentially going to have a competition. It's like I think it's going to feel a little bit more like an ImageNet competition or something like that. But my personal opinion is that maybe someone can look at BrainScore and say it's too early for someone to like start making these models and scoring them and being so concrete about it. But I think it has to be done. Like that's kind of my goal. And like it's, if you ask me where vision, like understanding of vision is, to me like pointing to some kind of platform like BrainScore is a concrete answer that I can give. That's my way of quantifying it. So yeah, it's a benchmark. And but you know, on the other hand, benchmarks have gotten some flack because like you were talking about, we don't know whether that's the right benchmark, right? Whether it's the right question. So it is concrete. But I guess we're progressing and asking better questions. Would you agree with that? Yeah, absolutely. And I think there is no like three or four benchmarks that will define our understanding. So I think the goal is to have more and more benchmarks. And hopefully we will see that like because it's the same brain that is giving rise to all that data. So if you're actually modeling that particular brain, then we should be converging to like a very small space of model eventually, at least as the dream. So of course, like there could be multiple different benchmarks and different ways people are probing the system. But I think the value at a brain score is that if we can get all those experimentalists and modelers on board, then they can provide those data or provide those sort of benchmarks as also targets for current systems instead of saying that like, oh, you know, your network is never going to operatic that. Like, okay, that's okay, that's fine. I mean, the networks are falsified under all possible benchmarks. So it's not a big, you know, sentence to say. But it's just like, how is the network failing? How do we know it is failing? And like, what could be the additions that you can make to the models that it improves it? I think to actually have a good quantitative tangible grasp on those questions, I think you need a platform like brain score to actually be there. I want to ask you about falsification because you've talked about how that's one of the useful parts of the modeling push is that they are falsifiable. But then, you know, you have models like the feed forward convolutional neural network that predict what is it? Like 50% of the neural variance somewhere around there. Right. Yeah. How does one falsify a model? Yeah, so I don't think, I mean, in the sense of falsification, those models are all falsified anyways, but then the question is, how do you build the next best model? When I think of that, I feel like, given some numbers like that, it then makes me kind of figure out if I build a better model, it should at least be better than the current numbers that are coming out of, you know, the feed forward neural networks or something like that. So I think, of course, you can, I mean, the question that you dismiss the entire space of models or family of models, as like completely useless, or do you say, like, that's the good start, let's build upon that and start adding elements to that, to build the next best model. So I think I'm mostly motivated by this idea that yeah, like, we have a good grasp and thanks to machine learning and AI for that, for actually building these real models and not like toy models. And so now that we have these models, there's capitalize on this momentum and get going and build the next, probably build the next best models. Although I'm talking about models in this way, but my personal life is mostly spend doing experiments trying to put holes on those modeling frameworks or models. So I'm actually very happy that those are all falsified, because I feel like that's my job to falsify them. And but the other part of my job that I feel like is important is that it's not only just to falsify them, but also get some data that is in the same scale and in the same sort of spirit that would help build the next best model or something. So it's just not good to shit on them. It's just also provide some, you know, material for them to work on, chew on and become a little bit better. So what, in your, so you're doing a lot of experimentation, but what's faster modeling or experiments? Experiments. Or faster? Yeah. I mean, I think building a better model is way more difficult than doing an experiment. This is, yeah, I think I'll debate anybody about that, because I think for me, like, you know, so again, depends on which field you are. If you are building this model for AI or purposes are like, so there, of course, modeling is way more faster than any behavioral experiment or any neural experiment. If we are trying to build a model of the brain, it's a, so it's like a, we're discussing about this engineering things that, okay, I have a problem of like, how is the brain working, but the solutions cannot be anything. It's like constrained by this biological system. So that is like a specific solution that we're trying to look at. And I think aligning the models with that is a very, I mean, I can build a model that might solve action perception or action prediction better than the current system, but that might not align with the brain. I think when I said like the modeling of is slower, I think is that bit, which is like having models that are more aligned with brain, because you know, like 2012, AlexNet came up and then now we don't even talk about AlexNet in terms of like computer vision. I think no, no serious computer vision scientists would say, like, AlexNet is my model that I start with. But it came to neuroscience and it's still here. We are still using AlexNet. So like, I think things come to neuroscience and they stay for a longer time because it's just very difficult or like discriminate among these models even. And I think there may be in here, I mean, for us, there's some of them are like, there's some deeper questions in here as well, maybe, because like when we say we have a model of primate vision, like like what do we actually mean? Do we have a model of a specific human or like a specific monkey, or are we modeling the shared variance across humans or monkey or are we developing a model of the, you know, all the possibilities like a superset of vision. So how well should a model of object recognition even predict behavior of one subject or some neuron that I'm recording for in a monkey brain? I think we need to think carefully about those questions because like, yeah, sure, like the model might predict, you know, one neuron in a monkey's brain at 50% explained various. But then how well does any other neuron in any other, you know, human brain predict that, other humans, IT neurons or something? Like so I think quantifying and sort of setting up the ceilings based on what we actually are modeling, are we modeling individual human beings or individual monkeys or, you know, this shared monkey population? I think those questions are sort of important. And then maybe we are done with like predicting core object recognition feed-forward responses because, you know, one monkey predicts another monkey at 50%, and there's no way you can improve beyond that. Like so it's, to me, I think because of these kind of, and of course, and I'm saying it, I'm realizing that like this is basically like, it's empirically challenged in something like that. So it's actually the experiments that have to provide these answers and we are sort of like limited by technology and how well we can probe the system. So that's why I think slower. Yeah, okay, well you said ceilings, so the way you've talked about it, it makes me, it's a fuzzy ceiling, I suppose. They're fuzzy ceilings in that respect. Sure, yeah. All right, so you're, I'm slowly coming around to the fact that modeling takes a long time. I didn't do any deep learning modeling. I did like a kind of a psychological model. It was very simple, right? And it took a long time, but experiments, I had to go in every day and it just was, you know, years to publish a single paper. No, I see where you're coming from. And I totally, I mean, that's, has been sort of my experience as well. I mean, it takes a long time to train a monkey to implant your rays and get the data. And maybe the area doesn't get implanted well then you have to implant again. Like there are multiple problems that can come up. But I just feel like at the end of the day, you have some data and if you have designed your experiments properly, that's like, especially neuroscience, which I think is still in the dark ages. Like it's sort of like novel data, you know, it's like just, it's anything you do, you can, you can basically, it's like novel data and a target for like a model to sort of predict. And I think in that way, it's faster because like, I can build a model like one, you know, just, you know, put some two convolutional layers together and call it a model. But is that really useful or is that really taking the feel forward? I mean, I mean, I, I mean, maybe I answered it too fast about like, you know, experiments are slower. Yeah, you need to think about that more. Yeah, I might have to think about it, but I think that, but I think I was, I'm trying to sort of like tell you a little bit of why I think modeling is actually going to be slower, especially like modeling the break. There's physical time, but then there's also heartache time. So maybe those are two orthogonal things, right? So like the other question would be like, where do you experience more heartache and obstacles? And do you think modeling would be the answer to that? Again, depending on your experience. So like, if I, if I'm like running a monkey, like after I have like, you know, brought a monkey to the lab and done an experiment, I have zero energy to do and think about anything else in the day. So it's like, I'm done for the day. And I think that way, yes, it's a lot more, I mean, at least because that's the experience I have at, I cannot tell how bad it is for like a modeling person or how bad is like to come up with, you know, like a giant mo, I mean, my co, I feel like it's like, oh, that's, yeah, like most is like the libraries are not loading, the version is not correct. So those are the problems that I usually, you know, face, but but yeah, but at the end of the day, the one's the model is training and I don't know. I mean, at the end of the day, I feel like a modeler is going to be more disappointed because the models don't really predict much more than the previous model. That's what the neuroscience experiments, if it's designed properly to begin with, I think it's always going to give more insight. All right, just a biased opinion made. Yeah, we're all biased as we know. All right, go. So again, correct me if I'm wrong, but the way that I see it, there's this core object recognition story that at the core of it is a feed forward convolutional neural network. And, you know, you guys in Jim's lab have done a lot to explain neural data. So that's kind of like the basis, the way that I see it. And then from there, you've done a lot of other work. Like you've started adding bells and whistles like recurrence and you've controlled, you've synthesized images to predict, you know, which neuron is going to be driven by a particular image. So you're making the models more complicated. And I've heard you argue that what we need is more complicated models, whereas from a philosophy of science, classic perspective, what we like are simple models, right? And because part of the problem with these deep learning models is that we don't exactly know how they're doing what they're doing. And to use a complicated model to explain a complicated organ like the brain, there's pushback on how much that actually buys us in terms of understanding. But you argue that no, we actually need them more complicated. Why is that? Yeah, I think depends on how you define complication. Because I think the reason why I might say that it's, we need more complicated models because the models are not really predicting what we are, we set out to predict. So I think making them simpler, I mean, I don't know, I mean, I don't think that's going to be the answer because the brain is complicated. So anything that is a simulation of the brain will look complicated in some sense. In the other sense, it will not look complicated because if you have correspondences and alignments with the brain, you can point to a part of the model and say, oh, that's before and like you can say, like that's before in the brain. So in that way, it might become less complicated over the course of it. It's just a definition of what complication and what is interpretability and what is understanding. I think those, and because there is no objective definition of those things, I think this kind of conversations usually lead nowhere. I mean, I'm trying to think of this thing like, for example, when I was in my graduate studies, during my PhD, we had models of motion after effect. And if I spoke to anyone like at VSS or SFN or cosine about these models, everybody would say like, oh, this is completely understandable, interpretable, simple models that we have intuitions about, which is like, okay, you show coherent, so you show a random motion pattern and you have these motion detectors, they're all firing, and they're all firing equally. There is no add up, like basically if you, after that, if you show a stimulus that is moving upward, the upward neurons will do something and it's going to be like some response, which is going to be higher compared to the rest of the group. If you are only showing upward motion for a long time, those are the neurons that are going to fire and get fatigued. And then when you show like a random pattern, you will see like, everything else is firing higher and the upward motion detectors are kind of finding a slightly lower. So overall, you will have bias towards saying, okay, it's going, maybe the motion is going downwards, something like that. And this can be modeled and people have modeled this. And I think those models compared to artificial neural networks now, like they might be considering a simpler, more intuitive understanding, but understandable models, less complicated. Now I'm thinking like, let's go to 5,000 BC people are talking Tamil or Sanskrit or an Greek or some other language, trying to explain, I'm trying, time traveling back, then trying to explain the motion adaptation model to them. They'll be like, go away, like, you know, what are you talking about? This is not, I don't understand anything. So these models are not real models of the brain, like, I don't know. And I think I feel like the same thing is happening now, which is like artificial neural network, but these, but remember like the motion model that I just mentioned was predicting this, this adaptation phenomenon, this behavior. So that was kind of the goal of this modeling. And for now, it had some relevance with how people have looked at the brain and neurons. So, but if I tell this in 5,000 BC people, we'll be like, I don't know, this is not mapping into our world view. And I think the same thing might happen right now with convolution neural networks and some terminologies and things like that. It's like, okay, this is too complicated. This map, I cannot like fit into my low dimensional, kind of behavioral space of like how this high dimensional, you know, areas are functioning or like responding. So I don't take that complaint seriously because I think with more familiarity with these terms and models that, that complaint is just going to go away. As the models become more and more powerful in predicting different behaviors and we will see, for example, use of having these models in sort of like, you know, real world applications. And I think that kind of fear of, oh, this is a two complicated of a system is just going to go away. And for those for whom like this won't go away, they'll just probably have to live with it. Yeah. Okay. But maybe the more intro, I think one of the reasons why I feel people is like, I like simpler models, it allows them to like maybe think through like if the model gets stuck, what to do to improve it. And I think that to me is like a real value of having a simpler, more interpretable model. And there, it's a question of efficiency. If you can have a complicated model, kind of self correct yourself, self improve itself, which is kind of a future goal, maybe that might just be a more efficient way of dealing with this problem than, then kind of like humans kind of coming up with their own intuitions of like what is a better model and things like that. And I think we were discussing about this engineering background to me. That might be something I'm more prone to accepting because of my engineering background because I just feel like there's a question. There's a solution and these are just tools to get to the solution. Doesn't matter if I intuitively kind of understand it or not as long as it's aligned with the brain data and things like that. It's fine. So one of the, I actually got even more excited to talk to you because after we had set up this episode, someone in my course asked, because I talk, in my course, I talk a lot about, I use Jim's work and your work to talk about convolutional neural networks and how, you know, how it relates to the ventral visual stream. And then someone in the course asked, what about the dorsal stream? Because I talk about the two visual streams. And this goes back to the question of like what it means to understand vision. And I know that one of the things that you're doing, so the question was like, why aren't their models for the dorsal stream as well? Why is it all ventral stream? And I know that you are starting to incorporate it and you have some background with the dorsal stream as well. And maybe we should talk about what the dorsal stream is just to bring everyone up to speed. But what, what do you, so are you just starting to incorporate other brain areas now? What is your, yeah. Well the first thing is that maybe if that student is interesting in doing a PhD or a post doc, send them my way because that's the kind of question I've been asked. I was also asking about like, what is the dorsal stream doing? Because I had spent like five, six years studying the dorsal stream, which is slightly ever since above the ventral stream in sort of anatomical location in the brain. Let's say what the dorsal stream is, like what it classically is. Do you want to say it? Or you're happy too as well. You can say it. So yeah, so classically there are two visual streams. It hits V1 and then it kind of branches off into a ventral stream, which is what the massive amounts of neuro AI and core object recognition is about where it gets processed over hierarchical areas up through V2 before IT until we suddenly have neurons that respond to whole objects. But the dorsal stream is classically the where or how stream, which is much more related to the motion and spatial aspects and our actions. So it's activity related to. And that's where I spent my career is basically from more or less in the dorsal stream. Yeah, so I don't know. Did I explain that okay? Yeah, yeah, absolutely. I'm usually now, like, I think I'm usually very careful about assigning some behavioral function to areas. I mean, mostly start talking about anatomical locations. Who knows? You might find that dorsal stream is just a big part of core object recognition. Right. Well, well, yeah, I mean, so the thing that has been, I guess, always known, but not paid so much attention to is that there's a lot of crosstalk between the dorsal and the ventral stream. But we've kind of studied them in isolation, right, as two individual separate things. Yeah, I mean, I think that's sort of I see that as an opportunity to sort of like really take this sort of studies forward and trying to incorporate looking at dorsal stream as well. Just one point I wanted to make is that I think there are folks who are beginning to build models of the dorsal stream in the same way as, you know, the ventral stream modeling has gone. I think I recently saw a paper from Chris Pax Group, and I'm sorry if I'm forgetting other authors. I think Blake Richard was part of it. Patrick was part like there. There's a I think it's a bio archive at least. There's work done from Brian trips group trying to model this system. Like, of course, like dorsal stream has a lot of modeling, you know, prior modeling work. That is not kind of similar to the convolution neural network stuff. But I think people are beginning to build them and they're different objectives that they're proposing as like sort of normative framework for like how the dorsal stream gets sort of trained up. Like I think those are nice hypothesis and we'll see like whether the data actually, you know, supports those, you know, models or stuff like that. But I think for me, sort of trying to get into this area, those are really nice work because like that gives me some baseline ideas or baseline models to start testing and probing, you know, when I start designing my experiments, I think those models will really help me to sort of make a good experimental design. But are you building? So I actually don't know like what kind of model because it wouldn't be just the same, you wouldn't just use a convolutional neural network to model the dorsal stream, right? And so are you building models yourself also or are you going to incorporate? I have not personally built any models right now. I've just been testing some of the models. Like so I started testing some of the models that were like mostly used for action perception or action recognition models. They have these like temporal filters. Like they're still convolutional. It's just like more dimensions to the convolution as like a time dimension. So I think those are like good starting points because like they're easy to build maybe because they can use the same kind of like training procedure. But I think we have to at some point become a little bit, be okay with being a little bit, you know, go lower in terms of prediction because we need to move from static kind of domain to a dynamic domain. And I think my usual experience has been that whenever you go, you make this jump like all these models start to kind of like not perform as well, not not predict the neural responses as well. And so I think to me like that might be one of the reasons why maybe some people are building these models and then actually coming out because they don't really predict anything. Like so maybe backing up a little bit like my main sort of interest in this dorsal venture interaction question kind of started when I was mostly recording, you know, showing static images to the to the monkeys and recording their responses in it. And I and these are, you know, objects that are either like natural photographs or you know, some kind of synthesized images. And I was thinking about my previous work in dorsal stream and it was like about motion and you know, like there were dots moving and greetings moving. But if I think about the real world, like I never see dots moving and a grading is moving in the real world. And objects are moving and to be like if I have to have any real world relevance of my current research, I just felt like, you know, it's a dynamic world. I'm moving my eyes and I'm moving myself and the objects are moving. And if I think of these questions or think of these behaviors, dorsal stream kind of pops up in any literature search that I do is like self motion, you know, motion of objects or motion of like not objects, but like maybe motion of like something in my visual field. But then I was wondering like, you know, like IT has this nice representation of what the object is. And if the object starts to move, is it all it does all of it fall apart? Like what happens? Right? So just out of curiosity, I just started recording from these neurons and when the objects were actually moving. And then I started kind of, you know, this is this work has not been published, but it's like the sort of the preliminary result is that well, IT kind of can predict where the object is headed, where it is moving. It's not we know from previous studies from Jim's lab that from looking at IT representations, you can tell where an object is. This was from Ha Haong and Dan 2016, where the object is located, you can tell in a static image. So there's one trivial solution where okay, like if you can tell where the object is located at different time, this you can maybe combine that information to tell where the object is headed or where the object is going. What I started finding is that like it's not only that it's like you can just take a snapshot of like maybe after you have started this movie, two and a millisecond, three and a milliseconds later, you can just look a small time and you can tell where the object has been going. So it's like there's a predictive signal of where the objects are headed. So that sort of like then I started thinking like maybe this is coming from the dorsal stream or is it, you know, like but again, these are again, like ways of thinking that I've kind of discarded in the last few years. So I feel like the way to think about this is like can a venerial ventral stream model explain this neural response is already then not to invoke the dorsal stream at all. Maybe they will fail and then the dorsal stream models are actually necessary to, you know, account for this neural responses and also the behavior that I can test based on these kind of stimuli. So that has sort of been my approach and the dorsal and the quick update on the ventral stream results is that these models they they're not really predictive of these kind of responses at all to some degree. That gives me hope. It's always good to have hope when you're starting your career, although not that you're starting your career, but you're new, you're new start. Yeah, exactly. But you said you've discarded thinking about it in that way from like different brain areas. Is that because you've discarded thinking about different assigning roles to individual brain areas or? Yeah, sure. Yeah, absolutely. I think I mean, that whole way of thinking is like, I think it's primitive. It's not going to lead to like. The brain is doing what it is to like make us go through the day and like all areas are coming together in some form or the other. And so I think it's I will never I don't want to come up with the answer that like dorsal is doing blah, blah. Like I think it's just part of a system that is trying to solve a behavior and the answer is going to be here is a model that has elements in it that are corresponding to neurons in the dorsal stream. And together they, you know, solve a behavior. Now you can ask the question if if I really want to satisfy someone like what is the dorsal stream doing you can start doing perturbation experiments in the model or in the brain and say like what happens to the behavior if I take out, you know, part of the dorsal stream or part of this and that. And but then I'm mostly worried like what is the answer like my answer is going to be like, oh, it takes a 10% hate for video a versus video C like I feel like those are the kind of answers that are really going to come out. But people are going to I mean, I might spin this off as like, oh, but this is about, you know, function X or like it's about something about predictive coding or I can get the answer in that form. But I think at the end of the day is just going to be a big lookup table of like you perturb this part of the dorsal stream, you get X hit on this particular behavior, this particular video. So that's why I feel like my answers need to be in the in the in the modeling kind of frame for yeah, words were we're limited by our language. It turns out this very special thing that we have language also is very limiting in some respects, I suppose. Yeah, but I think if the models can relate back to the language, I think then some of the, you know, problem or the tension might be relieved a little bit because I think now they're so for example, I mean, this is maybe slightly off topic from the dorsal ventral discussion. But like if you look at a model of ventral stream, like you can look at brain score and like say, okay, a resonator and one or something has some numbers associated like some scores. I can see why people have a problem with that model and why people say this is not interpretable because like there are parts of the model that are just don't know what it is, like how it is in map to the brain. Like I can call like some part as it here, some part, there's thousand different things in between that I have no clue of what they are and like it may be the model is not performing because of those, you know, computations that are happening in those layers. How do I relate this back to the brain or something? So I feel like that is a real problem and I think it is in our interest to start coming up with commitments to different parts of the model and then falsifying them based on those commitments. So like an interpretable model should to me should be like if I write up, so what is an interpretable thing in neuroscience? Like a paper, like the abstract of the paper is completely should be at least interpretable to anybody. So if a model has components to it that can talk to each part of that abstract, like, you have a task, you have a neuron, you say something and if you can basically map your abstract to parts of the model and if the model can map onto the parts of the abstract clearly, that I think just gives the model interpretability and I think that level of crosstalk and language I think should exist and I think that language I'm trying to sort of develop myself to, even when I'm thinking about modeling and experiments. Well I mean after all that about how we shouldn't assign roles to individual brain areas, you are doing some inactivation experiments, right? So what's going on there? Why are you inactivating individual brain areas? Yeah, so I think that's basically try to maybe, so there are a couple of studies that I think at least I've done recently one has been already published, which is like inactivating ventralidal PFC and looking at core object recognition behavior and also looking at representations in IT when the monkey was doing that task. And the goal was to basically expand or test whether these feedback loops that are existing between these areas are they actually playing a role in that specific behavior that we are studying because the current models are incomplete and they're not predicting enough so it's kind of makes sense that maybe there are other areas and there are other connections that are important. So that is not to say like PFC does X, right? It does everything, apparently. Yeah, I'm sure it does a lot of things. It's just like for me to actually ground this problem, it was more like what kind of role or what kind of signals do I, at least from the inactivation, was like what kind of signals go missing in IT when I inactivate them in it inactivate PFC and what kind of sort of deficits do I see in behavior? And then the data again, as I was saying is not like, oh, you cannot identify objects in an occluded scene or something. It's not an answer like that. It's mostly like here is a big data set. It's like, it's not satisfactory to many people. Here's a giant data set. It's clearly like you see, there's an average effect PFC, no PFC, okay, I've shown you this. There's a prediction that is coming out of a model that is like this model is a feed forward model. It might not be doing X, Y, Z and well, like that, those are the images where these effects are also much more concentrated on. So there's a story there is like, okay, it's clearly part of a system that is not the feed forward system that is like maybe going beyond the current feed forward system. But like at the end of the day, I think the next step is to build a model that has a unit or module that is called, you know, VLPFC and part of being that should produce the same kind of deficits. It's like, and this is where I think it's a very hard thing to do. It's actually easier thing for me to like, part of PFC and like get this data and say like, okay, this area is involved. But then build this model, I think that's going to be really difficult and I think and there are limitations to perturbation data, for example, I think, and this is might be like relevant to the conversation about perturbation experiments because I think even after this perturbation experiment, I think actually recording in that specific area with the same kind of task and same kind of stimuli might be more constraining for the next generation of model. And then exactly what I'm doing currently. At the same time, I was thinking like what kind of perturbation experiments might be like, you know, may have more benefit for the kind of models that we have right now. And that kind of led me to developing, mostly we say developing a lot, but it's like basically testing this sort of chemo genetic strategies where you inject a virus in a brain area and for in my case, I also implanted a uterray on top of it. So we injected dreads in V4 that was supposed to be like, you know, silencing or down regulating the activity in V4 and then we implanted a uterray. Sorry, can you say what dreads are because we have a need. I don't think we've even, I think we've mentioned them on the podcast before, but what are dreads? And then I also want to ask you, so you injected and then you in a separate surgery, then you implanted? No, it was done on the same, it was done on the same exact surgery. Sorry, sorry, I didn't interrupt you. No, no problem. So the, so the basic, I think idea is that you inject a virus that ends up sort of manifesting as a receptor in a neuron that you can activate or deactivate with various means. It's the same idea with optogenetics, the same idea with chemogenetics. In the optogenetics to sort of activate or that particular receptor, you need to show shine light on that neuron, on that area. For chemogenetic, you need to basically inject a drug into the system. And so there are some pros and cons of these two different, multiple different things. So for example, you're kind of like limited in terms of where you might want to inject for optal because light delivery is tricky because you have to be mainly maybe restricted to the surface of the brain. Deeper structures might be very difficult to target at scale. Maybe you can target like one or two neurons. In chemogenetic, you can basically inject the virus anywhere you want in the brain and it kind of gets activated through this sort of like injection that you do in the bloodstream. So it basically activates or tries to activate all these receptors that has been produced. But then there is like temporal limitation of optocan like go very fast, quick on off. But the the the dreads are more like musymol in some sense. It's on the effect is on for some time and how are you usually used to be? In my calcula no, I don't know weaker. No weaker. Yeah, I would that would not be so good. I think it's most so from my estimate, I think it's mostly on for like maybe a couple of hours and sort of. So it's like very similar. Musymol. At least the main time is like musymol. And what what I've been doing is like we have these arrays that you can actually test, you know, you can show the same images over and over again after you have injected the activated drug and you can sort of see how quickly or what is the kind of time course of neurons responding lower or higher. And then you can have behavior on top of it like the monkey is also behaving on different blocks. So you can kind of see like, you know, there's some deficits that are coming up and then the deficit sort of like go away at the end of the at the end of the day or something. So I think I am at least thinking of like how do I take this and like make it useful for models like okay, I can say like a V4 is involved in, you know, object recognition. That's right. Not too many people will be interested to listen to that. But if if you give me like okay, brainscore has like 1000 models that all have like 0.5 correlation for V4 activity. But now I give you some V4 in activation data and then 900 of them fall off and they cannot really predict the kind of, you know, pattern of deficits that V4 has. That might be as, you know, important than than tool or maybe an important problem. But as you see like here, you need to have a model that has like a brain tissue mapping of V4 and you know, where are you injecting the virus in the model versus in the actual brain. So I mean, there are parts of this problem that are still more complicated. But I think this, the chemogenic strategy at least for areas like V4, you know where you're injecting and these are mostly getting a topic area. So there's some level of, you know, correspondence in the models and then you have a neural data on that. So you can actually just say like, you know, like I don't care about like your assumptions just like fit to the neural data. You have V4 neural data within with an without activation. You have, you know, your model with an without activation just fit to all the data that you have got and then predict what happens to IT or predict what happens to behavior in the model. And that's how you validate the model. And I think that is a very, I think that's a stronger form of using sort of this perturbation experiments because I think it's not uncommon to see, you know, experiments where someone says, you know, this area, I perturb did nothing happen and someone said, like, no, no, you didn't do this blah, blah, blah. So it's like, if the answers are the yes and no, I think it will just stay there. It has to be sort of falsification of like competing models and then maybe some data will be more useful than the others. The other, I think, upshot of having something like this is that imagine you have monkeys that are doing these tasks in their home cages. Like we have a lot of monkeys that are trained up and they do these tasks all day in their home cages whenever they want because they have a tablet, they can do these tasks. You can pair this up with that system and you just need one person to just go and inject like something in a monkey and then basically you have days where you can like, you know, run this with some part of their brain code on code deactivated and you can multiplex even with the viruses, you can like, you know, target inhibitory neurons, excitatory neurons, you can have different viruses inject in different parts of the brain that have their own corresponding activated drugs. So that I think there's a lot of kind of interesting data says that can come out of this approach which should bear on the modeling question. How much of your future, what I want to know is like the vision that you have for your own lab and how much of it is going to be this kind of work and how much of it is going to be modeling and so on. I think a lot of this is going to be this kind of work and like just pushing the boundaries of experimental neuroscience. I think the modeling is like, it's like, that's going to be the backbone of the lab. The computational part is like, no answers can be provided from the lab if there is no model attached to it. So I will be collaborating with others. I'll have people, you know, working with people in the lab who will be building probably these models as well and testing them out. But I think that I don't think I will be, you know, happy at the end of my career if like I did not improve like a model or something of the system, even after doing all these different experiments. So it's going to be a mix of that. I mean, maybe I don't know, I should probably mention this like, I'm not, honestly, I'm not really interested in building the best model for core object recognition or dynamic visual perception or visual cognition, just for the sake of building that model and understanding how the brain works. I mean, I don't quite motivate myself that way. I think, and it's kind of, I mean, kind of interesting because like, I think for training purposes, these were the most concrete fields and most concrete labs that I thought, okay, this is where I should get trained. But I think I kind of wake up every day to sort of think that maybe my research is going to help someone's life. And I think this is kind of like, oh, wow, the great person. You are like, I really, I mean, I think I'm going to like a small story. Maybe this is please, you can cut it out if it's not relevant. I was I was I was working visual neuroscience and people know that I work in visual neuroscience back home in India. And what do you mean people know like the like India knows? My family, my family, my family, sorry, people, one billion people know, no, like five people, one billion people know that's more than that's more than that's what I do. So there you go. Okay. Yeah. So among those five people, maybe like 10 Indian families tend to be. So among those 50 people, then there are some of them that I don't I think they have some idea of like what I might be doing, which is completely wrong. And I think I had this encounter with someone and unfortunately their kid had had got diagnosed to be in the autism spectrum. And so I was meeting them and they asked me like, oh, so what are you working on these doesn't I'm like, okay, I'm working on visual cognition. I'm saying stuff like how do we reason and things like that? And this person turns to their kid and tell them that you know, your elder brother will one day like, you know, it's working towards the solutions and the skin is like very young can understand anything of what they're saying. But they're basically telling them that like he is going to come up with the solution that will cure you. Right. And it just felt like I just was feeling like I'm I was thinking I'm doing to do that. I'm failing. I cannot find any connection to like, you know, what this translates to. And at that really, I mean, there was kind of a pivotal like a point where I started thinking like I need to find real connections with what I'm doing and how that really impacts or translates not just this, you know, like the first paragraph of a grant saying like, you know, I'm working this next year like this is relevant to like blah, blah. It's really trying to schizophrenia. So like, so really trying to find something that I started. I mean, that's going to be a at least some part of my future research is like trying to find out how, you know, having these models that are this concrete models with brain maps, how are the beneficial to diagnosis and potentially treatment strategies in certain some of these neurological disorders. And I've started working a little bit towards these goals and I'm very excited about this because I think there are real benefits. And I think you were mentioning about this like neural control studies. And I think those are the kind of studies that are really sort of giving me hope that like there is a way to like contribute to this, to this part of my desire. That's kind of a magical thing. So that wasn't your motivator for a long part of your career, but from a place of guilt. It's yeah, but it's developed into a great guilt. It's a great motivator, but it's developed into like a real motivation for you. Like I never had that. I don't care about helping people. And so I always felt bad writing schizophrenia in a grant, for example, right? Yeah. I mean, yeah, I mean, it's a little bit philosophical. Like I don't even know. I cared about helping people in a somewhere. Maybe I'm basically thinking I'm trying to help people, but I'm just trying to help myself maybe thinking like, well, what if I have Alzheimer's or something in my old age? Like yeah, but I think currently at least I do feel like that gives me some level of satisfaction to think that there is potentially some link of my research that might be getting help to some for some reason. Yeah. I mean, it is interesting to think how through our work, through your work, through people's work, your interests change. And as you develop and as you ask different questions and answer different questions, it's just kind of a magical thing. So that's thanks for telling that story. Yeah, I think that definitely like impacted me a lot. But I am also like I think these are related issues. Like I think like you were asking about understanding and progress and like things like understanding vision and visual cognition. I think the moment we start to like measure our understanding like in the brain score where something like then I think these answers to like the clinical translation becomes more concrete maybe like like. So I think they're very related is just for me, it took me a little bit to like figure out and maybe I'm still working on it like to figure out where exactly are the most relevant parts of it. And I think my interaction with a lot of folks who are doing autism research like really helps. So for example, I've been in touch with Ralph Eros at Caltech and we are sort of collaborating on a project. I think those discussions and like reading the papers like really I think I think they have a lot to contribute to what I do. And I think our way of thinking about the system has a lot to contribute to that that research. Interesting. So do you know you mentioned the image synthesis work a little bit. Can we talk briefly about that because maybe you can just describe what the work is. I talked with Jim about this when he was on the podcast but we can kind of recap because it was kind of splashy right. And I kind of want to hear your thoughts on how you currently think about that work as well. Yeah. So this work was done in collaboration with Puyabashiv on who's at my guild now. And so me Puyab and Jim were basically we did the study together. And so the basic idea was that we were recording in V4 and we have models of V4 neuron and the question was that can you from the model you know come up with stimuli using the model. Can you come up with stimuli that puts the neuron in specific desired states. And one of the states that we considered was like let's make it fire the most it we can. So the model will tell me. Sorry. So this is the control aspect of understanding. Yeah. Exactly. That is like you know prediction and control. And this is the control part. So the models could predict but maybe they couldn't control because maybe the images that were synthesized. I mean there's a part where there's a separate technique which is how are you synthesizing the images and maybe there are ways in which that doesn't need to be attached to the model. That's specific model that you're using to predict that they can be two separate things. But but again like for us it was like you know we were using the same model to come up with the images as well. So we came up with the images we're trying to control the neuron and we said we were targeting like okay V4 let's make this neuron fire as high as possible there was one of the goals. The other goal was let's take a bunch of V4 neurons that kind of share the same receptive field properties and try to set one of them to very high and the others to be very low. This is like a population level you know control. So these were the two goals that at least we thought let's start here. And then we were asking like okay this question seems like you have heard of this before because like what does V4 neurons do like they respond to caravans? What does V1 neurons do? They're like gobores and orientation and V2 is like texture and like IT's faces like now you come up with this stimuli and you look at them and like I don't know what to call them like maybe they're something. But then for us we kind of like ignored that problem which is said like okay let's just take these new images and like see whether the model's prediction is right because then that at least you should show that like using these models you can control the neurons to some degree. And that was basically the study we had some success and we were comparing our you know success rates with like taking a random sample of new images or using the previous sort of thoughts on what are the stimuli space that excites these neurons like caravatures from. I want to hammer this home because the images that drove the neurons were and you mentioned this but I just want to reiterate that they were terribly unnatural right they're not not something that you would see well I mean there are elements that you would see in nature right but the majority of them weren't they just something that you wouldn't make sense. I don't know what even call them. There's some pixel you know conglomerations like. So there there were two studies that came out on the same day and I think the other images are even more scary. So this walk from Carlos Pauli and Marge Demonston Gabriel Kramin will shout so they're they were trying to control it or they're trying to like you know come up with the images for it those images look even more scary or the like they have because they have some kind of natural relevance they look like out of a horror movie or something but like the before the before images were more like textuary kind of images and we we are we are also restricting ourselves to like you know black and white images and things that so I think that that was part of the it was constrained in certain ways that led to those images but but as you were saying that yeah I did get a lot of attention and but but I think some folks have gotten excited about the wrong thing from the paper and the resulting images that drove before I think cannot be the protagonist of the story because I think that kind of became the story because like we like to say like faces excite it neurons or xyz excite xyz areas and I think in that formulation then it became about the images as sort of the our new understanding of the system whereas that was not about the images it was about look how what you can do with this model because this is the model that tells you that what is going to be the predicted neural response for any given image so I think that's what where we are in in in in terms of like that we think of this as a stronger test of the model because there are many models that can come up with different images then you can test those as well and I think there's work very very interesting work from Krigis korea Niko Krigis korea's lab about controversial stimuli I think those are the right kind of approaches at least to me like you pit these neural networks against each other and then synthesize stimuli and then test them it's a different kind of control experiment but at the end is basically about model separation and finding the best the best model it's not about looking at those images and making kind of stories stories about them yeah the the other side of this story though is that this should not make someone feel like oh you know solved go up your recognition this is the model why really they have yeah like so yeah I mean that that's the other thing I I feel like you know there's ways of presenting data that can prove our point it's a proof of concept study to me still it's like you know look like if you take this approach versus the other approach this approach like our approach is better or something like that's kind of the way to present the study but there doesn't mean that our approach is like the best approach or like we are done so do you do you have people suggesting that we're done do do I don't think we have people who explicitly suggest that we are done but they might use this as an example of like look how great the CNNs are and I think it depends on whom you're talking to because I can also use the same example to kind of like talk to somebody who's just basically saying oh CNNs have adversarial images and this is like a completely wrong domain of like models I can then use this example to say like look you can do some useful stuff but if I am coming up with things like you know you need recurrence and you need other areas to incorporate someone might go like but you can control reasonably well like why do you need to like incorporate all like so if you really look into the models you know look at the generalization of the models it's not that good it's like again not that is a very arbitrary like word use it basically yeah yeah but you feel like in some sense you're your own worst critic right because you you see all of the nuts and bolts and you see what's missing and what needs to happen and so do you feel like people are too complimentary or too impressed with the current work because I I mean they should be well yeah I think they should be but I think they shouldn't also like everything else they should just I mean I actually think this is our responsibility I mean to sort of also expose where the I mean if you read the two papers together like the neural control paper and the recurrence paper they're basically one paper is sort of highlighting how you can use them the other papers sort of highlighting like here are the images that humans and monkeys are good and models are failing so these are the ways to improve it so I think if you take all of these studies together then you might get a more balanced perspective and I think my goal at least I mean sometimes for a lot of reasons I mean you know better that like you need to sell the studies in a certain way but I think in these kind discussions are like in papers in the discussion sections like we we should always be highlighting sort of the confounds or the potential you know places to improve these models I mean even for core object recognition these models fail in very trivial ways that are maybe some people who are just reading this paper might feel like oh that's probably already solved yeah maybe they don't exist maybe this is the thing that I've created in my head more guilt more uh yeah absolutely I know that one of the things that you're interested in is visual reasoning right and I don't know if you want to explain why you're interested in it and what it is but one of the ongoing criticism so so non-human primates is kind of like the gold standard right in neurophysiology and you need an end of two you need two monkeys to publish classically but and recently there have been you know a lot of people working more and more in rodents and mice and of course there's always been the disconnect between mouse brain and human brain and one of the reasons why people like to study non-human primates is because it's like the closest thing that we can study that resembles human brains do you see limits to studying non-human primates to you know get at our intelligence so the reason why I ask you about the visual reasoning is because you're starting to ask so object recognition is a fairly simple thing right I know it's not simple but it's a you know we recognize objects but now you're starting to ask more cognitive higher cognitive quote unquote questions and I'm wondering if you see limits to using non-human primates for that yeah I think the answer will be sort of I mean my answer to that question would be maybe based on the kind of data that I will be collecting in some sense so the way I see this problem is that like you know ultimately at least for myself I'm not suggesting that everybody has this approach but I'm pretty human centric in my worldview and I think my goal is to find out like how humans solve particular problems so they are basically like the main model that I'm interested in so I think we start from human behavior on different tasks and ideally we'll have a model which is like currently maybe you know some form of a convolution neural network which has many areas other than ventral stream like dorsal stream we have seen and they will be kind of like predicting parts of the behavior of the humans and maybe at full capacity or something and I think at least one angle of approaching the monkey research would be like can I get some neural data that might be constraining for those models might improve those models my and usually I mean the way people go about it is that the collection neural data come up with an inference that is more can be like summarized as like a very smaller kind of principle like have recurrence or like like a smaller model and then they incorporate that idea into the bigger model and ask like did I improve my my model my my my bigger model I can do that I mean I'm probably going to do a bit of that basically like look like it looks like this other areas in the monkey brain is associated with this particular behavior and maybe that is going to improve my my my my my my my my development of the models the other thing could be like you just directly you know feed the data that you're collecting into the model building itself so you're getting a lot of monkey data so it's then it's a matter of like questions of like how much data is enough data and and I think we are getting more and more data so it's I think this is the right time to like start putting them in the models like so right now I'm involved in a project where all the data that I've collected is getting kind of filtered into the training part of the model and the models have been regularized with that data essentially so like and those models are becoming better predictors of core object recognition so like that is one way of bringing in the monkey neural data and the monkey behavior maybe to this problem the other way I think about this is that maybe you know humans and monkey share a very I mean it's not maybe it's probably proven in many ways that we share a very similar visual system so if even if I just get responses of the visual neurons in IT or other areas during showing some of these movies or some of these like you know videos on which the task is based off I can be providing constraining data for the model of like you know you need to be in this representational space and then solve a problem so like it's a two part kind of approach where the the neural data is basically constraining the representational space of the model and then on top of that you add a decoding layer that is the greeting those representation and you can have multiple ways of decoding the task and then you ask like which one you know or you can then compare it to human behavior and I think this is this could sound novel or surprising but like this is exactly the thing that Jim's lab like our lab has been doing for core of their recognition for quite a while where we were recording in monkey brain but then comparing the decoding models output to human behavior I have now started working like because I was also getting the behavioral data to from monkeys I have started now working looking at trial by trial and like image by image behavioral correspondences with monkey neurons and you know human sorry monkey behavior but it was basically monkey neuron human behavior we had a paper with Rishi Rajalingam looking at human key neural responses to like words and non words and their correspondence to human behavior on those sort of orthographic processing task so I think there's a way to like do this kind of separate it from a behavioral task because I think maybe if you're asking that do we does the monkey need to do the behavior for them to be relevant to this task and I think the same applies to rodents and other species it's just to me the the correspond like the ultimately again as I was saying continuously throughout this you know discussion is that at the end there is a model and whatever you do you need to kind of show that that adds to improvement of the model on something and now I from my just what we're talking about I can say like maybe my goal is like not to improve like prediction on human behavior to ceiling but maybe it's like if I'm doing maybe predicting behavior of neurotypical subjects versus you know people with with autism do I have some traction on that problem maybe like I can do you know like inhibitory excitatory imbalances I can create them more easily with chimo genetic perturbation in a monkey and then test what those representational spaces are and those could be like kind of constraining ideas for when you're building models of people with autism so I think there are many many ways in which and I'm I'm I'm saying all of these ideas and with the risk of sounding like a scatterbrain person like has to but I think at the end of the day I think these are the things that excite me so I think I won't be able to solve it all by myself so I am hoping that a lot of people who are kind of maybe similar minded we all come together and get a fry and tackle this problem so co neuro AI so you know a lot of your at least you know most recent career has been using deep learning models to shed light on brains on so this is the arrow from AI to neuroscience do you see and part of what you're doing also is using brain architecture and neuroscience some details to improve the models bit by bit like you were discussing do you see neuroscience helping AI or does AI not need neuroscience can AI just scale up and and go to agi or what that's a interesting question and also I think I'm probably not the my answer might not be that satisfactory just because of my lack of knowledge in a lot of these domains but I think I think of this problem in different ways so like if I think of this as like okay I'm going to build a calculator and should I constrain myself with the brain data no it's going to be like terrible calculator for scientific computing or something so like if that's the goal of an intelligent system is like to compute you know you calculate things fast and like then I think constraining it with neuroscientific ideas and data is like a bad idea now if maybe we can make a distinction of like behavioral data and actual neural data so I if I want to prioritize in my head like which data might be more informative to building models in for AI I think behavioral data will come first before neural data some of the examples might be like moral machines kind of data that's part of the MIT media like I think if we are trying to constrain a system to work like humans the human behavioral data I think will be key to to to constrain this I mean that's kind of been the success of deep learning right is because it the old way in neuroscience was to build a model out of kind of intuition and then compare it to data and the new deep learning approach is to build a model and train it to optimize it for a task like an animal or organism would perform and so it's all about behavior and lo and behold the model predicts neural data well also right yeah yeah definitely but that's I mean I was maybe making a slight distinction between like overall performance in a behavior versus like following the pattern of human behavior and the error pattern so like okay imagine a trained models are trying to get the labels correct which is a behavior but like humans might not always get those labels correct and like they might have different patterns so I think I was mostly thinking like this error pattern of like what kind of decision do we make given some kind of confusing stimuli or things like that those kind of data might be more relevant to models if they want to sort of operate in a human regime because I'm thinking of like a system that might be like you know helping somebody go through life who are unable to do the things in their life like that that machine or robot has to interact with with the person and then it's I think might be important for that that that system to be constrained with human behavior to some degree for those purposes I think behavioral data is very valuable at least that's how I think about it for example also AI in healthcare might be something that is might be very constrained and there I think maybe the neural data might have some bearing really on that I mean it still has to be shown I think I mean yeah but I feel like there might be some I mean as I was saying that this ideas of like you know how does the brain differ in a neurotypical subject versus the atypical subject that kind like it just depends on the scale of the data and how we're getting it that and that that's the relationship of the brain representation to behavior I think those kind of data might help us to build better models of the atypical systems and then use solutions that might be catered to the atypical system I mean now I'm kind of you know being very abstract by me I mean I can come up with like a dream sort of example where if you know exactly how like a system is learning for example a new task and you can do that for both atypical and neurotypical populations you might be able to use the atypical model to kind of come up with a learning sequence that produces neurotypical behavior even though it's a atypical system so I think that kind of that is definitely within I think the genre of like AI healthcare kind of like approaches so I think that way neuro to AI links probably are more clear to me I think generative models might have you know boost if they're regularized with neuro system data that that is another maybe no angle but yeah but it's it's so I would just not I mean what I'm mostly worried is that it's not like it doesn't obvious that if you have some brain inspiration or like neural data is going to improve AI models right that's that's what I'm kind of maybe pushing back against it's like maybe you can get behavioral data and that's enough and you don't need to poke around but isn't it interesting that you know these deep learning neural networks are based on 70 80 year old neuroscience like fundamentally the idea of a neural network back with even the logical units I mean yeah so and you're adding more biological constraints to your models so it's an interesting I mean I'm thinking of that like so the first part I agree that that's like you know that that's where all of these ideas might have come up and that's a good reason to keep you know looking at neuroscience for you know inspiration for building better models but if I look at the last 10 years I really don't see a concrete example of like you read paper in nature neuroscience a general neuroscience and took that idea and implemented in a model that drop out being used by at the end they're like engineering hacks like I mean yeah both groups like to use it as PR which is I think the reason why so it's great for that purpose but I think in reality at the end you can have an eye I mean I mean that's just fine to me like even if you have an idea of a drop out and then you figure out how to really like tweak it to make it part of a model that does something that's great and I think in that way it's really good to have neuroscience as a inspirational kind of umbrella on on top of everything good good for my career because I'll be able to talk to those but but I think I definitely think there is there is purpose of I mean yeah that will be used of neuroscience for AI but we need to be careful to not oversell it maybe or maybe we should I know but I think it's the other way around for me makes more to me it's more valuable especially because I think you know you're trying to measure data in the brain that is noisy this is like sample limited and then build theories and models around that like what to expect like how to think about high-dimensional spaces blah blah so like to me like once you have a model that is doing a very you know high-level behavior and very accurately that complex system gives us the opportunity to like really figure out how to even analyze a complex system like so it's to me that's a huge bonus from these networks because neuroscientists I think have been trying to do both things at the same time like like understand like build the complex system and then figure out how to analyze the complex system and here are networks that are already built up and you can formulate like different theories based on I think to me that's like a huge advantage of having these networks and this they they really become like the starting points and the high pot the may maybe base hypothesis for a lot of this neuroscientific experiments so that's kind of like at least how I have been mostly getting excited about the the crosstalk between the two fields we talked about how there's this kind of archaic fallacy I suppose for you know naming a brain region giving it a role right and the modularity of the brain prefrontal cortex does x that that sort of thing and we've talked about well I guess I mentioned about how language actually limits us in some sense do you feel like we understand what intelligence is do we have the right notion of what intelligence even is to start trying to you know to continue trying to build quote unquote AI I don't we I mean I know which which which scientists I'm we're thinking of like I think for me I probably don't have a complete understanding of what intelligence is but I have a friend of understanding of what kind of intelligent behavior I would like to build models for and so that's where I'm just a kind of the engineering engineering maybe like talking because I know what problem I have defined I won't know the solution so like this kind of task that are slightly above you know recognizing an object and like trying to figure out like what different agents are doing in a in an environment or like trying to predict what might happen next like this kind of behaviors I think are fairly intelligent behaviors and and my goal is to build models and and try to figure out how the brain is actually trying to solve that problem so in that way I'm fairly happy about the definitions of intelligence but then again we'll get into trouble like I'll get in trouble I'm saying what is intelligence maybe like the you know the typical like IT you know scores or I you know IT IQ scores I think they're heavily debated and so yeah I just feel like what I want to say is that we can keep debating about what is the right score what is the right way of quantifying intelligence but we have to do it in some way if you want to have any measurable progress so I have defined it in some way and I will keep you know improving that definition and you know expanding on that definition but but I think intelligent behaviors are to me not that controversial anything that I can do that my three-year-old son cannot do almost seems like a definition of like a little bit more intelligent but he might be learning faster than me so at this stage like the kind of definitions like that maybe that exists but like yeah you have a three-year-old I do have a three-year-old is that the only child we yeah yeah he's our only child oh man that's that's kind of a hard patch going through and starting a new job and all that so I feel sorry for you I mean it's a wonderful thing obviously but you know it's challenging early on so yeah yeah it's yeah yeah yeah yeah yep or are you gonna go ahead go ahead I must say like it's I'm happier on average like like in like after taking into consideration everything around the child I think overall I'm happier that we have a son that's the most I will say it's a tiny it's like a P yeah P equal to 0.04 yeah right I used to draw this a pie chart where I that I would show people like you know do you like having kids and it's like 51% yes right 49 no but yeah all right maybe I'll cut this because I sound like a real jerk are you are you hiring in the lab are you looking for students what's what's the situation yeah yeah I'm definitely looking for postdocs and grad students to work together in my lab so I think if folks are interested I mean the grad students are that they're basically gonna be recruited through yorks uh-huh graduate program yeah and the postdoctoral candidates I think I'm like just gonna you know talk to them individually and then see where the sort of you know alignments lie in yeah definitely like so if folks are interested in whatever we spoke about and maybe they read some of the papers and think this they're interesting directions that they might want to pursue I'm definitely interested in talking he's the future of neuro AI folks it's uh this has been a lot of fun co-congratulations again on the job and uh gosh I'm just excited for you it sounds like you you have a lot to pursue and um things are uh looking up not not that they're ever looking down but um congrats yeah thanks Paul I mean I mean there has been a lot of promises made I feel like I'm kind of like making a lot of promise and I hope I am able to deliver I feel like as long as I can quantify what those promises are I can tell you in maybe a year where I have been how much I have you know oh delivered so we'll check in in a year we'll check in we should check in but yeah but I'm excited I think I think this is worth doing so so I feel like I'm I'm all excited to get on with it this been great co thank you yeah thank you so much brain inspired is a production of me and you I don't do advertisements you can support the show through patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science go to braininspired.co and find the red patreon button there to get in touch with me email paul at braininspired.co the music you hear is by the new year find them at the new year.net thank you for your support see you next time cover
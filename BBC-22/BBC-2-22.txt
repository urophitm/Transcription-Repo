Hello, I'm Adam Rutherford. And I'm Hannah Frye, and in this special series of him and me, we're going to be tackling one of the biggest scientific and cultural ideas of our age, artificial intelligence. Yes, AI is the subject of the Reflectures delivered this year by Stuart Russell, professor of Computer Science at the University of California, and founder of the Centre for Human Compatible Artificial Intelligence. For each of the lectures, we're going to be taking his theme and getting stuck into the source code. And today we are deep behind enemy lines, embedded into one of the most emotive and consequential aspects of AI. That is, it's role in modern warfare. Yeah, now war and conflict are often hugely significant drivers of technological innovation, arms races to develop better communications, cryptography, security, and let's not be around the bush here, more efficient ways to kill people. But as the era of AI takes hold, we are at the dawn of a new kind of warfare, one which involves lethal or tonnumous weapons, weapons that locate, select, and engage human targets without human supervision. And by engage, you mean kill. Yes, kill. OK, good to know that you've from this is up front. So these are all fairly terrifying words and concepts, but this is nevertheless the world in which we're living, or at least are about to. This is something the Prime Minister Boris Johnson talked about to Parliament in November 2020. A soldier in hostile territory will be alerted to a distant ambush by sensors or satellites or drones instantly transmitting a warning using artificial intelligence to devise the optimal response and offering an array of options from somebody and air strike to ordering a swarm attack by drones or paralyzing the enemy with cyber weapons. Now to help us out on the battlefield, we are joined by a guest in the studio, Ulrika Franke, Ulrika is a senior policy fellow of the European Council on Foreign Relations think tank. She specialises in the impact of new technologies like drones and artificial intelligence on geopolitics and warfare. Thank you for coming in, Ulrika. Thank you for having me. Now, are there positive reasons for AI and automation within the military setting? Yes, definitely. So what's important to understand is that AI is not one technology, right? But rather AI is an enabler. So AI can enable all kinds of different military systems and military functions. And not all of them are basically frontline systems, combat systems, systems used to kill people, so weapons, but they are also really important users of artificial intelligence in logistics, for example. Just make things possibly cheaper, faster, more efficient and that ultimately is good for the military and also the taxpayer. I think that makes perfect sense. And that, of course, is also what AI is doing or can do for the whole of society more broadly. One of the focuses of Stuart Russell's re-selecture was really about the frontline, about the use of AI on the battlefield, with systems that run independently of human handlers. So let's just have a quick listen to that. It's exactly the same kind of autonomy that we give to chess programs. Although we write the chess program, we do not decide what moves to make. We press the start button and the chess program makes the decisions. Very quickly, it will get into board positions no one has ever seen before, and it will decide based on what it sees and its own complex and opaque calculations where to move the pieces and which enemy pieces to kill. Or should I say engage? That's exactly the UN definition, weapons that locate, select, and engage human targets without human supervision. There's no mystery, no evil intent, no self-awareness, just complex calculations that depend on what the machine's camera sees, that is on information that is not available to the human operator. So when it comes to autonomous weapons very specifically, can you help us understand a week what we are talking about? What are they and what are they not? So these systems, as was just said in the lecture, are military systems, weapon systems that are highly autonomous in the sense that you don't have a human being that controls the full targeting cycle they can. Find a target, confirm a target, engage a target, and kill someone without there being a human at any point who actually says whether or not that is correct. And this is kind of the most extreme case of autonomy in the military realm, right? Because you could have systems that aren't quite as autonomous. For example, where you do target acquisitions, where you find a target autonomously, but then you come back to a human being and say, well, actually, is this the target you're looking for? Do you want to engage it or not? So you could have different steps, but these lethal autonomous weapons systems or killer robots are those conceived to be as autonomous as possible, where it is indeed a machine that takes a decision over a life and death and decides to kill someone. So we're not talking about remotely piloted drones. We're not talking here about defensive surveillance to foil surprise attacks. We really are talking about killer robots that you can sort of fire off and forget about. Pretty much, at least, that's what a lot of both the public and the political debate is focused on, and also what's being discussed, for example, at the United Nations, where they're trying to come up with some kind of rules to ban or limit these systems. OK, Orik, our country is actually working on these types of machines. Who is taking this stuff seriously? Right now, we are in a situation where the big militaries around the world, so the United States, China, Russia, but also smaller players, are really looking into the different ways of how they can use artificial intelligence, of how they can use autonomy in their military systems, in their military operations, in order to make their militaries, yeah, potentially faster, stalfier, but also to develop really new military capabilities. Right? This is the holy grail of why we're looking at military technology in the first place. Come up with something that we haven't been able to do before, that the potential opponent isn't able to do. And so all around the world, really, militaries, arms industry are looking into different ways of using AI and autonomy in warfare, I'd say. If many countries are involved in this, if this is almost universal in developed countries, then what is the sort of timescale we're looking at? Which countries? What are the specifics of who is where? So one thing everyone has been looking into is so-called loitering munitions. And here I would say the Israelis have been quite leading, but other countries are not too far ahead. And we've had in particular two systems, the Harpy and the Harap. These are so-called kamikaze drones. So they're put up in the sky, arm systems. They're put up in the sky. They're told to watch a territory, so-called, killed box. And they're told if you detect the following signature, radar signature, for example, you engage it by diving into it, exploding with it, and killing it. And there doesn't need to be a link to the human operator again, so you basically put it up in the air, forget about it, and it does its thing. And those systems are in place. They're ready to be deployed. They have been deployed. They have been deployed, yes. So for example, we saw the use of loitering munitions in the recent conflict between Armenia and Azerbaijan, the Nagorno-Karabakh conflict. So that's one example. It's on the battlefield. Now, we don't necessarily know the level of autonomy in these systems, because in theory, they could also go back to the human operator and ask whether they should engage a target or not. But in theory, these are pretty autonomous and have been used. Another area of development where we're seeing really quite concrete examples are drone swarms. So swarms of several units, can be airborne drones, can be on the water, can be all kinds of units, where the United States, in particular, but also China, has really invested quite a bit of money and an effort to test these systems out, where a dozen or a hundred or even a thousand units operate together as one, right? And they can do waves of attack. They can create flying minefields. These kind of military capabilities. And that's very attractive from a military point of view, because again, here we have a really new military capability. We haven't seen before. But we also have, in Europe, for example, a French German Spanish project called F-Cars Future Combat Air System, which is kind of the European next-generation fighter that will involve quite a lot of AI. And as far as we know, may involve armed drones swarms, which again will need to be quite autonomous. There's quite a lot of other ephemisms that I've noticed that have been used to swarms, loitering munitions. That's a cracker, I'm just kidding. It's extraordinary. But we're still talking quite theoretically here at the moment. So let's just pause and consider the type of scenario in which an autonomous weapon might be deployed. It seemed to me that AI would enable a lethal unit to be far smaller, cheaper, and more agile than a tank or an attack helicopter, or even a soldier carrying a gun. A lethal AI-powered quadcopter could be a small as a tin of shoe polish. And this is where the shape charges and explosively formed penetrators come in. About three grams of explosive are enough to kill a person at close range. A weapon like this could be mass produced very cheaply. A regular shipping container could hold a million lethal weapons. And because, by definition, no human supervision is required for each weapon, they can all be sent to do their work at once. And if we know anything about computers, it's this. If they can do something once, they can do it a million times. So the inevitable endpoint is that autonomous weapons become cheap, selective weapons of mass destruction. Now, as you said earlier, Urike, these drones are neither science fiction, nor do we have to wait long into the future for them to arrive. You can buy them today. They are advertised on the web. And so, Adam, I had a look online and I found a couple of different drones that are thought to be capable of autonomous strikes. And they're already available, and they have very slick advertising videos that go with them. If you want to have a little look at this. So this is a website, which is pretty slick looking, and it's got a, oh, I don't know, it looks like a fairly standard quadcopter. And there's a video which I'm going to watch right now. So let's have a look at this. So when you look at this quadcopter initially, it looks like the kind of thing that could be tossering around your guard. Yeah, I've played with one of those. Some lovely video footage. All right, so it's hovering. It's doing what quadcopters do. And now they're showing, well, some mannequins, some sort of crash test dummy on the, dummy standing on the ground and there's the drone. Okay, now it's going in sort of military mode. And it's sort of pausing above. Oh, no, it's really going for it. And it's just exploded all over the heads and taking out quite a few people with shrapnel. That is, I would describe that as absolutely terrifying. Yeah, I mean, I have to say taking a little bit of a controller and view here, I think it's really important to make sure that we're really saying what exactly is the scary part and what exactly is the new part here. I think when it comes to, yeah, loitering, minniscient autonomous weapons, we really need to make sure that we don't confound the fact that they are military systems and are there to kill people with what's really problematic with them, which is the lack of human oversight, which is potentially a technological issue of the system being a black box and us not really knowing how it comes to a decision. And that's an absolutely valid point to make. I have to confess, I don't spend a lot of time looking at weapons systems. So it's all a bit surprising to me. I suppose another fundamental difference here is that where if you've got an F18 flying over a target, I mean, F18 is a pretty expensive, quite difficult to deploy, whereas in this scenario, if you have one killer drone, actually it might be quite easy to have a number of them at once. Right now, you know, the research is really expensive. These platforms are really expensive to develop, so we're nowhere there yet. But it is true that if we could have an autonomous drone systems or autonomous aircraft, it may end up being quite a lot cheaper than what we have at the moment. And so you could have more of them, which also means that more actors, so smaller countries, even non-state actors, could have them. And that, of course, is a bit of a scary future. Well, the idea of lots of them at once, the idea of a swarm is something that Stuart Russell has also been thinking about. Now here is a clip of a short fictional film that he was involved in making. It was a film that was screened to the United Nations in 2017 in an effort to alert leaders to the potential dangers ahead. Now, this time, this is a fictional promotional video about an imaginary CEO of a company. A $25 million border now buys this. Enough to kill half a city, the bad half. Nuclear is obsolete. Take out your entire enemy, virtually risk-free. Just characterize him, release this swarm, and rest easy. These are available today. Is this a realistic portrayal, here we go? It's definitely a plausible portrayal, because what we see in this video is basically just combining different AI-enabled capabilities, facial recognition, autonomy, and weapon systems that pretty much already exist. So, yes, it's plausible. Are there any armies that are testing swarms? Yes, many of them, including the United Kingdom. Actually, the United Kingdom, I remember, I think it was in 2019, where then the defense minister said that by the end of the year, the UK would have operational swarms. Now, that didn't happen. It was a surprising statement to begin with. But it kind of shows that, yes, there are a lot of militaries around the world that are looking into this. Now, listen, I'm going to slightly change tack here, because it's very easy for me with no military experience and very little understanding of the complexities of the theatre of war, to express naivety about the inevitability of war. So, I want to propose a couple of sort of devil's advocate positions, provocations for us to talk about. And the first should be like this, that is it's not simply better to have more accurate, more precise targeting of enemies than in a very simplistic way, how war has been perpetrated for the last 50, maybe a hundred years, which is to bomb a village or a convoy, where the risks of collateral damage are so much greater. Wouldn't AI technology just improve the efficiency of war in those grounds? So, I'd say more precision is generally better, but not at any price. And if the price is, for example, the fact that no one is responsible anymore, that we kind of forego the situation where this human being in the loop, maybe it's not worth it, arguably. Actually, I'm one of those people that says more important than the ethical consideration of the security policy considerations, where we could end up in arms-raised scenarios, flesh wars, so escalations that no one wanted caused by these weapons. But as we move forward in time, if these kind of technologies can get into the hands of rogue actors, it does change the playing field that you could discriminate based on somebody's face, on a target an individual person, or perhaps somebody who has a particular characteristic, like anyone who has red hair or a particular skin colour, or maybe even based on what somebody was wearing. Stuart Russell made exactly this point in his lectures. They could wipe out, say, all males between 12 and 60 in a city, or all visibly Jewish citizens in Israel. Unlike nuclear weapons, they leave no radioactive crater and they keep all the valuable physical assets intact. And unlike nuclear weapons, they are scalable. Conflicts can escalate smoothly from 10 to 1000 to 100,000 casualties, with no identifiable calamitous threshold being crossed. What Stuart Russell just described is something I would more expect from state actors simply because, you know, of funding, but what he describes is basically combining different AI and able technologies, or non-AI and able technologies to create a future that's really scary. I do want to just account to your devil's advocate position, if I may, because so far we've been talking only really about the use of these kinds of weapons in a sort of war-like setting. But that's not necessarily the case. There surely would be a possibility, or you could, that non-state actors would be able to get a hold of these things. It depends a little bit. I mean, right now they're still pretty cutting edge, so non-state actors aren't likely to get their hands on it, but maybe we'll see a future where they can get a platform such as a drone, and then they can literally just kind of download an algorithm from the internet that will allow them to combine those drones into an autonomous drone's warm and then use them. So that's definitely conceivable. Everyone has a phone that has facial recognition software on it, which five years ago, 10 years ago was just an impossible dream. But now that technology is available. Could it not be the case that in five, 10 years time, you know, you could buy this stuff off the dark web? Potentially, yeah, but then again, there are countries where you can buy a gun at Walmart. That's also really scary, right? So let's put it this way. What you describe is a risk and a danger, but of all the things that I can imagine where this development is going. This isn't the one that keeps me up most at night. What does keep you up at night? It's a possibility of an international arms race and actually, ascolatory international arms race, and what's been called flash wars. So the arms race scenario, the idea basically is that countries start acquiring, developing, manufacturing these autonomous weapon systems. And once they're in the world, potential opponents just feel so scared that they need to react by doing the same and acquiring, either defensive, but also offensive systems. And we get into this escalatory logic, which very often ends up destabilizing the world. So that's one. The other scenario that I also think is incredibly dangerous is what's been called flash wars. The term comes from flash crashes on the stock exchange, where automated systems reacted to each other and caused crashes within seconds or milliseconds. And the idea in the military realm is that you have an event, you know, something happens. It could be an attack, but it could even be just a bug or something. And an autonomous system reacts to it, attacks an opponent, the opponents autonomous system reacts to that and you end up in a war in a military escalation that no one wanted. And these two concerns, I mean, in many ways, there are echoes of what happened with nuclear arms races, nuclear accidental escalation here too. I'm thinking in particular of the story about Stanislav Petrov. Yeah, this was a Russian soldier in charge of nuclear weapons. He was sitting in his bunker at one point and he was being told that the Soviet Union was being attacked by the United States with nuclear weapons. He got this news and his job basically would have been to say, okay, counterstrike. Now, luckily, he looked at the situation and thought, you know, some things are odd here. I mean, first of all, you know, politically it doesn't feel like we should be entering a nuclear war right now. Also, I think the system told him there was one or two nuclear weapons flying towards him, which isn't exactly how you would imagine a real first attack. And so he basically decided that I don't believe the computer. I'm not going to retaliate and thank God he didn't because it was indeed a mistake and the danger is that a machine may actually say, oh, well, there's a nuclear attack. I'm going to retaliate now. Hopefully, and this would be the counter argument, maybe this AI enabled system would be so clever to indeed also realize all these things. But yeah, this is exactly the danger. And now the one difference, by the way, from between what we're seeing right now and the nuclear realm is that it is so much easier for even smaller states to procure and develop some parts of these capabilities than it was in stillness for nuclear weapons. Well, I suppose if we accept the premise that the only way to win this is for nobody to play, at all, well, that's not going to happen because war is part of our global culture, unfortunately. And it's kind of naive to think that we can just completely opt out of conflicts now and in the future. So I guess the key question is how do we maintain meaningful control? How do we continue to, how, as you say, a role in making the decisions when a lot of the action of these types of technology is autonomous? In the end, we keep doing this by A saying what we want and don't want to develop by making sure we understand what the implications are by keeping, yeah, I'm an Ontario's and the Iron Family Manufacturers accountable, but trying to come up with some kind of international laws or international norms, I would rather say, but this is difficult. I wonder whether, I mean, this is an awful thought to have, but I think that history probably is listed with examples of what I'm about to describe, which is that the only way that people begin to become fully engaged is when there's a disaster. Or when instead of a school shooting in America, when are we going to see the first drone shooting in a school in America? And then it becomes an unavoidable part of the public consciousness. Yeah, I think you're right. And that's very likely. So for example, the fact that law-turing munitions were used quite extensively and quite successfully in the Nagorno-Karabakh conflict really put this on the map for many political decision makers that maybe a little bit less for the public. Often it is really the disasters and the accidents and the terrible situations that bring these discussions forward. There are people who are really calling for change. I mean, the human rights watch for instance, who is campaigning to ban these kilobytes altogether. But also there are agreements among computer scientists to sign up to some sort of code that says that they won't design the software that's required by these types of machines. What are your views on the effectiveness of those kind of interventions? Activists have actually played a really important role. I would argue that it's primarily the campaign to stop kilobytes and which is an activist group that has put the topic on the agenda for many states around the world and specifically for the United Nations where in Geneva the conference was certain conventional weapons. CCW, they've been discussing banning or regulating these systems since 2014. So I think the public pressure here is quite important but it can only go so far because in the end you need the states really involved with efforts of arms control or trying not to develop these systems. When it comes to the private sector it can be difficult for scientists to really see whether something is civilian or military how it can be used eventually and to come back to to Adams earlier point whether this is necessarily bad because just because it's used by the military if it's the military of a democratically controlled state making that military better arguably isn't the bad thing. So this is tricky. What do you think is the likelihood that we will get international political agreement on how to move forward with this? I think it's very unlikely to be honest. I mean I've been following the discussions in Geneva quite closely and they are largely stuck for a number of reasons. A because it genuinely is tricky and B because the technology is developing and there are different interests and the current political climate is tricky. To be honest I don't expect any kind of real treaty or a ban or anything like this to be put into force anytime soon. That being said I still think that these discussions are incredibly important because they could establish at least a little bit of soft norms, soft law and agreement of what we really need to avoid. I acknowledge and awareness do not go completely in the wrong direction and then wake up one morning and realise that we've really took the wrong path but I have to admit I'm somewhat pessimistic. Okay final question and I suppose this is a question which is thematic for the whole series really but I want to ask it specifically about what we're talking about, this subject which is AI in warfare, is there something qualitatively different about the development of AI in warfare compared to the many many historical technological developments of I don't know just the 20th century that have enabled more efficient and more effective warfare? I would say yes. I think the biggest difference is the fact that you no longer or you may no longer have human beings making decisions over life and death and you may have computers autonomously taking these decisions and that is a qualitatively new thing. And yet again it sounds like we really are entering into new territory with artificial intelligence. Well thank you very much for joining us Ulrika Franca. Next week's episode is going to be fractionally more cheery but no less important. We're going to be talking about how AI will change the way that we work. Let's imagine that technology creates a twin of every person. Your twin is a bit more cheerful, a bit less hungover and willing to work for nothing. How many of you would still have a job? I'm Hannah Fry and I'm Adam Rutherford and we'll see you next week.

Hello, I'm Professor Hannah Fry. I am a mathematician who writes about artificial intelligence and its role in society. I'm Dr Adam Rutherford and I am AI Curious. Now, we're taking a break from our usual curious cases for a different kind of investigation. Over the next four episodes, we're going to get stuck into the world of artificial intelligence of sentient machines. And what all of this means for the future of humanity. Hey, no big deal, guys. We aren't going to be wading through the weeds of some of the biggest ideas in machine learning, delving into the depths of what data can and can't tell us about ourselves, and unpicking some of the mysteries of what lies ahead. Now, in this series, we're bouncing off the back of this year's re-flectures, where the supremos of public intellectuals opine on the issues do sure. This year's lectures are called Living with Artificial Intelligence, and they're being given by Stuart Russell, Professor of Computer Science at the University of California at Berkeley, Holder of the Smith-Zuddy Chair in Engineering, Director of the Centre for Human Compatible AI, and author of Artificial Intelligence, a modern approach. Seems like a appropriate list of credentials for such a role. Now, for each of the four lectures, we're going to pick up on the theme, get hold of some of the ideas that Stuart has discussed, and take them apart, examine their circuitry, and solder them back together for you. In this first programme, we are starting with a surprisingly difficult question to answer. Just what is Artificial Intelligence? Here's what our re-flecture Stuart Russell said on the matter. From the very beginnings of AI, intelligence and machines has been defined in the same way. Machines are intelligent to the extent that their actions can be expected to achieve their objectives. But because machines, unlike humans, have no objectives of their own, we give them objectives to achieve. In other words, we build objective-achieving machines. This whole idea started way back in the 1840s. Adelauvl√© said that machines would eventually be able to do whatever we know how to order them to perform. So basically machines that could follow a recipe? Exactly, and then Alan Cheering a full hundred years later, picked up the button and added that he thought machines could give rise to surprises. That if you set an appropriate objective, it was possible for computers to achieve it by doing something that could be described as thinking. Right, well that's not following a recipe anymore. That sounds more like the sort of technical challenge on Bake-Off. So you can give them the ingredients and ask for a pomegranate glaze, and the machine has to work out how to make it without actually seeing the instructions. Exactly, and that is where we are now, Bake-Off Technical Challenge with AI. If you're careful, it's possible to set up an AI to complete a task without giving you any instructions on how to do it. Okay, now I think part of the issue that we need to discuss here is that for most people, myself included, our understanding and knowledge about artificial intelligence mostly comes from pop culture, it comes from movies like The Matrix or X-Mac and Er... Hey, did you work on that film? I might have worked on X-Mac enough. You should really tell us about that one day. Yeah, I'm sure I will. Anyway, well I'm guessing that these films aren't factual accounts of how most AI research is actually focused at the moment. Yeah, weirdly no, weirdly it isn't. Tell you what, let's ask our guests. Today we'll be joined by Azim Azar, an entrepreneur and analyst and author of the book, Exponential All About Technology, AI and Society. Anne Kate Crawford, who works for Microsoft Research and is the author of the Atlas of AI about the planetary costs of artificial intelligence. Okay, so Azim, what is your definition of AI? Well, I think it's a very hard thing to define for an ordinary person. I think of AI a little bit like having a stroll in Paris during the spring time. There's something that is recognizably Paris in spring, but is it the Eiffel Tower? Is it the angry waiter? Is it the beautiful espresso you're drinking? Which of those components makes it Paris in spring? And I think the reality for most of us is that that's what AI is. We sort of know it when we see it and we know it when we don't. That is a beautiful description. It really doesn't answer the question at all. But it also poses, it tells us why this is such a difficult question to ask. So what's the difference between machine learning and artificial intelligence? Well, machine learning is a particular type of software that learns from experience and to use your bake off analogy, it is a piece of software that you could give the ingredients to and without the detailed recipe, you'll be able to figure out what the final bake should look like. The very earliest versions of artificial intelligence needed to have every last step spelt out in great, great detail, which made them very, very brittle. And so today's systems largely use machine learning. And when we say artificial intelligence, we kind of mean machine learning now. They've become synonymous. Kate, turning to you, it seems to me that there are as many definitions for AI as there are AI researchers. What's your definition? Well, of course, there are a million technical definitions. And I won't bore you by going through all of those. But one of the things that I think is really important when we try to define AI is that we open the lens beyond just looking at technical practices to look at the social practices and the infrastructures behind AI. So AI is not just a set of algorithms and data, it's also a set of people in rooms deciding what problems should be solved and what ways they should try to solve them. All of these components are actually incredibly important to shaping the AI that we have in our everyday lives. So in that sense, I think the term artificial intelligence sometimes sends us off on a merry chase into sort of visions of science fiction, rather than actually looking at the fact of how it actually works and fails today. Okay, Azim, how do we know if something really is artificial intelligence, rather than, I don't know, just a spreadsheet or some fancy software? Well, one of the things that we'd expect from traditional software or spreadsheets is that they'll do the same thing every single time. There won't be any arbitrary or strange behavior that crops up. Whereas with an AI system, we might expect the responses that we get to vary. If you think about how you might use Netflix, well, you get given a different recommendation for a film to the one that your partner might get, or to the one that your mum might get, even though you're both asking the same question, which is, what film should I watch? What other examples are there of artificial intelligence, which are genuinely embedded in the way we live our lives from the moment I wake up to remember that I go to sleep? Look, I think a great example is when we hop in our cars in the morning and we use the GPS system. So the way that the GPS system figures out which roads we should take, the way that it might adjust according to the traffic, those are all fundamental problems of the domains of artificial intelligence. OK, well, it's here and it's part of our lives. So let's talk about the reality of where we're at with all of this, because AI, as it already exists, has no shortage of success stories or failures. Right, well, in my world, roughly meaning biology, loads of people are already using AI, and here's an example which I think is a sort of definite win. In drug discovery, because creating new drugs costs, billions of dollars, and it sometimes takes decades of work to get from the disease to the treatment, and mostly those pathways end in failure, because biology is really unpredictable and really difficult to model. What AI is becoming very good at is making better predictions about the molecules that drugs can actually target in the body. So have a listen to what Daphne Coller adjuncts Professor of Computer Science at Stanford University, it says about this, she uses AI to help design drugs. In the last couple of years, we have started to see some successes. There are drugs on the market that use elements of machine learning in their design, and by using those have been able to greatly accelerate their path through an effective medicine. I would say that in the context of COVID, Moderna uses maybe not fully sophisticated machine learning, but data science elements and how they design their vaccines. So COVID, which caused this incredible acceleration in the ability to try out new things, has given us a few of these sort of early success stories. I do believe that in five years, we will have in the clinic drugs where again, a machine learning model contributed extensively to the identification of the target three years ago, when we started in CETRO, we had a lot of skepticism. It ranged from people thinking it was all a bunch of baloney, that mindset has disappeared, and everyone now realizes that it's going to be a hugely important factor in how we discover and develop drugs. There's some pretty impressive applications there. What are the stories that you're most excited by and impressed by that you've heard of at the moment? I think the biology one is really, really impressive, and it's what I'm most excited about. It's been so hard for us to take the tools of science and engineering and apply them to this realm that we all live in. There are 10 to the 80 atoms in the universe, so that's 10 with 80 zeros, roughly speaking. But if you look at a single protein of which there are many, many, many millions, a single protein can fold up in 10 to the 300 different combinations, that's 10 with 300 zeros. How they fold determines how they behave, whether they are helpful harmful or just frankly a bit useless. We've not been able to take techniques to that to automate the discovery of exactly the right protein that we need, because it's been computationally too difficult to do. These new techniques of artificial intelligence are really starting to prize open that door. I think well beyond the realm of pharmaceuticals and healthcare and drug treatments will start to see this impact throughout our industry. That's one area I'm genuinely excited about. It's not only in biology and healthcare though, right? There are advances in education and transport and art and design. There are lots of things to look forward to. There's a lot to look forward to in the educational context. There's a great app called Photomath, not Maths, unfortunately. It allows you to take a photo of a particular problem and then we'll solve it, but it will give you step-by-step instructions on how you solve that problem, which means if you're doing A-level maths, you've got a tutor who can assist you and in a sense teach you some of the ways through solving that problem. Only one equation at a time there if it's photomath rather than the American math only, perhaps. Okay, there are some big ticks in the plus column there. Kate Crawford, I want to ask you for some examples where it hasn't all been Sunshine and Oli Pops. Oh, well, look, it's a very long list, Adam. Where to begin, we've had voice recognition software that fails to recognize female voices. We've got facial detection systems that don't recognize women of color. Facebook and Google have shown more highly paid ads to men and not women. There, of course, was Apple's credit card scandal where it was offering up to 20 times more credit to men rather than women. But what is, of course, interesting is that all of these failures point back to how AI systems are designed and quite commonly, it's because they're trained on the data of the past with all of those structural inequalities and issues buried within it, which is then projected into the systems of our future. I guess there are other concerns about this stuff too, that there might be a danger when you're giving an algorithm an objective, but you know that the algorithm doesn't really understand context, doesn't really understand new ones. Are there examples that you have of where AI's maybe been a bit naughty? AI will often be naughty because it doesn't understand the objectives and we can't give it any of the common sense. So there was an example when researchers designed a system to play the computer game Tetris where all the blocks fall from the sky and you have to stack them together and you have to stay alive for as long as you can. And the AI figured out that if you press the pause button metaphorically speaking, well, the game would stop for a moment and you would have no chance of losing any lives and that would become the optimal strategy for staying alive. It works. Not the most fun games. Not quite in the spirit thing. But there are serious aspects of this too. There are serious aspects of this because we use AI systems throughout our economy to do things like decide whether people qualify for mortgages or for credit cards. And so if you specify an objective to narrowly and you don't take into consideration all of the wise and where-for and the nuance, you might end up pressing pause on someone's mortgage application because that's the best thing to do rather than offering them a mortgage at decent and fair terms. And setting the right objectives. This is a problem that is only going to get harder as we go forwards in the future. Here's Stuart Russell again. Now the problem is that when we start moving out of the lab and into the real world we find that we are unable to specify these objectives completely and correctly. In fact, defining the other objectives of self-driving cars such as how to balance speed, passenger safety, sheep safety, legality, comfort, politeness has turned out to be extraordinarily difficult. This should not be a surprise. We've known it for thousands of years. For example, in the ancient Greek legend King Midas asks the gods that everything he touched should turn to gold. This was the objective he specified and the gods granted this objective. They are the AI in this case. And of course, his food, his drink, and his family all turned to gold and he dies in misery and starvation. We see the same plot in the source or as apprentice by Goethe, where the apprentice asks the brooms to help him fetch water without saying how much water. He tries to chop the brooms into pieces, but they've been given their objective. So all the pieces multiply and keep fetching water. As in, I wonder if you can bring this up to date for us because there is a more modern version of this same idea. The more modern version is the AI that we ask to make paperclips for us. Now this is a theoretical philosophical game, but it was one that was presented by a philosopher based in Oxford called Nick Bostrom several years ago. And in this story, the AI has to make as many paperclips as possible. That's the objective that we set for it. So it starts by making paperclips and taking in deliveries of whatever metal is needed. And then it realizes that humans are driving around in chunks of metal or cars. And there must be millions or billions of tons there. So it controls these cars and grabs them in order to melt them down and turn them into paperclips. And then it realizes, well, it needs a lot of energy to melt the cars and to bend them into paperclips and to make more paperclip making factories. And that humans pesky little things that we are, we are using up lots of energy to grow food and to cook that food and to keep ourselves warm and to keep ourselves clean. And so it starts to use those resources for its giant network of paperclip factories and so on and so on. And all based on that single objective of maximizing number of paperclips that you make. I mean, this is fundamentally the plus of the matrix, right? Yeah, I think there are a lot less paperclips in the matrix than... Well, yeah, I mean, I still would watch it. Now, what you're talking about here is the idea of misaligned objectives. I should reassure you that there is a key reason why we don't have to worry about that particular outcome yet. Because mostly AI at the moment just does one thing, right? You know, because a chess-playing machine can't work out your taxes. It probably can't even play noughts and crosses. A paperclip buying machine won't be able to do the things you've just described like go on eBay or meltdown cars. Well, it won't be able to right now. That's absolutely the case. The AI's we have are really, really narrow. But there are some AI researchers who hope that this might not always be the case. Some who are aiming for artificial general intelligence or AGI. Here's to it, Russell. The goal of AI is and always has been general purpose AI. That is machines that can quickly learn to perform well across the full range of tasks that humans can perform. And one must acknowledge that a species capable of inventing both the gravitational wave detector and the Eurovision Song Contest exhibits a great deal of generality. Now, inevitably general purpose AI systems would far exceed human capabilities in many important dimensions. This would be an inflection point for civilization. Do you think that's true? Do you think that having generality would be that transformative? Well, this is the interesting question. Is whether you believe that these types of technical systems can be generalized? This idea that we're headed towards sort of mythically powerful AGI should actually be questioned a little further. I mean, I noticed that Stuart sort of refers to the fact that we could have systems that perform well across all of the tasks that humans can perform. Well, let's just think about the sorts of tasks that humans perform today like just this morning. I gave my kid a big hug and made some lunch so he could take it to school. I don't see an AGI system capable of doing that. These questions around human emotion, around embodiment, around relationality, and around context. These are things that actually computers are very bad at and we've had no real progress in many of those domains. So I think we have to have a little bit of skepticism about whether AGI is actually coming in the way that it's often been told that it will be as something so clearly superior to human intelligence. But it might actually be something quite different. You know, I really resonate with what Kate has said there. It's very easy for us to fall into large questions that might only become relevant quite far in the distant future when in the here and now there are practical benefits from AGI and there are very real practical dangers. And I'm quite glad personally that a number of philosophers are worrying about the bigger questions. But I'm also glad that it's just a small number of philosophers because where we live today, there are lots of benefits and lots of risks that we need to deal with. And yet there are a number of very serious scientists who are thinking about AGI and who are convinced that this will be a very big deal. Stephen Hawking too had something similar to say on the matter. Success in creating AGI could be the biggest event in the history of our civilization. But it could also be the last unless we learn how to avoid the risks. The rights of powerfully I will be either the best or the worst thing ever to happen to humanity. We do not yet know which. Stephen Hawking saying this is either the best or the worst thing that could ever happen to humanity. These are some pretty dramatic sentiments we're expressing here. How might we expect AGI to compare to other enormously transformative technologies that we've been invented in the past such as the internet or telecommunications or electricity? Those are really big questions. I think that AGI will be really really important and certainly as industrial technologies go, things like comparing it to things like the car or telephone or electricity would be really quite reasonable. For people in the 21st century, we largely don't think about our electricity. But certainly in the 1920s, you did think about electricity quite a lot. And we're still at that thinking about it quite a lot stage. But over time that will change and AGI will largely disappear into the background. What do you think is a more realistic view then of the long-term future once we achieve AGI? Well, I think immediately I'd stop you there and say once we achieve AGI, I mean, I think even this idea of artificial general intelligence is itself quite suspect. And what you'll notice is that even in the debates about AGI just in the last five years, companies have been moving the goalposts. They're saying, oh, we're a step closer to AGI. Is this potentially AGI that's now general? And we've seen that with protein folding. Are we moving towards AGI? Well, in actual fact, we're not. We're just creating more sophisticated systems that are better at actually seeing patents in large collections of data. So let's be really specific about what we're talking about. And I think hopefully we can demystify some of this quite sort of theological framing of AI. Hypothetically though, if something like AGI were possible, what would that look like to you? Well, I mean, it's interesting. I think we could look to the people who are currently making the most dominant AI systems in the world, that's sort of the big five technology houses. By which we could say Facebook, Microsoft, Google, Amazon and Apple, and say, what are the priorities and what are the sort of political economies driving those companies? And how would AGI be used to actually further those interests? And in some ways, I think that should scare us more than visions of robots throwing over humanity. It's actually more about look at the types of corporate priorities and the types of concentrations of power that we've seen in just the last decade and then times that by a thousand fold. And then we might be looking at something like AGI. So I think with AGI, if such a thing were possible, and I think it is very much hypothetical at this point, we'd want to be asking who is creating it? What are their priorities and how are they be using it to benefit themselves? Because I think in many ways, this is why we see so many of the interesting debates right now about how do we make sure these are public interest technologies that are actually public utilities rather than privately owned and proprietary systems? If this is something that is potentially as powerful as we're hearing here, does it matter who gets there first? Do we risk a metaphorical arms race here between the world's leading powers? It's sort of a winner takes all race to get to AGI. Oh, I think we're already there. We're certainly already seeing that used rhetorically as a way to generate the type of AI war with all of the sorts of defense funding priorities that would come with it. We do have a small number of countries who are really leading development of these technologies. Here's Ecclip actually on what Stuart Russell has to say on the subject. We have Putin, we have US Presidents, Chinese Secretaries talking about this as if this is how we are going to win. We're going to use AI and that will enable us to rule the world. And I think that's a huge mistake. One is that it causes us to cut corners. If you're in a race, then safety is the last thing on your mind. You want to get there first and so you don't worry about picking it safe. But the other is that general purpose or super intelligent AI would be essentially an unlimited source of wealth. Arguing over who has it would be like arguing over who has a digital copy of the daily telegraph or something, right? It doesn't matter. If I have a digital copy, doesn't prevent other people from having digital copies and it doesn't matter how many I have, it doesn't do me a lot of good. So I think we're seeing on the corporate side actually a willingness to share super intelligent AI technology if and when it's developed on a global scale. And I think that's a really good development. We just have to get the governments on board with that principle. Kate Crawford, Stuart Russell just said then that we shouldn't think of it as an arms race, but you think that we're already there. So how do we square that circle? How do we develop AI so that it isn't an arms race? I love the fact that Stuart points to the fact that the minute you frame it as a race, people will cut corners. But there's another thing that happens too. Is you create a race to the bottom? If it's a race, then you're like, do whatever you can. It doesn't matter who you exploit or where you get data from because we must win. So it's not just safety. It's also ethics that I think gets cut out of the story here. So in so many ways, this actually requires a substantial reframing of how we actually think about the politics of artificial intelligence at that sort of national and international level. And as Dean, coming to you, if you had your way then, how would you smooth over the next few years of research? We've already seen the problems of cutting corners. If you think about what happened around the last couple of American elections and how misinformation and disinformation spread across the social networks, which are all driven by these algorithms, we saw the costs of running too fast and cutting corners and stripping out the safety and the checking. I think if we want to build technologies that are aligned with human flourishing, we need to figure out how to have a conversation about many of the issues that Kate has raised. Who is participating in the design of these technologies? How are they tested? Who frames what the objective function should be? And the good news is that there are some things that are happening in this community. It's known as the AI ethics community and you're starting also to see governments and international bodies realize that it's quite important with a powerful technology like this to be asking these questions. Now, I don't think we've necessarily figured out exactly how to turn that into the software code that's developed, but we are at least starting to ask the questions. Okay, well, we've been talking about metaphorical arms races today, but that does bring us to the topic of the next re-tletcher and our next episode, we're going to be talking about a real thing. Yes, next episode, we are going to be discussing warfare. AI in military technology and we're going to be debating the controversy and ethics around machines designed to make kill decisions, lethal autonomous weapons. It seemed to me that AI would enable a lethal unit to be far smaller, cheaper and more agile than a tank or an attack helicopter or even a soldier carrying a gun. A lethal AI-powered quadcopter could be as small as a tin of shoe polish. Thank you to our brilliant guests, Azim Azarenkake Crawford and we will be back next week. See you then. Goodbye.

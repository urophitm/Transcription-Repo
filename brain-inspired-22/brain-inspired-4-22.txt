 What A and D is? The assumptions are terms. Do you think is holding back neuroscience and AI? And why? I looked at the questions, I didn't really rehearse anything, I think it's kind of more interesting to kind of do it on the slide. This is Brain Inspired. Hey, my name is John Day, I just graduated from undergrad at the University of Michigan and I'm currently a research assistant at the IRCN at Tokyo on a professor in Mingbeau Kai. One thing that has been a major takeaway from the show is a recurring theme of our creativity. It's mentioned in different ways sometimes implicitly in a number of different episodes, but there are two in particular that stood out for me. The first one was with Sam Gersman in podcast number 28 where he said, quote, I think it's hard for students to follow the path of greater resistance because they look around and they see what's happening, what's successful, what's not successful, and it's natural for people to gravitate towards the things that they perceive as being more successful. But the paradox is that to be really successful, to really make a difference, to change the way people are thinking, to change the course of scientific discovery, you have to swim against the current. You have to pursue ideas that are not popular, that people might not agree with, that might not really make sense at first. You have to preferend to crackpots and read poetry and take long walks and do all the things that aren't ostensibly the right things to do, the productive things, the good uses of time. So we have to be really attentive to the unappreciated importance of uselessness. End quote. The second was episode number 86 with Kent Stanley and when she was talking about open endedness and constraints versus objectives. And that episode's family seems to make the case that pursuing something because it is interesting can be a good enough objective itself, especially when it comes to human innovation. I think these ideas are just as applicable as a way to approach research as they are a way to approach life. Interfering Sam Gerstman's thoughts in a slightly different way, I thought, you know, perhaps not only should I read poetry and take long walks, but in some way or another, perhaps AI and neural networks should be encouraged to do so too. Regardless of how silly that might seem, these moments for the podcast have served for me as a reminder of the importance of doing things a little bit differently and the intuitive or human aspect of research. And these seem to be especially important things to keep in mind in the field of brain-inspired AI. In particular, I have already come across my fear share of people who are adamant about AI and neuroscience not belonging together. These reminders have encouraged me to trust in the validity of pursuing interesting things both of my personal pursuits as well as in the pursuit of understanding intelligence. Thank you, John. And thank you, Sam, for the memorable Crackpot quote. Yeah, let's all be more creative. Stick to our intuitions. I like that. Alright, on this fourth installment of the 100th episode Smorgas Board, I asked previous guests to answer the question posed by my Patreon supporters. What ideas, assumptions or terms do you think is holding back neuroscience and or AI? And why? So I'll give my own answer because why not? I think one assumption holding us back is the assumption we're all wanting to explain the same thing that we all have the same target of explanation or targets of explanation. The reason it's holding us back is I believe it leads to misunderstanding each other, misinterpreting each other because we have different goals and interests. I know I personally have had kind of a non-shelence or carelessness in general about specifying even to myself specifying what exactly the target is of my own understanding with respect to various questions. And I've really been led to this realization or opinion. It's actually just an opinion, of course, through many of the conversations I've had with guests that have been on this podcast, both on air and off air and, of course, through my own learning and exploration. As an example, something John Krakauer brings up a lot and you'll hear him in at least one of these episodes bring this up is the need for pluralism in explanation. The kind advocated by philosophers and historians of science like Hossuk Chang, Hank Direct, whom John turned me on to, and plenty others like Karl Craver who's been at this for a long time now. I think that's right and I'm all on board with that, but I also think there's likely some interaction between pluralistic explanations and just the varieties of targets of explanation. So in some cases, the pluralism actually might refer to different things to explain, although we often conflate those different things as if we're talking about the same thing to explain. An example that comes to mind to me is this recent brandishing, I'd say, of behavior as having an explanatory primacy over neurobiology for understanding brains and minds. To me, this all depends on the scope and the targets of what you want to explain and which of the 17 different approaches to levels of analysis you're using, of which Mars levels are the most famous recently. There's a rich philosophical literature on these issues with plenty of disagreement and it's something I'm enjoying continuing to ponder and it's made me more careful at least, although maybe not much more skilled yet in thinking about what some offered explanation is really pointing at is really trying to explain. Anyway, that's one thing I think is holding us back, but you're about to hear around 25 more better answers to this question. I list and link to all these folks in the show notes at brandinspired.co slash podcast slash 100-4 and as usual, I randomized their order of appearance during the episode. Alright, let's do this. Let's see what's holding us back so that we can solve all this stuff way before we're dead and maybe even upload our minds so that we can exist until the great heat death of the universe. Bit of suma. One thing that neuroscientists need to appreciate more and I learned that also in the last years is the importance of everything outside the cortex. So I'm working always in the cerebral cortex and I think it's a really important structure, but I think I increasingly realized that it's not all that's there. You have all these important subquarticle structures thinking of the thalamus, the basal ganglia de cerebellum and so there are many supporting things that these structures do, but also I think important processing steps that these subquarticle structures enable. And that is a really important question. I think we now have the technology also to rest these questions by having tools that allows the record from those subquarticle structures and also looking at the interactions between those subquarticle structures and the cortex. So I think that is where a lot of progress will be made in the next couple of years. Case Lindsay gets the computational neuroscience unit at University College, London. Okay, what ideas assumptions terms do you think is holding back neuroscience slash AI and why? I don't know if this is, I guess it's an assumption. I think that neuroscientists might actually think that the brain is simpler than it is in a certain way. And I mean, we don't have good intuitions about how complex hierarchical nonlinear systems behave. It's very hard to think about how those systems behave and to predict what they'll do. And that's, I mean, that's definitely what the brain is. It's a nonlinear hierarchical complex machine and our simple intuitions for how to understand it, I think might be leading us down a wrong path where we think we can study these very simple tasks and find neurons that encode the simple variables of the simple tasks. And oh, it's just this clear chain of this neuron fires and leads to that neuron. And obviously on some level it is. It's just neurons connected. But really actually understanding how information is encoded in a brain area and is passed on and transformed in other brain areas, it's just so complex. And I feel like I only really appreciated how non-intuitive interacting neurons can be when I started trying to understand artificial neural networks, trained artificial neural networks and playing around with those and just looking at how you can modulate activity at one level in a network and it can create these changes at another level that are just not exactly what you'd expect at all. And so I do think that perhaps we think linearly, we think as systems neuroscientists we kind of think, well, if the neurons that represent this thing in this area, if they go up, then they'll drive the neurons that represent that same thing at the other area and it's all a clear story from there and really, no, none of it's clear. I don't think I think we need to really embrace the immense complexity that just having a bunch of little non-linear units interacting can create. Okay, so my name is Marcel van KÃ¤rven. I'm a Chair of the AI Department at the Dunders Institute. I think what might be holding us back in neuroscience is a lack of realization that AI and particularly machine learning is very critical to understanding the brain. And of course, on your show, I think we talked to people who do realize this, right? That there's this big synergy between AI and neuroscience, but I think AI is definitely playing a very critical role, which might not be seen by all neuroscientists. And the reason I say this is that AI is ultimately about understanding the fundamental principles of intelligence, right? So as my guide to the AI, I guess, he's finding that at some point as AI is the science and engineering of making intelligent machines. And I think that's a very important definition. And I also see AI as a science, namely, really trying to understand what the mechanisms are that allow adaptive systems to survive in complex environments. And if this is not about a brain, then what is, right? So to me, that's a very important point. Andrew Sacks here, what ideas, assumptions, or terms do you think is holding back neuroscience and why? One is biological plausibility. It seems very innocuous. You'd like your theories to sort of fit broadly with our background intuitions, but I think the way it's deployed is, in fact, not helpful. So it's often used to say that a certain set of details about the brain are extremely important to be included in any model, right? So things like certain aspects of the electrophysiology of neurons. If you don't have them in there, it can't be biologically plausible. But I think the truth is the brain is so complex. It's very hard to tell what type of algorithm the brain could be running. And ultimately, I think science is supposed to be based not on intuition, but on hard evidence. And so rather than satisfying ourselves with intuitive judgment to plausibility, maybe a better approach would be to say, how can I falsify theories that come out? If you think a theory is not biologically plausible, then it should be easy to do an experiment which would really torpedo it. So for instance, maybe you think the brain doesn't do second-order, like Newton's method optimization. But really, how do we know? And the brain, so people will wax poetic about how complicated a single neuron is. Who knows? Who knows what could emerge out of the complex circuits of the brain? So it seems to me worth reserving judgment a bit and trying to understand how we can empirically test these principles rather than relying on intuition. And on the AI side, I think assumption, which may be as worth questioning, is that nonlinearities are fundamentally mysterious. So I think a lot of people have just given up hope that we will understand deep network systems. And I don't think there's reason to give up yet. Making progress on these questions, they're difficult questions. So it will take a long time. But I think we still can understand nonlinear systems to some extent. We have in many other fields. And so I don't see a reason to stop trying in AI. Hi, my name is Jane Wong. I'm a senior research scientist at DeepMind. What ideas assumptions or terms do you think is holding back neuroscience AI and why? Well in neuroscience, I think it's the idea that more neural data or recorded data is necessarily better. As we have better and better methods for recording and collecting brain data, it's sometimes assumed that we will necessarily gain increased understanding and insight into how the brain works. But I think that without a paralleled increase in our ability to model and create theories for that data on which to scaffold our understanding of that data, I don't think that it will create much additional insight. So I think we really need to focus on increasing our theories for that neural data. And I think that that typically means that we need to be tying that to behavior and to understanding tasks and what those tasks entail. In AI, I think that the idea holding us back is the idea that the better model performance is necessarily or is equal to better research. I think that working with toy environments and smaller models can often bring much more insight than deploying huge models on benchmarks that yield you, you know, gain in a few fractions of a percent over the current state of the art. The former, I think, is research, so getting better understanding and maybe a new sort of innovative way of modeling something, but that maybe isn't fully optimized. The latter, I think, is engineering. It's very impressive engineering. I don't always agree with the way that our field has equated that to being the best research. So I think that we need to maybe place a bit less emphasis on just benchmark performance and comparing two numbers and more on analysis and insights. And I think that a large part of that means that when you introduce new models, you should first validate them on much smaller toy environments and to make sure that you really understand what they're doing and that your performance is due to something that gives you insight or that you can truly understand. Thomas Nossalaris, Department of Neuroscience University of Minnesota. What assumptions are holding back neuroscience and why? I'll just speak about neuroscience. There's an assumption that there's a gold standard measure of brain activity. And that to truly understand the brain, all other measures of brain activity have to be converted into this gold standard. I think that that assumption equates the way that we think about phenomena that occur at different scales, I think in fact, depending on the question you're asking, measurements at very different scales are sometimes more or less appropriate. So voxels instead of cells, for example. I don't think this is a crisis necessarily, but I think there's a misplace emphasis on maybe spikes. I know that's heretical to say, but I think it's true. And I think we're going to learn a lot when we start looking for and trying to articulate computational principles that make predictions at multiple scales and for multiple different kinds of brain measurements. This is Steve Potter from the Georgia Tech Laboratory for Neuroengineering. What are the ideas assumptions terms that you think are holding back neuroscience and AI and why? Well, the belief that we are on the right track with deep learning multilayer artificial neural networks is holding us back. Deep learning networks have provided lots of advances lately, but to think of them as a way to help understand the brain, I think, is a big mistake. They are drawing attention further and further away from actual brain physiology, to create AI with deep learning systems. To me is like putting bigger and more complicated gasoline engines into a car when all along we should have stuck with electric motors and batteries, for example. We should stay focused on brain biology for inspiration for how to make more brain like AI. That's my opinion. My name is Kendrick K. What ideas slash assumptions, slash terms do you think is holding back neuroscience slash AI and why? I guess I'm going to change the question slightly. I'd rather answer the question, what not necessarily ideas holding things back, but like what is the lack or what's the deficiency or what's the challenge or why is this holding ever hard? I guess my answer is pretty predictable. I'd say something like the fact that the issues were tackling neuroscience slash AI slash whatever term you want to apply to all this kind of work, try and understand the brain or try and understand intelligent complex behavior is the fact that we lack a coherent, we as a field, I suppose, lack a coherent perspective. Meaning that there are many different ways to think about it, conceptualize it or not really share what the right way is to do it, many different systems of the brain, many different species, many different experimental methods, many different analyses, many different new sort of machine learning concepts and approaches and frameworks. And of course there's no magic bullet. We don't know which one is right so we keep exploring. But I guess what worries me, we're not really worried about what intrigued me is how as a field we can consolidate these things. It's very hard for one individual to do everything or even know just a sliver of what's all out there. And in fact, that reminds me of Paul's podcast. I mean by now Paul has heard so many diverse perspectives that in some sense Paul's the smartest person in the room. So maybe Paul can solve this problem. And I see this all the time in different subfields and people out there, they do their one thing, they know their thing, but they're sort of blinders. They're not really aware of the many other things going on out there and many different other approaches and ideas. But in some sense we need to consolidate. In some sense that's one of the goals of CCN to bring it back to that idea. And there are many extra science factors here. Like of course there's not an incentive structure that really promotes having a very broad integrative view of a lot of things. And we have limited resources. Of course, so we can't really know everything. So maybe the hope is that science is somehow a hive mind that like even though individual scientists don't know much, we will eventually somehow in kind of a swarm sense move in the right direction. And so I think of people as vectors. So as your vector pointed in sort of the right direction and like what's the vector average of all the scientists out there, is it actually pointed sort of incrementally in the right direction? Are we just sort of random walking all over the place? Which is obviously a pessimistic view. Blake Richards. There are a few assumptions, such terms, such ideas that I think are holding us back. One of the most important ones is the entire question of an eateness. I think that a lot of ink and shall we say characters get spilled over the question of how much an eateness should be in a neural network. And you know, to what extent is learning the answer, etc. And the funny thing is that I think that that's arguably a distraction from the reality which we've all known for a long time, which is that you don't want to fully hard code and eat things into the system, but you need a lot of good inductive biases to help encourage the system to learn particular types of things. And I think that the problem is that the entire question of an eateness sometimes distracts people. Myself included, like I'm not susceptible to this, because someone will count something in terms of an eateness or non-an eateness, and then the other camp who do or don't like that term will get their backs up. When really it's all just about everyone's reaction to this term an eateness. I think that the funny thing is that the actual amount of daylight between the different positions in this area is not that great. And so really the best thing for everyone to do is just to pursue their research strategy and try to find just the right amount of inductive biases or not to give them good results. And we shouldn't presuppose that X amount of an eateness is what's necessary or not. I think that distracts us often in these discussions. The other thing that I think has been holding us back in neuroscience AI as a specific kind of subfield is an obsession with having plasticity. To explain that a little bit, I see paper after paper after paper as a reviewer and area of literature and stuff where people are trying to do various different things with heavy and plasticity. And typically what they mean by heavy and plasticity is just that they're just going to take the correlations between presenaptic and postenaptic activity to do the synaptic weight updates. The reason that I think that this is holding us back is because I don't see what the utility of this constant quest to get heavy and plasticity to do everything actually is. What do we buy from all of this research? The reason I don't understand what we buy is, first of all, from an AI perspective, sometimes people like to suggest that these heavy and learning algorithms will produce something better for AI. But no one's ever, not only has no one ever demonstrated that, but no one's ever really been able to mount a convincing theoretical argument for why that would be the case either. Really if you can follow a gradient, why wouldn't you follow a gradient? Stochastic gradient descent works well. The other thing that I think the reason I think it's a problem is because I think the real reason that people get really into heavy and plasticity is they work on the understanding that we know that the brain does heavy and plasticity. And this is what I want to challenge everyone in the field on. The data on synaptic plasticity is extremely heterogeneous. And there are some experimental protocols where you get clear heavy and plasticity and other experimental protocols where it's anti-hebion, other experimental protocols where it's something else altogether. It depends on all sorts of little configurations in your experimental protocol. And furthermore, when it comes to actual plasticity in the In vivo animal, there's growing evidence that there's all kinds of complicated things going on that aren't incorporated into these heavy and models. So first and foremost, we know that neuromodulator systems are clearly having additional impacts on synaptic plasticity that are not considered in heavy and models. Another example would be astrocytes. There's growing evidence that, you know, real cells help to regulate synaptic plasticity in a way that would totally change the game computationally in terms of what you could build into your synaptic plasticity rule. And then one of my personal favorites is we know that a lot of feedback connections can influence plasticity. As we've shown in our paper, if you use that principle in a model with apical dendrites, you can even follow gradients. So I think given all of the computational and biological considerations, it's not clear that the answer has to be pure heavy and plasticity in the sense of like the learning algorithm can only pay attention to presenaptic and postenaptic activity. From either an AI or a neuroscience perspective, that's not a desiderada. And I think the field would actually gain a lot by letting go of the obsession with that particular form of learning rule. I'm Jay McClellan. I'd like to comment on the AI side more than the neuroscience side. I think in the AI side, there are lots of really smart people who are thinking very hard about encouraging greater degree of systematicity in our agents and our learners. And I don't think our agents have sufficiently systematic behavior as it stands, but I don't think the way to get there is the usual approach that I see most people trying to take, which is to figure out a way of building in some architectural constraint that is going to encourage systematicity. The successes of models like Bird and GBT3 and many, many other models, in my view, have a lot to do with the fact that they absolutely estue building in and explicit systematicity as such. The successful language models have created a situation in which they could use much more context than previously was possible with LSDMs. The attention mechanisms allow finding relevant information over a huge temporal swath of prior context without a gradient loss problem. And I think it's not the building in of systematicity that's resulted in greater success. What Chris Manning and his group have been able to show is those models actually exhibit a good deal of systematicity in their representations. They're extracting grammatical structure, but not because it was built in, but because they are able to exploit it through the learning process. What I look for is more progress, figuring out how to innovate with deep learning and other kinds of architectures without explicitly building in the solution you're looking for about letting it emerge. My name is Jim DeCarlo. There's a notion I notice a neuroscientist that they don't appreciate yet cross-validated prediction of X where X can be any kind of set of things as an actual goal of science. I've come to realize that scientists are taught that prediction is even what they should be trying to do. It's a strange concept. They think they're trying to understand. This relates to the idea that if you think you have an understanding, you should be able to predict. If you don't, you don't have an understanding. I believe that may be because neuroscientists aren't trained in always in that mold of what sounds like machine learning terms. We need to figure out how to take those terms and make neuroscientists comfortable with them because I think they agree with them in spirit, but we haven't, me among myself, haven't been able to communicate enough that that's actually what we all want to do. It's just how we talk about it differently. Talia Conkel What ideas assumptions terms do you think are holding back neuroscience and AI and why? Not the cuff. I think biological plausibility might be holding back neuroscience and AI just a little bit. I think, here's a question. Is it interesting if you end up with a model system that has some interesting representational properties, maybe they're even brain-like or human-like, but are clearly not solving it in the way that human minds and human brains are solving it? Is that interesting? Some people might go, well, it's not biologically possible. So no, why would I care about that solution? But if you take a more cognitive stance that representation exists and is provable is interesting. How did that get built? And what are its properties? And is it possible to arrive at a similar kind of representation through more biological properties once you understand that representation in the first place? I think it's perfectly a reasonable end. But even if it's not biological, I think it's an interesting kind of level of representation and approach anyway. So I think maybe they're sort of models as model organisms with solutions in and of themselves and representations in and of themselves are interesting from a cognitive science perspective. And I think biological plausibility can be sort of approached from the other end. Like we've got it built in a non-biologically possible way and it has a lot of the properties that we want, can we get there in a biologically plausible way or something that has levels of abstraction that sort of match a little bit more with our systems neuroscience level understanding? So I think that might be, I don't know, a slightly different perspective that might open up some new routes. Origazon from Princeton University. I think the thing that old as the most is the idea that the brain have to learn role, that the brain have to understand the solution in order to act. We always think about extrapolation as the part of the brain. The brain have a limited experience. From this limited experience, you have to learn the rules and act in completing new setups that you never been in. And I think what's true for the machine learning community and to the human brain actually, that the problem they're trying to solve is very different. They try to interpolate and to predict within the parameters they are learning. So then memorization and simple interpolation can be a solution that usually we despise a scientist because scientists we want to have the essence, we want to have understanding. For us, a model with 150 billion parameters is not a model. A model have to be like Newton-Low, they have to be like five interterbal parameters that we can really understand and manipulate. And it's like over-fit with billion parameters to billion data points to get interpolation to think that our similar in essence doesn't feel smart enough. And I think this is the main reason that stops us from having progress. This idea that the brain really learned the rules. And it's very difficult for people to understand what we are saying because as scientists, we try to understand, we don't try to act. And as a scientist, our mistake, we think that the brain is also a scientist, that the brain also try to understand and not trying to act. But actually, the brain puts more premium in action than in understanding the brain have to act and survive and do the right outputs. The brain doesn't care about understanding as much as we do. This is Wolfgang Mars from the Gratz University of Technology in Austria. A lot of theoretical work and modeling work also in theory and neuroscience is really based on the assumption that you have only one or two types of neurons, say excitatory and inhibitor neurons. But lots of no results tell us that there is a fairly large set of neurons which has different response properties and also particularly have different slow variables. And so therefore, I think that we probably need to face this also in our theoretical work. And for example, in the first step, when you look for results from dynamical systems or conceptual inspiration, almost all dynamical systems that I have seen being studied, they are consists of homogeneous units. They are all of the same type then. And I don't really see yet theory and conceptual tools to help us to understand dynamical systems which consists of 100 different types of components. Let's take a quick little break and then we'll get back to the responses. I think my favorite thing that I learned from Braini inspired is probably the work coming out of Jim DeCarlo's lap with convolutional networks in the visual stream because there is someone coming from the AI side and getting into the neuroscience side. It's just so gratifying to be able to bring that up every time someone says that deep learning isn't useful. Well, look at this, there's work they're doing with synthesized images and targeting neural populations. So that's just really fun to pull out out of the back pocket. The work that Megan Peters' lab is doing in metacognition, that was really interesting to learn how you could drive apart someone's performance on a task while increasing their confidence on that task even though they're actually performing worse and worse. One of the most important things I learned might be the fact that it's okay and really necessary to think deeply about philosophy when doing neuroscience, whether you're interested in the difference between explanation and prediction or you're contemplating what we can learn from evolution in terms of thinking about it as a process of information compression or thinking about the resource constraints that it places on the system and what impacts that might have for how the system develops or the usefulness of creativity and mind-wandering or the usefulness of mistakes and lapses. These are all things that I feel like don't get enough serious consideration when you're just talking science. But the guests on brain inspired, I think, do a really good job of reminding all of us how important it is to take a step back and think about the bigger picture. This is Paul T. Sek from the University of Montreal. Well in terms of neuroscience, one of the things that holds us back is that in order to solve a problem we need to define them explicitly. And I think we don't have the right definitions for many of the problems that we're trying to work on. So one example that I always say is that we define the problem of behaviors and information processing problem where the goal is to produce the right response given some input. And that seems like a reasonable way to define the problem but I think it's incorrect. And I think it's just as incorrect, as I'll make an analogy here, I think it's just as incorrect as describing a car as a device that converts chemical energy into kinetic energy. Which of course the car does that. But I think it's much more informative to think of a car as a device that moves people from place to place. And then energy conversion is just part of that larger process. So I think process confirmation is something the brain does. But it's just a means to an end. The real purpose of the brain is to control the organism's state. In other words, it's to generate outputs that result in right input. And so it's all about control. Once we define it that way, now we can talk about the component functions in a more correct way. And so I think in neuroscience, and AI, we think so much of sort of you receive an input, you think about it and you produce an output or you produce a knowledge or something. And I don't, I think that's like removing most of what the brain does and trying to treat this one aspect as a separate thing. I don't think that's a good definition of the real problem. It's Patrick Mayo. What ideas assumptions term do you think is holding back neuroscience in AI? Yeah, I don't, I don't think there's any specific terms holding neuroscience. I'll speak about neuroscience. Holding neuroscience back. I think it is very easy to fall into the trap of looking for phenomena that match the terms that we already have in, say, the English language. When the phenomena and the mechanisms for those phenomena in the brain may not nicely align to single words or already defined terms. I hate to say that we might need to define new terms since that's also a problem is people defining new terms for things that already exist. But I think about something like attention and fractionating attention into subcomponents and it's possible we're just going to need more specific or better terminology. My name is Conor Cunning. I'm a neuroscientist at the University of Pennsylvania. What ideas do you think are holding back in neuroscience and why? Well, personally, I believe the overly strong emphasis on reductionist explanation of thought is one of the main things holding back in your science. Why? The space of credible mechanistic models of things that are like the human brain is incredibly big. For example, if we say every synapse can have 10 different values, 10 to the 15 some synapses that then it means that we basically have and let's say each synapse can take one out of 10 values. We have a 10 to the 10 to the 15 dimensional space. If we measure a parameter perfectly, it gets us from the 10 to the 10 to the 15 dimensional space to the 10 to the 10 to the 15 minus 1 dimensional space, which is just as big as the space in which we're way before. Measurement only allows us to make progress if the set of models that we consider is small. And I think the field is being very imprecise about the dimensionality of the models that we're really looking at. David Pople and I work at NYU and at the Max Blank Institute. What's holding us back is that we forgot that we have a past. So our methods fetish has let us forget that our colleagues in the 19th century and the first half of the 20th century thought through a lot of these problems in a careful way already. And with some of my friends and colleagues, we've been arguing that the kind of behavioral, cognitive, computational decomposition of problems into their elementary constituents should play in more or should be more in the foreground to make real progress in neuroscience and perhaps AI as well. You ignore Helmholtz, Fesner, Tinbergen, Mar, etc. at your own barrel. Read old stuff. This is Brad Love from UCL. What ideas assumptions do you think are holding back neuroscience AI and why? I guess I could use my previous answer here, though that was more about AI. So let's focus more on neuroscience, where I think there's a different problem of integration. So there's so much research. There's more scientists than ever and we all publish more papers than ever. But all these papers are kind of little. And I'm not talking about like they're in little journals. I'm talking about the papers that appear in nature, neuroscience, neuro and e-life. They all involve narrow paradigm and kind of their own internal world and preferred brain area. And it's hard to see how it's all going to be put together. So maybe like a tedious social psychologist would be like, go on about experiment or degrees of freedom and replication issues. But that's not what I'm talking about. I'm talking more about theoretical integration. So it's a real challenge. I think how we're going to link all these findings together in some coherent whole. Modeling could help if it was applied beyond one paper, which it not often is. So that's one way is just having models that could link many findings together. So you can just basically understand the model, to understand the domain. But in general, I'm not really sure if we're on a path to integration that would lead to a human understandable science. And multi-level theories, theories of different granularities or levels of attraction certainly would help make sense of the increasing volume of findings we are creating. But I just don't think we could keep going on like this, like generating all these one-off hits, these papers that seem exciting, but the whole is less than some of its parts. The field is progressing for sure. Then of course, I guess there's all these fads that aren't helpful in red herrings, so objective aspects of consciousness, which isn't even a scientific question beyond cataloging correlates. So I guess there's just this problem of integration, and then this secondary problem of just fads and obvious dead ends. So I'm Rodrigo Canquiroga, neuroscientist at the University of Lester. Well I think it's what I say before. I think we need to look more deeply into the function of the human brain and see that there are some principles in the human brain that we are still not even trying to replicate in computers or in artificial intelligence. And maybe when we start to see how to implement these principles, which I have no clue how to do it, I have no clue how to make a computer decide what to forget and what to keep and extract the common sense. But if we start thinking about these processes, which are quite natural in the human brain, maybe this will lead to a very major breakthrough in artificial intelligence. I'm Steve Grovesberg. Part of the problem is reliance on old computational ideas. For example, deep learning uses steepest descent learning by gradient descent. But steepest descent learning by gradient descent was discovered by the famous German mathematician GauÃ. These are old ideas that have just been juiced up a little bit. And they can't explain autonomous adaptive intelligence, which is really, to my mind, the main challenge of future computational science. More often deep learning is supervised. You need to have a teacher giving you the answer on every learning trial. But much of the learning that we experience is unsupervised and indeed adaptive resonance theory can learn in arbitrary combinations of unsupervised or supervised learning trials just like we do. And we have a concept of the neural science that sounds prosaic when I say it is averaging, I think. So when we record, particularly in systems of neuroscience, when we record neurons in response to the animal being shown a picture or played a sound or we recall the neurons when the animal is moving or deciding or planning or whatever, then almost the reflex action, we have a neural response to the same thing happening, so it average it in some way. So we end up with a tuning curve which shows for a given neuron, its average response to different properties of the stimulus, so different properties of the angle of a bar or the frequency of a tone or something. Or we end up with the hypercom and sort of PSTH use these peri stimulus time histograms with graphs, you draw where you get the neurons firing during a particular period of the task and you align it to say when the sound happens and you average over its responses and you get this lovely graph showing its average response over time to that stimulus. But the problem with both of those things of course is that they are really useful to us as the sort of reader and observer to try and make sense of its complicated data. So they appear in papers and their historical legacy when you have to draw these graphs by hand. But obviously they are quite misleading about how the brain works. So the brain never gets to see these averages, right? The brain gets to work in the moment with what this barrage of spikes is happening, each neuron sees a barrage of spikes in a different set of its inputs each moment and that's what it gets to work with and it gets to be part of its own next barrage of spikes when it sends a spike. And the brain just gets to work moment to moment with this huge vector of spikes. But the PSTH in particular is so common that it's underlying, it underlies and tuning curves, underlie most of our theory development. So even people who work on incredibly complicated analysis methods of looking at high dimensional data or finding out what sort of ways neural populations are encoding simultaneously different properties of the task, like dot motion and color or encoding arm movements or encoding whatever aspect of the decision making process they're looking at. They often begin by simply everything in the neurons to get the PSTHs and then start from there. So they're just looking at the low dimensional or high dimensional of the average responses. Whereas the average response of course hides a practically infinite array of individual responses on each trial. So that average over say 100 trials could be having hiding 100 different responses that when average together have this one beautiful peak. And this really struck me, and I've been struck me this about a decade ago, a little bit of the work where I was looking at a single dopamine neuron and it was responding to a painful stimulus applied to a poor. And you do the PSTH and about 20 milliseconds after the poor was touched by a little electric shock, you got a big jump in the activity followed by a pause and then it goes back to baseline. That's the average over, you know, it was about 120 trials of this thing. But then when you look at the individual neuron responses and you take a clustering algorithm, and cluster those individual neuron responses together, you see there was actually three qualitative different responses that when the pine pool was stimulated, it reset where the neuron was firing. So you either received pushed it forward in time back in time or made it spike immediately. And those three different responses average together look like this beautiful peak and trough PSTH or which none of the responses individually looked like that. So presently because it sounds, I think averaging is taking us very far away from how the brain is actually working. John Crackauer, I think that the biggest thing holding us back is that we can't imagine that there may be strong pluralism. In other words, true multiple ontologies. In other words, it may well be that there will never be, I think what you would like there to be is a proper alignment of the mapping between the implementational level and the folk psychology of everyday life. In other words, it may well be that words like jealousy and lust and envy and justice are going to have their own causality and meaning and you're not going to be able to in some isomorphic way map them onto neural mechanisms. And I think that there's going to have to be an ability to deal with that forever just like maybe I've discussed this with you before that light can be a particle and it can be a wave. I think that's what's going to hold people back is not being able to cope with the possibility that there is some kind of, I think Karl Craver has talked about this recently. I think he calls it pluralistic preservationism. I think that's what we're going to have to accept is that sort of strong version of pluralism. And I think we'd be a lot happier if we accepted that in science. In other words, it's psychologists and neuroscientists could live together with that pluralistic preservationism. What I think is difficult is how to actually mimic the things we've been discussing by an AI. See that's a different problem, right? Explaining the brain versus creating a new one and all that pluralism in terms of understanding isn't necessarily going to immediately translate by taking that pluralistic stance into a better AI. It's not clear to me that getting the understanding right, getting the multiple or ontology's right is necessarily going to mean that we're going to create a common sense. I don't, it doesn't follow one from the other. Do you see what I'm saying? Hi Paul, this is Yuri. The first one is what ideas, assumptions, terms, do you think is holding back neuroscience and so on? The other one is related in my mind. Do we already have the right vocabulary and concepts to explain how brains and myus are related? My answer. Neuroscientists began to study the brain, buying into a system created by philosophers and psychologists for understanding the soul and the mind without ever asking how those terms whose brain functions we are trying to understand, such as consciousness, were brought into our thinking in the first place. Neuroscience has inherited this paradigm from such philosophy driven framework which portrays the brain or more precisely the soul and the mind as a tool to learn about the true nature of the world. Early thinkers used introspection and gave names to mental operations and now millennial later research for neural mechanism that might relate to their dreamed up ideas. Of course, an inevitable consequence of this framework, what I call outside in, is the assumption that the brain's fundamental goal is to perceive signals from the outside world, process, such information, correctly interpret them. In order to respond to these signals, an additional operation is needed. Wedged between the perceptual inputs and the organism response is the terrain of a hypothetical central processor. This is an entity that chooses what to do with the process of information. This poorly understood but often speculated about terrain has been referred to by various terms such as free will, homunculus, consciousness, executive functions, intervening variables, black box or more recently decision maker, depending on the experimenter's philosophical inclination or whether the hypothetical operation is applied to the human brain, brains of other animals or computer models. Yet, of course, they all refer to the same thing. The key assumption in this perception, decision, action paradigm is that information is processed properly so that something somewhere in the brain can decide to select the correct action. An implicit practical implication of this outside in framework is that the next frontier for progress in contemporary neuroscience should be to find the central processor somewhere in the brain and systematically elaborate the neural mechanisms of decision making. This is exactly what's going on at full speed in today's neuroscience. Over the past decade, decision making has become a bus term and applied to virtually all research without posing a bit and asking ourselves, do we know precisely what we are looking for? In my new book, The Brain from Inside Out, I argue that this outside in framework may not be the best strategy to understand the brain. Brain evolution didn't start out to generate a program where the end product should be the human level cognitive faculties. Instead, brains evolved to induce actions and learn to predict the consequences of those actions as afforded by a particular environment. The brain is not interested in the true nature of the world. Instead, its main preoccupation is to help its host to survive and prosper in its niche. I speculate that this action-centric approach is more strongly embedded in evolution. The neurophysiological findings are more compatible with it and the problems can be formulated differently, including the problem of the relationship between brains and minds. I suggest that by trying out this inside our strategy, perhaps some of the currently controversial term may become dispensable. The choice of a particular framework is important in our everyday practice because frameworks shape our ideas both about experimental design and interpretation. I don't think this is the strategy that my suggested strategy is perfect. Yet, I believe that this alternative is perhaps more fruitful than the currently dominant outside in framework that so strongly influences AI. So my name is Stefan Leiden. So I think one thing that is holding back neuroscience and AI is this idea that we can simulate to bring to the low level of detail and that will answer the questions that we have about the higher levels. Of course, it's a very exciting idea and it's a worthy pursuit to make a scalable answer to see how big we can get these virtual rates and also to maybe gain insights into what happens to these systems is they exist of not thousands of millions or billion parts. Many things can emerge and the behavior can be unexpected. But I think the problem is that in order to explain what happens at the higher level, we need to have an understanding of how we can get from the lower level to the higher level. And I see this, for instance, in quantum explanations of consciousness. So I'm okay with the idea that consciousness is a quantum process and someone might be able to convince me that all this quantum process is a consciousness happening, but it doesn't explain the fact that I'm conscious and my spirit is not or that I'm more conscious than my cat is. I think the only way we can solve a problem like that is if we look at the higher levels and think about how we can move from one level to the next. And what we need to let go of, and this is, I think, the most difficult part is the reduction is idea that there's one single level at which everything can be explained. And this is typically the level of physics. Only enough when you think about quantum physics is actually the level below physics, what we still tend to call it physics. But moving upwards, we always have this sort of gut feeling that the real science is expressing it in physics. So if we can sort of go down from consciousness to psychology to biology to chemistry, we end up with physics and that's the level which we explain it. I think if we can let go of this idea and actually give a sense of almost like a validity at these higher levels and explain how you can move from one level to the next, I think that will open up a whole box of possibilities and also really pivotal in moving the neuroscience of the AI field forward. I'm Nathaniel Daw at Princeton University, Princeton neuroscience institute. I think we're still missing top down understanding. Again, it's easy to make a bunch of measurements and describe them as a stamp collection perspective on science. But to achieve understanding, and also to achieve AI for that matter, I think it's going to be necessary to impose more kind of top down theoretical ideas and not just expect that if we catalog every synapse and the worm that we're going to understand how the worm works, I think that's just a lost cause. And something else has to discipline that and I think it's going to be top down ideas from computer science. Brain inspired is a production of me and you. I don't do advertisements. You can support the show through Patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science. Go to braininspired.co and find the red Patreon button there. To get in touch with me, email Paul at braininspired.co. The music you hear is by the new year. Find them at thenewyear.net. Thank you for your support. See you next time.
 We are not the first one asking the question about what intelligence is. People think there's a first one to ask the question or to build something, what am something. And it's not the first time to put it mildly. It's always a trade-off, capacity versus complexity. And you want to find the minimum cost and minimum capacity agent that's capable to conquer the complexity of whatever future tasks that agent will be exposed to. But if the agent hits a wall, the agent will have to have the ability to expand itself and continue learning. What I've learned from two years trying to do the new AI project and idea, that it was, first of all, much less well-defined than AI for newer. Here you like search for a black cat in the black room and you're not sure if the cat is there. That's it. You were a 4 AI. This is Brain Inspired. Hey everyone, it's Paul. Happy Holidays. I hope you're well. Today I speak with Irina Rish, who is currently at the University of Montreal and also a faculty member at Mila Quebec AI Institute. I wanted to have Irina on for multiple reasons, one of which is her interesting history having been on both sides of the AI and neuroscience coin. She's also worked at IBM as you'll hear, working on healthcare, and also neuroscience inspired AI. We have a pretty wide-ranging conversation about much of her previous and current work. We talk about her work on alternatives to the back propagation algorithm. We talk about her ongoing work on continual learning, which is a big topic in AI these days. As you probably know, deep learning models suffer from what's called catastrophic forgetting, where when you train the model to do one thing really well and then you train it to do another thing, it forgets the first thing. Humans don't suffer from this problem and it's an important problem to tackle moving forward in deep learning. We discuss many of the methods being used to try to solve continual learning and some of the inspirations from neuroscience along those lines. We also talk a little bit about scaling laws, which is roughly the relationship between how big and complex a model is and how well it performs over a range of tasks. We also talk about definitions and Irina's definition of artificial general intelligence and how she views the relationship between AGI and continual learning. We talk about a lot more. You can learn more about Irina and the show notes at braininspired.co. slash podcast slash 123. On the website, you can also learn about how to support the show on Patreon and possibly join the Discord community of Patreon supporters who are awesome. Thank you guys and thank you for listening and joy. You're actually kind of a perfect fit for this podcast because on the one hand, you have a background in a lot of computer science and I guess your early work was in applied mathematics. So you kind of come from that side, but I know that you're interested in using among other things, among the many things that you're interested in, in using some principles and ideas in neuroscience to help build better AI. Could you just talk a little bit about your background and how you came to be interested in being inspired by brain facts and etc. That's a very interesting question. Indeed, sometimes I ask myself and I try to dig into the past. The question is how far in the past we want to go. Indeed, a couple of years ago, I joined Miele and University of Montreal, but before that, I was at the IBM research and I was over quite a long time initially in the department of computational biology where I did indeed focus on neuroscience, neuroimaging and applying statistical methods, machine learning, AI, two analysis of brain data. So that's mainly where I kind of really got, I guess, deeper into neuroscience, psychology, psychiatry, type of topics. And I'm still collaborating with a group. It's my kind of long-term collaborators in computational psychiatry and neuroimaging Gisier Machichi and his friends. So that was really exciting. And that's, I guess, where I actively was pursuing this idea of the intersection between AI and Nura. But I think the interest in that intersection started a long time before I even joined IBM. And I usually, I realized that I could track it back to my, I think, elementary, even middle school years. I used to go to mass Olympia in Russia. I don't know if it's just, it's a, it's too long of an answer. Well, let me ask you this. So, so when I was in college, there wasn't even a neuroscience program degree. And I don't know that I would have been, you know, I started in aerospace engineering and then moved on to molecular biology. And I don't know if neuroscience was available as a program, whether I would have actually chosen it. But so I was going to ask if that's what was limiting, if you had that kind of kernel of interest, why then go into applied mathematics? That was your first degree, right? Right. Yeah, I mean, I probably should explain indeed what I wanted to mention. The reason was from practical perspective, I was going to mass Olympia. And I was quickly realizing that you don't have that much control over your brain. And like you want to solve a problem and you kind of heating the wall. It's like what's going on? Then sometimes it works. Sometimes it doesn't. You want to understand like why and how to make it work better. And then you see people around you, some of them struggling and able to solve anything. Some of them are able to solve like more than you do. And again, you wonder what the difference? What does it take? Then you start reading books like Poia, how to solve it, how to learn how to solve problems and this and that. But it really came from very practical goal. I need to figure out how to solve all those problems quickly because I want to win the Olympia. So what do I do? My brain doesn't seem to be cooperating. So what should I do? Like how do I make it work? So you start digging into how to make brain work. And then you run into books. Accidentally, which say, well, whether machines could sing, it was a Russian translation, I guess, of the famous Turing's work. That gets me into thinking about AI when I'm like 14 or something. That's why I go to computer science. The closest to computer science at that point, kind of in Russia was like applied mass. Essentially applied mass slash computer science, but formally applied mass. And that's kind of that's how it goes from there. So like my focus on computer science and AI actually came from the really very practical goal. I can't understand how this brain works. So I make it work better. That's pretty much it. Then you realize that it's psychology, psychiatry, your science and many other things that study brain, like whatever goes. Okay. So so at IBM, then you were part of a team. I think you said you're in computational biology and so you're a part of a team that was sympathetic to using principles from biology to help make machines better, right? Okay. The focus of the department was not on machines. The focus was on healthcare. So the focus was on how to make humans healthier. That was conditional biology in your science and your imaging kind of groups focus. So it's not the focus of that group was not really on AI. And I was kind of back and forth between focus on AI and computer science and machine learning to biology and back. Originally I started in machine learning group for distributed systems, changed names a few times. Then I moved to this computational biology center. And then in the past few years before moving to Milla, I moved again from computational biology department to AI department of IBM. So as I said, I was kind of iterating between the two multiple times. The focus in those past couple of years before moving to Milla was indeed to bring kind of neuroscience inspirations and ideas to improve AI. So my latest focus on IBM was indeed on what you're asking about. And that was new AI kind of project between IBM and MIT. That was going on. That kind of remained part of my focus when I joined Milla and was part of the direction for that seven year program that I'm leading the Canada Excellence Research Chair. The new AI component is one of the kind of access along which things are supposed to be developing. We'll see. Okay. So this is what I wanted to get to because I'm curious about your experience about your colleagues and their sort of opinions about using neuroscience or neuroinspired. Tools to build better AI because that's very much the industry side of it. And you have a lot of like passionate opinions about whether we should be looking to the brain to build better AI or whether you know, we should just continue to scale with what we have, right? Right. I know both sides really well. We just say on our scaling, those workshop last weekend from London. Yes. Okay. So so what did your IBM colleagues if you feel free enough to talk about it? Sure. What was kind of like their reception of your focus on using neuroscience? Again, the colleagues in AI department were colleagues in the bio department. Well, I think actually this whole new era AI, new era for AI and AI for new era, all this idea is actually grew out my multiple interactions and discussions with my friends at computational psychiatry in neuroscience department and primarily Gijerma Chechi. And I think what really also helped to shape my views was the interactions between world not just discussing technical aspects of AI or technical aspects of brain imaging or even neuroscience, but there was lots of philosophy in those discussions because like a Gijerma had first degree in philosophy and the second in physics and then went to neuroscience. And I think that really made him like stay kind of apart and ahead of many colleagues. So I'm really like really, really grateful for those discussions because they helped me to shape my views as well. So what I'm trying to say that while the healthcare department, the combi department was focused on healthcare applications, the idea of using neuroscience and biology inspirations for improving AI was very exciting for at least several people there and it's still exciting and we would like to kind of do more along those lines. When I moved to AI department at IBM, again, it was kind of a mix of opinions because just like in the field and general and I agree with that view, it may not necessarily be the case that the only path to intelligent artificial intelligence systems is mimicking the brain. Moreover, like even my colleagues neuroscientists never said that we have to mimic the brain. The tricky question is like what is the minimum part of those inspirations that might be necessary to transfer? And that's just like Jan Lecunnel, which gives us example about airplanes not flattening their wings. So you don't have to copy the biology exactly and yet you want to come up with some common laws that govern the flight. Some aerodynamics of intelligence in our case and that's a tricky part and that's I think where everyone agrees. So nature found the solution. There are some properties of that solution that might be specific to evolution and nature and perhaps we can abstract from them but there are some parts, the question is which ones that are probably invariant or necessary for any intelligence. Finding those invariant properties is a good open question and I think that's subconsciously everybody doing AI is trying to do that. But I definitely am not in the camp of first you need to completely understand how brain works and only then you can create artificial intelligence. I don't think so. Just like with airplanes and engineers there, they don't have to be like biologists understanding birds perfectly right? You need to understand enough but the good question is what is enough? Well it's interesting because we're going to talk about a few of the neural inspirations that you have focused on and in some sense I don't mean this is not an insult at all but it's almost like we are sometimes cherry picking and just trying kind of one thing at a time we think this might be important. I think that might be important when what we could do which you say is not the right path and I agree as we could instead of cherry picking these facts that we're going to discuss in a little bit you really could go more all in and try to, there are people trying to quote unquote build a brain and they're still having to abstract out a lot of things but that push would be to build in more that we know about the brain that rather than less it seems. But I want to ask you before we move on about philosophy I happened to see a panel that you were on I don't remember the the source it may have been the main conference. Yeah yeah yeah yeah we had the interesting discussion about philosophy there. Well it didn't go that far but you got into a back and forth a little bit with Suria Ganguly and who finds philosophy useless and you made the push that in fact it is useful so I just wanted to know. Nothing is useless you can learn from anything but let's not go there. Okay okay so you don't want to make a case for philosophy. I can make a case for philosophy. Let's hear it. I think what happened there maybe it was as usual by the way it's not specific to that panel. People mainly disagree because of differences in their kind of definitions of interpretations of terminology and unfortunately that's a universal problem in the field like many concepts are not well defined and in general I mean that's the main reason people argue because when people actually nail down details of what they are doing for or against surprisingly many cases they all agree. So I think the problem was what people understood as philosophy when I say the word philosophy and it means something to me it probably meant different things to different people so they were really arguing not with my point but they were arguing with their own understanding of the world. Yeah that's why because I don't think Suria or anyone else will disagree in general that if you have different disciplines whether it's philosophy or neuroscience or psychology or psychiatry or any any any type of discipline that started mind and its function how it works and what are the mechanisms at different levels in different ways and philosophy is one of them and even more not just philosophy I mean you can think about Buddhism and I brought this example to me it's empirical science of how mind works which has several thousand of years of knowledge accumulated in very different terminology and so on but that's that's a data that's that's knowledge accumulated by people from observations right so there is some truth to it and the question is like how to dig that truth out since we're coming from different field use different terminology again how do you translate philosophy and Buddhism to machine learning slang in sense so people will understand it not everything there might be relevant but we are not the first one asking the question about what intelligence is what usually it amazes me again I don't mean it as a salt but and plus it's very natural it's always happens but people think there's a first one to ask the question or to build something what then something and it's not the first time to put it mildly there many bright minds that for many years were facing similar type of questions just in different circumstances so I think it might be useful to learn more about what they found it does seem to be a recurring theme that there'll be a hot new trend and then it turns out a hundred years ago someone already had written basically the answer you know and laid out the groundwork for it that then then we go back and something that we resolved had already essentially been solved yeah it is I mean it's nothing specific to our field or our time right it's always been like that it's probably always going to be like that but that's just why I mentioned philosophy and also I mean I know I know I essentially meant the same thing that Suria was saying himself that we are trying to kind of discover the common laws behind intelligence whether biological or artificial and kind of pushing it forward common laws behind how mind works or how it could work and how you can kind of affect it in different ways so it works differently and I think any any source of knowledge about like people asking similar type of questions and finding whatever answers any information like that you can learn from all these data all I actually was suggesting that yeah let's try to learn from all data what and what data being different disciplines but okay so there's a problem here right where throughout the years all the different disciplines have continued to progress and it is essentially impossible to be an expert in all disciplines so how you know what's the right that's why we need a GI it will be an expert in all of them and they can tell us which disciplines we need to learn but we will need to learn the knowledge for us and convey to us in understandable manner I'm just quoting Ted Chiang short sci-fi story from major but it's only half joking yeah yeah well so you are interested in that that's a goal to build a GI and we're going to talk about lifelong learning in a little bit I want to ask you about back propagation first but would you say that's one of your goals yes and no a GI is not the final goal in itself it's an instrumental goal the final goal as I was always putting AI as augmented rather than artificial intelligence to me just the goal of building a GI never felt truly motivating like why do I care about machines well do you know what a GI even is I don't really know what a GI is because that's another thing where people have different definitions and two points yes it's one of those terms in machine learning which is not well defined and I know that's that's creates lots of confusion and there isn't a lot of debates in the Miller on the topic of a GI there are different definitions and different people again mean different things one practical definition could be just stick to the words it says artificial general intelligence so general means capable of solving multiple really multiple problems to me that means general broad versatile which relates to continual learning or meta learning or transfer learning but kind of push to extremes so like truly versatile AI that can do well at least pretty much anything we can do not a narrow opposite of narrow broad general so that can be just a relatively clear definition at least to me of what a GI could stand for there are many other definitions and we probably could write like a list of different ones but I think yeah it's absolutely right it's not the term it's not a mathematical term yeah do definitions matter definitions matter I mean yes and no again so you can have different definitions what matters is for people before they start kind of working together on something or discussing something to agree on definitions because again the main reason for debates sometimes unending debates is at the core that people did not agree on definitions and what comes to my mind whenever I listen to machine learning people debating something or pretty much anyone debating anything the picture of that elephant and seven blind man touching different parts of the elephant and saying no elephant is this no you're wrong elephant is that no it's you wrong and they're all right and nobody's wrong but they didn't agree on definitions and they don't see the full picture that's all I've come to think that the purpose of debates is to talk past one another and not progress at all well that's not the purpose to me it's a said reality yeah you can do that you will probably have some fun um maybe for some limited amount of time and then pretty much you just wasted the time and everybody moved on so what was the point I don't know I mean we're agreement and for example purposes yes if you try to learn something or converge to something or make some progress then probably not okay so you want to agree that's good we don't need to debate that issue then okay so um you've done work one of the ways that you have thought to use neuroscience is on the question of back propagation and maybe before because you had you've done work on what's called auxiliary variables like a back propagation alternative so I'd like you to describe that and you know where that came from but before doing that could I because we've talked about back propagation multiple times now on the podcast I had Blake Richards on way back when you know um uses the the morphology of neurons and the electrically decoupled uncoupled um apical dendrites and blah blah blah burst firing etc as an alternative and now you know there's this huge family now of alternatives to back propagation yes I'm curious about your overall view on uh that progress that literature yes uh yeah that's very good question and actually in fact we are working right now with a group of people at Miele and outside of Miele on extending different target propagation so basically that line of work is still going on although it was a bit in a back burner for a while okay and there are as usual at least two motivations here whether you come from neuroscience and you try to come up with a good model of how uh essentially learning happens in the brain um basically how the um credit assignment for mistakes happens in the brain and whether back propagation is a good model for that or you can come up with a better model so this is one motivation and many people who kind of are less concerned with um competitive performance or alternatives to back propagation and more concerned with really understanding how it works in the brain they focus on that and I also totally agree with that view as I said I mean there is no contradiction once you clearly state what your objective is you cannot say that they are wrong or you are right or vice versa because they just optimizing different objective function they want to answer the question how would best model what happens in the brain their objective is not to beat you on M니stance afar as long as we all agree on what objective is it's not wrong it's interesting line of research and that's kind of initially what motivated also work on beyond back probe kind of just trying to understand things better and plague is definitely uh doing lots of things in this direction and other people but on the other hand there is uh another objective like if you come from the point of view of AI person who says okay I understand I want my analogies with brain if and only if it helps me build more effective more efficient algorithm so when you come from that objective you can start wondering purely computationally what limitations of back propagation and uh what could you do differently or better and how to solve those kind of shortcomings and usually people were always claiming that yeah there is problem of vanishing gradients exploding gradients yeah there is a problem that basically back propagation is inherently sequential because you have to compute the chain of gradients and you have to do it sequentially but again one hand in brain processing is purely parallel second in computers if we were able to do it in parallel it probably would have been more efficient and better would scale better as well so you want this parallelism you want to avoid possible gradient uh issues uh so what do you do and that's where many optimization techniques came starting from this uh essentially Yandlick Khan's own thesis mentioned alternative to back propagation that later was called target propagation and all it meant is another view of the problem so basically instead of just optimizing the objective of the neural net with respect to the weights of the neural net being unknown variables you're trying to find you introduce auxiliary variables that are different names uh it all comes from the just trivial kind of uh equality constraint there that those activations or hidden units uh they are equal to what to that linear function of previous layer transformed by some non-linearity and you can play with that you can introduce extra auxiliary variables just the linear ones and the another one uh another set non-linear transformation of those own supports you can write it purely as a constrained optimization problem you can modify constrained optimization and to just like this Lagrange and whatever so Yandl mentioned that in the thesis and kind of was looking into that later so it's not something that was not kind of considered before just people didn't really try to push directly optimization algorithms that would um take into account those auxiliary variables explicitly and to me the work from 2014 uh was a ASTAD paper that forgetting the name right now i mean that basically motivated us to start looking into auxiliary variable approaches and then there was a whole wave of this optimization approaches anyway so they all tried to do the same thing they tried to introduce activations or linear preactivations as explicit variables to optimize for that would reformulate your objective function for neural net learning in terms of two sets of variables one being your usual base and the second being those activations and that had some pluses and minuses as everything the pluses would be that once you fix activations it completely decouples the problem into local layer wise subproblems so you can solve for those weights completely in parallel um basically the chain of gradients is broken and it's good so you don't have by definition any vanishing gradients or exploding gradients because there is no chain and second thing you can do things in parallel so those two things are good there is also some similarity and more kind of biological visibility because you take into account now activations in this neural net explicitly as variables and essentially interpretation of that is also you view them as a noisy activations which they are unlike classical neural nets where they always deterministic variables deterministic functions and real neurons they're noisy. Real neurons are not fully deterministic functions right how about their relus? yeah so the non-linearity is a separate thing but even just the fact that in artificial neural net they're totally deterministic that's also quite in kind of simplification so there are other kind of I mean there are other flavors of this auxiliary variables and kind of target propagation methods in our kind of approach which is essentially in line of the this auxiliary variable optimizations where you can write the joint objective in terms of activations and weights they think here is we still use the same weights for kind of forward propagation or basically computing output given input as well as for the optimization or in a sense like backward pass there are other flavors like target propagation, a yarn, a lecon and different target propagation by your show and your students and all flavors of methods on top of that they use two sets of weights the forward weights and backward weights which may be even more biological plausible than those auxiliary methods that I mentioned and then there is lots of kind of flavors and variations on that and actually it's nice to see this subfield expanding recently and there were some papers at New Reaps last year and so on so forth so it's all interesting it has its plasas and those things such as parallelization and by definition lack of vanishing gradients and exploding gradients no matter how deep the network is or how long is a sequence in recurrent network LSTM or something those are good but you move into different optimization space and empirically whenever you try these methods on standard benchmarks and standard architectures they are not always performing as well as a classical backward propagation and that was one of the issues with the whole field of alternatives to backward how to make them competitive there are multiple successes but they are not like completely kind of putting back prop out of the picture plus we didn't aim to do so we had some successes on fully connected architectures we had successes on SIFAR and MNIST we had some successes even on RNNs and even on simple coordinates but what we learned I mean that was good because it was first time when you actually do see those improvements in the paper at New Reaps by Sergei Bartunov and Jeff Hinton and others they also kind of were trying different alternatives like target prop and variances of that and unfortunately they couldn't show that it would be or be comparable with standard backprop once a image map so there were this kind of many unsuccessful attempts there were some limited successes and the question is still open whether such alternatives can become true state of art and the hatch is I think you shouldn't beat your head against the wall trying to use alternative optimizations like that on architectures like condonats and so on which were so well optimized to work with standard backprop you need different architectures and maybe the fact that we really tried hard to beat backprop on classical resonance and it didn't really work maybe that's a problem resonance go well with backprop something else will go well with auxiliary variables and target prop it's a hypothesis but I think it's kind of something to try anyway I think I think if you make those methods work you will get benefits of much better parallelization and scalability you will not have this nagging issues of potentially exploding Covenishing Gradients but you will have other problems you will possibly have convergence problems and alternating minimization type of algorithms and so on and so forth I mean there is no free lunch hmm since you mentioned architectures I want to pause and ask you if you think looking to the brain for architectural inspiration makes any sense at all or you know because it's a whole system with lots of different architectures interacting and if you're thinking like different optimization methods might work better or worse with different architectures if that is another avenue where where we should look I think it would be useful to explore it again there is this cheated debate within the field inductive biases inductive versus scaling the most generic architectures say transformers or even scaling multi layer perceptron ultimately it's a universal function approximator if you scale it enough it probably will do anything right it's probably just not going to scale very efficiently to put it mildly so yeah that's why maybe transformers are better but the question okay here is the thing inductive biases or priors versus scaling very generic type of networks I might be wrong my personal opinion but I think just like historically whenever you have not enough data say brain imaging sometimes it's small data sets or in medical application zones of course when you don't have enough data then using priors or inductive biases from the domain is extremely helpful they take role of regularization constraints and if there was regularization constraints or priors so inductive biases are right they help tremendously and you can perform really well despite having small amounts of data and that's where you could kind of use specific architectures and so on and by the way that's why say convolutional networks were so successful for such a long time right but then you start scaling right and the amounts of data if you have those amounts of data they go way beyond what you had before your model size goes way beyond what you had before you scale the number of parameters while maintaining like some kind of structure of the network like your scale weights and depth and something there are many kind of important caveats here about how to do it so so scaling model will actually capture the amount of information while you scale data so I mean the smarter ways to do that and less smart ways but say you do it right now it looks like those priors inductive biases become less and less important and we do have empirical evidence say visual transformers at scale in terms of data start outperforming convenettes and by the way that's why I think looking at scaling laws is so important you have two competing architectures you see how this scale and you see that in lower data regime convenettes are so much better in higher data regime it's vice versa and that approach that kind of empirical evaluation of different methods architectures or whatever you compare by looking at the whole curve rather than point that one architecture another architecture one dataset and another dataset it's not that informative all those kind of tables their plots scaling laws plots are giving you much fuller picture and give you better idea say if you can scale what type of method you should invest into and apparently if you can scale visual transformers would be better than convenettes but still question remains what if there are inductive biases such as maybe those brain architectures and so on that can improve your scaling exponent essentially what it means when we talk about scaling exponent is that empirically it's fun that the performance of models expressed as basically the cross-entropy laws on the test data and or laws or classification accuracy on downstream tasks they usually seem to improve according to power law you've seen probably those papers by Jared Kaplan and his colleagues from open AI and now I guess entropic and so on and so forth and all those papers show you power laws which are straight lines in log log plot and the exponent of power law corresponds to the slope of the line in log log plot and the whole billion dollar question in the scaling laws field is what kind of things improve that slope so you get better improvement in performance for smaller amount of scaling therefore cheap and scaling by the way involves here not just scaling the data and scaling border size but also scaling amount of compute because you may just even keep the model fixed and data fixed but let your algorithm compute more and sometimes you see very interesting behavior like groking paper from workshop but they clear last spring where they just trend their method for a long time just for good to kind of kill it and then there was a sudden face transition from almost zero to almost one accuracy they just managed to find some point in the search list of course it yeah well it was not intentional apparently but it happened to be that for that type of benchmark and architecture they used it was the case that somewhere in the search space there was a place with extremely good solution surrounded by not so good solutions and if you find that place you can jump there and that's your face transition from x zero to one anyway we kind of trying to explore those face transitions recently with my colleagues at Miele as well so back to your question the question inductive biases versus scaling as usual I would say maybe inductive biases plus scaling because certain inductive biases maybe can improve the exponent and at least you want to explore that they might be useful I wouldn't kind of throw the water I wouldn't throw the baby together with the water as I say so saying okay let's just scale multi-layer perceptron yes of course you can do that it will be just extremely inefficient don't you want more advantages scaling laws and maybe inductive biases can help you reset it doesn't have to begin the two camps fighting each other although I understand it's more fun to fight than collaborate human nature it's more fun to do both right just like a scaling and inductive biases the answer is both yeah it doesn't have to be either or it would be both the question is what inductive biases help to improve scaling and that's a good question it might be it depends like Jared Kaplan was presenting at our workshop I should a couple of times we had two workshops so far one in October one just now last week and again he mentioned that in his experience again for that stating that problem for GPT-3 improvements due to architecture did not seem to be as significant as just kind of scaling and that's totally fine it doesn't completely kind of excludes the situation when inductive biases may be some for some other problems yeah it would be much more important how do you explore the full space of architectures though you have some some limited amount of exploration right right um that's that's a good question I mean it just like with everything neuroscience inspired it's such a huge yeah space on its own and to be completely honest you cannot just go and ask your scientist what in your opinion is the most important right inductive bias that AI people should use it doesn't work this way at least not in my experience because they say I we don't know don't know like you tell us what you need and then maybe we can think and suggest what kind of inductive bias can best help you with what you need so what I've learned from two years trying to do the new AI project the idea that it was first of all much less well-defined than AI for newer where you take methods to analyze the data that's we know how to do here you like search for a black cat in the black room and you're not sure if the cat is there that's you're a for AI but I think what helped interaction with those neuroscientists let's say look I need okay you're asking what I need for a I'd like my system to be able to continue to learn I mean well it has to be learning how to do new tasks and work on new data sets and it should keep doing this because I might have my robot walking in the wild and it has to adapt or I might have my I don't know personal assistant chatbot and it has to adapt to my changing mental states or it has to adapt to other people so it will have to keep looking into different data environments tasks and doing them well at the same time I don't want it to completely forget what I've learned before because it may have to return to it and may not have to remember I don't really push for absolute lack of so cold catastrophic for gaiting fast remembering is fine so just like basically few short learning of new things and few short remembering of all things would be just fine after all I don't remember myself the courses I took in the whatever years of my undergrad but I could remember them hopefully so you wonder how does brain do it and that may be more specific questions I say okay how does continual learning happen what are the tricks and then people and they are actually using those inspirations I mean this whole bunch of continual learning methods were inspired by some kind of some phenomena in your science for example kind of freezing was slowing down the change of certain weights in the network if those weights are particularly important for some previous tasks in it was kind of more formalized in work like EWC from deep like more or the elastic weight consolidation or synaptic intelligence also there are many of those regularization based approaches essentially in continual learning and it's kind of one flavor of well very abstracted inspirations I would say but from this phenomenon that does happen in the brain or replay methods again so this classical example of having hippocompose that essentially forms very quickly kind of memories but then they consolidate it say during sleep and you kind of hapsis longer term knowledge and preferential cortex so having this complementary learning systems approach yeah yeah so that is another example again it's very much simplified and abstracted it has it through some neuroscience but then it kind of gives rise to machine learning algorithms like rehearsal, Sudary Horsel, Replay, Generative Replay, Zone and so forth but the idea of yeah there is also the third kind of direction that people take usually in continual learning more like architectural based approaches when you essentially expand your network model, expand your architecture depending on the needs of your learner and that also has its roots in you can connect it to things like adult neurogenesis that even adult brains apparently do grow in your neurons when needed and they do so in specific places like hippocompose well the dejairos of hippocompose it's still happening there it doesn't stop at the like beginning of adulthood and yes there was dogma 30 years ago or more that adult brains do not grow in your neurons well apparently they do in mammals in rats they do it in the hippocompose and alfactory bulb you can imagine alfactory bulb is very important for rats and other mammals because they do use smell quite a bit and humans apparently alfactory bulb doesn't matter as much anymore as in rats but they still have some neurogenesis happening in hippocompose and what was interesting and we kind of did some work on top of that had a paper about neurogenetic kind of model although very simple version of that and the idea was all the empirical evidence about neurogenesis in the literature suggests that there is more neurogenesis happening when the animal or a human is exposed to radically changing environment like different tasks or different environments in continual learning so then it is associated with more neurogenesis if the environment is not very complex and it's not changing you kind of don't really seem to need to expand capacity of your model like you have some new neurons being born but they die apparently in higher rates so it's like use it to lose it if you don't need extra capacity you won't have extra capacity if you keep challenging yourself and you keep kind of pushing yourself to extremes in total in new situations the new neurons will be incorporated and your hippocompose will expand somewhat I mean to some degree of course as everything so it's an interesting observation and it associated with possible ideas of expanding architectures in continual learning to accommodate for new information that cannot be possibly represented well using existing model so let me just recap what you've said there because you you covered a lot of ground so you kind of just transitioned I was going to ask you about continual lifelong learning and you just transitioned into it so and you talked about the three neuroscience principles that have been implemented and the whole point of lifelong learning that there's this huge problem in deep learning called catastrophic forgetting where once the network is trained on one task if you train it to learn a new task it forget completely forgets the old task right and so there's been this explosion in lifelong learning methods one of which is continual learning is that under the umbrella of lifelong learning because there's transfer learning meta-learning continual learning and now there's meta continual continual meta and on and on right okay yes okay so it's again question of terminology like we're kind of stepping on the same rake of machine learning terminology again I gave it to do it a day, same male last summer one way actually to me and I guess too many people lifelong learning and continual learning as synonyms okay and they all just mean that you would like your model to learn in online scenario where online means you get your samples as a stream instead of I mean you can get them as sequence of datasets or a sequence of batches or a sequence of just samples but the point is you have that sequence of data you keep learning and you do not have an option of keeping the whole history of datasets or you might even have that option but you may not want to constantly retrain because it's not so efficient so continual and lifelong learning and this approach are synonyms meta continual continual meta and so on so forth are still within the umbrella of continual learning but kind of different formulations of how you might go about training your system to do that continual learning again as I said I'm sure people have different definitions so it my mind it's a particular definition of continual learning which just means online non-stationary learning by non-stationary I mean any change to the environment or input data whether it's a change of data distribution it's a change of task or both so as to transfer okay so transfer learning again I have the slides in the tutorial as well as in the class slides for the continual learning class they all online on my web page I gave it for two years in a row it was winter semester class 2020 and 2021 but I'm not giving it this year because I will be teaching neural scaling laws yes there you go that's your new thing okay well continual learning is related to that it's not like we completely jump to something unrelated it is related but with more focus on their scaling models data and compute and continual learning being a problem that you also trying to solve that's why but back to your question about transfer meta and the made adaptation so on first of all I have it in my slides second well the slides are based on their very very nice tutorial by I think the launch from 2020 anyways so the picture defines each of those problems and shows how they are different and how they similar transfer usually assumes that you have two problems and by learning on one you're trying to kind of we're trying to be able to use that knowledge to do better on the second problem as it is not necessarily any notion of remembering or being equally interested in doing both problems it's like more uni-directional again question of terminology to me transfer is a property of your model and algorithm and continual learning is a setting in which you would like transfer to happen which means while learning I always would like to improve or at least not make it worse my performance on the past which means backward positive transfer or at least backward non-negative transfer at the same time I'd like to hopefully learn better and faster in the future because I only learned so much so ideally I would like to have some positive transfer to future and that view of not equating continual learning with catastrophic forgetting issue but rather more general view of continual learning as a problem of maximizing transfer in both past and the future that kind of also came out of our joint paper on Met experience replay from 2019 with Matt Rimmer and I very much kind of support that view more general view of continual learning and especially when it comes in the context of not just supervised or unsupervised but continual reinforcement learning as an ultimate continual learning setting yeah so that's kind of where these different aspects may align and come together to build kind of the same same I don't know what one big picture of what we're trying to achieve agent that can adapt and still be able to return back to what's learned maybe by retraining slightly with a few shorts like the flexible avoiding or forgetting and adaptation that is fast and the aspects such as transfer playing the key role the meta learning is essentially very similar to continual learning but it does not assume that your different datasets came in a sequence they available to you at the same time and you try to learn from them human model that will be very easily adaptable to any new environment or dataset presumably from the same distribution of datasets so that's essentially meta learning and vice-vay out of distribution generalization is another related subfield which is to me essentially zero short meta learning because out of distribution setting is give me multiple datasets or environments and I will try to learn a model that basically distills some common invariant robust features or in general common invariant robust predictor so that next time you give me dataset for testing that is different from my training it's out of distribution and yet shares that invariant relationships which are essentially closely related to causal relationships if you give me that I will do well on that so it's extreme case of meta learning because meta learning will I want to do well on that out of distribution dataset just give me a few samples to adapt so they all this terminology in a sense comes together and has many shared aspects it's just as I said unfortunate Babylon tower situation and machine learning that makes it difficult the set of ideas is much less dimensional than the ambient dimension in terms of our terminology so let me do machine learning compression it's long overdue well it's also the yeah it's also the the variety and problem statements but I want to ask you about that in a second because so so just backtracking once more so you talked about three inspirations from neuroscience to help with continual learning you one was the variability in plasticity say you learn you want to remember a certain task there's evidence that those those synapses form stronger connections and are less likely to change right and so you lower the plasticity moving forward so that you can maintain good skills on a certain task right you also talked about the complimentary learning systems approach using you know these two different learning and memory mechanisms one is fast and very specific and then that's associated with the hippocampus and then there's this general slow and generalizable learning happening and that's associated with the the neocortex and of course you use replay which has been used and you know I think originally it would dequeen to solve the Atari games and that's what the region is so I used much much before well of course yeah that's okay no thank you for correct in your science I know I what right when I said originally I was like oh that's not good I'm just kidding I'm just kidding but okay and then the third was essentially inspired from neurogenesis which is the idea that especially in the hippocampus of an adult you can continue to form new neurons and that this might help us informing the memories so what I wanted to ask you about is the issue of the facts in neuroscience continuing to change because it's an empirical science right so I thought that recently there was backlash against this idea of neurogenesis in adults it's you know it's like one year milk is good for you the next year it's really terrible for you based on new evidence so there are these I didn't know that the evidence was still pointing toward neurogenesis as a concrete fact right because we're always finding out new things so the story is always being revised in neuroscience and I'm I'm wondering if that affects your own approach because if we hang our hat on neurogenesis and it turns out we actually don't create new neurons or something um I don't you know then how do we move forward from there well the good thing about computational I don't know machine learning kind of side of that even if it happens in a nature there was no new new neurogenesis to start with we don't care we have algorithms that works better yeah you can always go back to that now you know you have your AI hat back on so exactly that's why I never have only one hat I have multiple and I actually don't even think it's effective to have single hat ever although I understand it's kind of people tend to have one hat because we are so I don't know we saw kind of tied to the notion of our identities including identity in terms of scientific views right so common it's so dear to our heart but in reality the notion of identity in general is much more vague than we might think but I don't want to go into philosophy here I'll take long time and we don't have the time it's a separate conversation right anyway okay so um so then coming back to all the all of these different problem statements and how you were saying you know are different definitions but really they're all kind of converging um onto like a uh a lower dimensional problem space given the various approaches to training and testing and metal learning you know all the things that you just covered do we actually understand how humans learn well enough the developmental literature the like psychology like do humans do all of these different problem statements or would it uh is there a so I know that there's a little bit of work at least on you know the humans actually showing a little bit of catastrophic forgetting in certain circumstances but do we actually understand human behavior enough and how humans learn not at the neural level but at the behavioral level um to map that onto these different continual learning metal learning transfer learning do humans do all of that do humans do some of it you know does that make sense yeah it definitely makes sense well first of all I mean there are many people who kind of focusing on this type of studies and developmental neuroscience so I really would love to read more about that as you just mentioned earlier no human being unfortunately can be on top of all the literature and all the fields related to mind so there is lots of interesting recent work as well and I have some colleagues at UDM working on that and for example Guillaume Dumas in psychiatry department and E. Leifmuller in neuroscience department there is a lot there but in terms of okay not looking at new read just looking at the behavior and asking the question whether human do transfer learning or continual learning particular settings I think yes because in a sense the notions of particular settings in continual learning they came out of researchers thinking about like what do we do in real life in different situations say we have robotics uh what are kind of scenarios there the robot moves from one room to another room environment changes all these settings and specifically of them oh now it's not just room didn't change but I gave Robert different tasks they all came out of our anecdotal observations of our own behavior of like just common sense reasoning in our heads about what people do so in a sense yeah I mean whatever you have right now in continual learning fields for example or transfer it came out of our knowledge about human behavior because in a sense where else would it come from well yeah yeah so in this sense yeah but more on that like and more specifically like studies about how it's being done and like what effects that what kind of makes it better or worse for that we would need to talk to our colleagues in psychology and your science and read more about that and I'm pretty sure as I said I mean I'm not claiming I'm completely on top of any literature that yeah we're infinite hats well that would be that would be the ultimate goal but for that first I need to create a GI then you connect with a GI and then you have your augmented brain which can finally stay on top of all that literature that's my true motivation all right all right okay so I have one more question about about continual learning and then we'll begin to wrap up here someone in the brain inspired discord community asked whether the learning trajectories of artificial networks impact have an impact in continual learning right how much it retains and how much it forgets over successive tasks and training have you do you have you studied the learning trajectories at all because that's something that's being looked into in deep learning theory these days as a something that matters yeah the learning trajectory okay the questions also it depends on several things it depends on say the sequence of tasks like curriculum learning which which can matter a lot indeed actually it matters not just for continual learning it matters for say adversarial robustness even it matters for various aspects of the end model like how what sequence of data was given to it and how it arrived there but of course it also depends on say particular optimization algorithm that basically different trajectory leads you to different state in the weight space you leads you to different model or think about a different yeah different brain artificial brain and of course they will different terms of their properties and terms of forgetting and so on so I guess when we were looking into again I'm thinking about this MAPE per Ms. Mad and others yeah obviously the trajectory matters the question is how do you how do you know what a local kind of constraints or local kind of conditions you should use to push trajectory into the desired kind of direction because like all we can do is just to use kind of some local information just like as gradient is right and I guess well things that change trajectories as I said data can change trajectory all kind of regularizations can change trajectory basically regularizations are precisely things among other things that change trajectories a lot you have objective standard objective of your senioral net and you're trying to optimize for that and then you add things one example say you say I really would like to make sure that I have that positive transfer or at least non-negative let me add as a constraint the product of gradients on new and old samples and I want the things to be aligned I do not want my new gradient to point in the direction opposite of the gradient on previous samples because that would mean I will be decreasing performance on the past task I will be forgetting so I'll try to add at least locally regularizer again like in mere paper with smart for example just one example that will push my weight kind of trajectory in that direction and so on support so basically any regularizer you put there with some desirable features but without of course any guarantee because it's all local it will change the trajectory so in a sense the whole field of continual learning playing with at least the regularization based field it's all about changing trajectories and then changing the final solution so there's a good theory about how to do that really yet right again I don't want to claim I mean there are there is various work there was a paper on them like try to theoretically understand the continual learning algorithms but for specific type of them there is what was the name are too good all gradient descent and that work they okay there is various work but to me continual learning is still the field that is lagging behind quite a lot in terms of theoretical understanding I was going to ask what your outlook is for continual learning if we solve solve continual slash lifelong learning is that the same as solving AGI and you know are you optimistic about well you know what's what what is the normal number that people say 20 years and that's when we'll have solved everything it's always 20 years or so right oh some say less than that what's your number 10 or less 10 years for lifelong learning yeah okay and AGI that equates to AGI or okay when we solve lifelong is that solving AGI okay so again difference in terminology I apologize to people who might like really strongly disagree with me and I know some people who will but I'm not gonna say let's drop some names I know no no no no no we're not gonna do that but those people know well there is also this kind of you which is still an open question the view about salt what does it take to solve continual learning whether it equates to AGI and so forth so if we assume that AGI for all practical purposes is general artificial intelligence by general it's versatile broad it can kind of learn to solve pretty much any task that is as people often put at economically valuable so say AGI is kind of a model that can solve all economically valuable tasks say as good as or better than human something rough like that the question is if you kind of put that agent in the wild it will have to do continual learning right so it needs to be kind of solved the question is whether you approach solving it by trying to train that agent in continual learning manner or as scaling crowd will tell us or at least part of the scaling crowd I'm not overgeneralizing that maybe it's enough just to really portray in a humongous well foundation models on multi-model data not just language not just video not just images like video on perhaps even all kind of time series data but once you portray in it it essentially solved continual learning I had this question during the workshop discussed and its ongoing debate what I would say is that for any fixed set of possible tasks that you will give continual learner like for example recent submission to a clear on scaling and continual learning for a fixed set of tasks yes sure scaling model scaling amount and diversity and complexity or information content of portraying data will at some point conquer the complexity of the fixed set of tasks and yes you will solve catastrophic forgetting you can capture information of all those things and you can do all of them well but if the stream of tasks and continual learning continues growing right infinitely will your portraying model hit the wall at some point or not and that's a good question and I think it's interplay between the model capacity I always keep saying that also in my tutorial on continual learning capacity of that portraying model that you learned and the complexity of unseen part of the universe that your agent will have to adopt and I would say that what you really need to look at would be a relative scaling of how your model capacity that depends on size, architecture and information content of the data you trained on which depends on their amount or number of samples but other things too how that capacity scales with respect to complexity of downstream tasks so to me relative scaling loss would be the most interesting thing to dive into and I think it makes sense it's always a trade-off capacity versus complexity just like great distortion theory and information theory and so on and you want to find the minimum cost and minimum capacity agent that's capable to kind of work well conquer the complexity of whatever future tasks that agent will be exposed to but if the agent hits a wall the agent will have to have the ability to expand itself and continue learning so to me continual learning is the ultimate test for anything that is called AGI. So that sounds like incorporating principles of evolution into but maybe yes okay so any portraying model may hit the wall and I believe it will have to keep evolving and if it won't be able to evolve itself too bad okay Eurena this has been fun I have one more question for you and that is considering your your own trajectory if you could go back and start over again would you change anything would you change the order in which you learned things or order of your jobs how would you start again? Oh that's a very interesting question I'm not sure I have an immediate answer to that but in one of those realities I might have been taking totally different trajectory from the one I'm on right now I probably would have been skiing somewhere in Colorado working as a ski instructor. Oh I do all of that except work as a ski instructor you should try what I do. Yeah I would be full time ski instructor. Okay I just just went two days ago yeah look Trimblant is pretty good too so it doesn't matter some good mountain and just I know. Why don't you come visit me and we'll go ski and we'll see if we can change your trajectory. You're and you welcome to visit Trimblant pretty nice. All right very good well I really appreciate the time so thanks for talking with me. Thank you so much for inviting me it was fun. Brain inspired is a production of me and you. I don't do advertisements you can support the show through Patreon for a trifling amount and get access to the full versions of all the episodes plus bonus episodes that focus more on the cultural side but still have science. Go to braininspired.co and find the red Patreon button there to get in touch with me email paul at braininspired.co the music you hear is by the new year find them at the new year.net thank you for your support see you next time
 The Given the continual surprising progress in AI powered by scaling up parameters and using more compute while using fairly generic Architecture CGGPT suite. Do you think the current trend of scaling compute can lead to human-level AGI? And if not, what is the critical missing component? Or do we need to do something fundamentally different? Hi, this is Jeff Hawkins. I'd like to congratulate Paul for the hundredth episode of the Brain Inspired Pumpkin. This is Brain Inspired. Yeah, so in fact, yeah, like you're the role model your podcast made me start a small podcast on my own that's called stimulating brains because I really wanted to listen to a podcast about deep brain simulation, which is my field. And there was none at least I know none and I thought that's a niche and Yeah, so so even now yeah on the website it says that it's heavily inspired by your your podcast and the intro is similar because I just You know, I tried to to learn from the best. Thank you so much for doing that Oh man, that is It's absurd and ridiculous, but thank you Andy. It's also very kind I'm glad you've been inspired to start your own podcast So everyone go check out Andy's stimulating brains podcast. It's at stimulating brains.org Where they discuss shocking topics Oh, it's sad really I just I'm sorry. I couldn't help it there All right, welcome to the third part of the hundredth brain-inspired episode Bonanza. I'm Paul as always Here is today's question that many of my previous guests answered Given the continual surprising progress in AI powered by scaling up parameters and using more compute While using fairly generic architectures like GPT3 Do you think the current trend of scaling compute can lead to human level AGI If not what's missing? All right, so there's the question for this episode My tourist response here is no The we cannot scale using what we have right now largely that's because I like a few other people Who respond in this episode reject the premise of the question AGI what is that human level AGI what is that I reject the premise that that's a valid question So I would have you probably not responded to this question But I appreciate the thoughtfulness with which many of my guests did here So I ran all the responses through a recurrent convolutional neural network to determine the order this time Not really I randomly sorted the order in a spreadsheet Old-school lazy style like usual in the show notes a list each person and link to their information So if you hear something you like you can visit the show notes brainenspired.co slash podcast slash 100 dash One two three 100 dash three So we have a few more of these collections to go and I want again to just think all of the previous guests who took the time And made the effort to share their thoughts for this occasion Please do consider supporting brain inspired on patreon if you're getting value out of the show Which I hope you are It's really quite cheap to do and it's the sole source of income for the podcast. I don't do advertising I hate advertising on podcasts. I understand it, but I hate it. So I don't do it. All right. Where we go This is Wolfgang Mars from the grads University of Technology in Austria If scaling up is not sufficient what's a critical missing component? Not towards human level AGI a lot of thought and research at the moment starts to work on Generalization but this much more drastic form of generalization We give test examples which doesn't come from the same distribution at the training example And so they usually know one refers to this as O O O D generalization and I think we are No very little About the way how the brain achieves this O O D generalization It's kind of an anecdotical Inside there, but we don't really know how the prior knowledge is really encoded in neural networks Of the brain and how this prior knowledge is used to support abstract reasoning then and this No problem Is related to the previous problem that I have sketched namely Taking to count that the brain has been shaped by evolution So therefore I don't expect that we find there really a clean theory how the brain Has managed to use prior knowledge to achieve O O D generalization then it may be a bunch of hacks which altogether work quite well then This is Paul Humphreys at the University of Virginia I want to address the question of where the scaling up contemporary computational devices Will enable us to reach artificial general intelligence But I want to address this question from a slightly different perspective In part by not looking at purely cognitive abilities But emotional intelligence And in particular the faculty of empathy which is one of the ways in which human beings Can bond with one another Imagine that you are in a situation where your mother has just died Or your favorite pet has just died And somebody or an artificial device is attempting to convey their sympathies to you There's a major difference between a person who is pretending to be empathetic And someone who is genuinely feeling the suffering that you're undergoing Now there's little doubt that artificial devices Will be and in fact in some areas already have been Developed in order to behave really mimic The ways in which human beings interact with one another in these kinds of circumstances But here the question is whether this would Lead to a devaluation of human expectations about how to behave in those kinds of circumstances We're all familiar with the ways in which People's modes of argumentation Over social media have changed in the last couple of decades And it is spilled over From the artificial domain into our interactions in real life So suppose that we became habituated To artificial nurses in a hospital Trying to console us after the death of our mother We would not have the kind of expectations that we now do For real nurses who in general are very highly skilled at conveying the sympathy with us In similar situations Perhaps because their own mother has died So we become Inured to a behavioral approach Which is very different from what our current expectations are And then we would transfer this perhaps To our expectations from other human beings as well as from robots Now this may or may not be a good thing But it will be different Because we currently draw a sharp distinction Between ordinary human beings and con artists Whether a con artist is swindling old people out of their life savings by pretending To be sympathetic with the elderly person's situation Or in an extreme case of serial killers If there's no distinction between pretending To be empathetic with other people And actually having the genuine internal emotions We will have moved into a different kind of moral territory Then we now inhabit And I'll leave it up to you to assess for yourself Whether you think that this is a good or a bad situation Chris Elias Smith I'm grouping these two together because I think they share an answer Recall that the questions are What is the most important disagreement in the field and what is the right direction Do you think scaling with more parameters in generic architectures is going to lead to human level intelligence One important disagreement in the field Is whether we need to worry about cognitive level processing and representation or not I think Gary Marcus and Jan Lecune had something of a twitter war over this one actually Essentially models like GPT-3 assume we don't If we have enough data we can learn everything we need to from that data Including what we might have thought of as conceptual organization Cognitive manipulation of concepts How to usefully model the world And all the relations in the world And all this kind of stuff so all that higher level cognitive stuff At a more abstract level you can actually think of this as a version of the question of whether we should allow ourselves to impose structure on the networks and representations in our models This is a version of the same question because including concept like representations exactly an example of imposing structure on the network to representations I'm actually a big believer in the importance of this approach for several reasons One it's resulted in huge successes in the past just look at convolutional neural networks This is the workhorse of DPI those networks adopt the structure we find in the biological visual system Another reason is that nature has had billions of years to determine the right network structures and representations that we start with when we're born So the amount of computation that that evolution represents is kind of unfathomable So we're going to have to jump the queue if you will in some way and I'm guessing that the insights we get from neuroscience and psychology And good old ingenuity are the kinds of things that are going to let us not pay that huge computation And finally I come back again to the spawn model There's really no way we could have started with a blank slate or a really deep generic network And just train that network in order to give us a model that does what spawn does Of course because spawn is a neural network we can actually backprop through the final version after we built it and we can optimize But to get it to that original functionality using just backprop and a lot of David just isn't a viable option for several reasons One is that models like GPT-3 cost about 12 million dollars to train their 175 million parameters If we assume that we can just scale by parameter count spawn would still cost 1.4 million dollars to train and frankly my lab doesn't have that kind of budget Second this approach assumes that there is a data set that covers the 12 different tasks that spawn can do Unlike the terabytes of data on natural language Getting that kind of data for intelligence tests and copy drying and so on just isn't possible And third we haven't seen much evidence that we can train a single model that does a lot of different tasks Tasks like vision motor control reinforcement learning intelligence tests and so on With this one big data set This kind of heterogeneity of task is really foreign to most neural network models So ultimately I think that my arguments speak to a need for integrating methods Not that one's better than the other I left deep learning my lab uses it all the time But we also use concept like representations and we impose the structure of the mammalian brain on lots of our models In some ways the approach I'm suggesting here is what I argued for in my book how to build a brain It's an integrated hybrid approach to building systems that can achieve biological cognition Andri-Saxx here Given the continual surprising progress in AI powered by scaling up parameters and using more compute While using fairly generic architectures do you think the current trend of scaling can lead to human level AGI? I think the answer depends on how precisely you read the question So that sometimes these questions can be can can lead to apparent disagreement because people interpret them slightly differently So if you take that question very literally i.e. whether Scaling compute can in principle lead to AGI? Then I think you'd have to answer yes because if you are allowed to train different sub parts of the network in different ways and you craft a very particular data set Then almost no matter what the end answer looks like you know what sort of principles we end up embodying in a system that has AGI You could probably train some network with some carefully arc Setup tasks such that it would do a reasonable job approximating that But I don't think that's what most people mean right so that the other Interpretation is whether a standard deep network with today's architectures and learning rules and data sets It's just a billion times bigger Is going to become conscious and clever and there I think the answer is no Today systems are taking really fantastic steps toward that goal But they're only steps and I think we have new concepts waiting to be found and Discover what's the missing critical component and how could we get these components fastest Well for starters if I knew I would be writing a paper right now So I don't but I think one important aspect is the richness of environmental feedback So gbt3 is trained on all of Wikipedia, but still it doesn't get the targeted instruction That humans get and I think that dialogue with an intelligent agent who wants you who's an in teacher student relationship where the teacher is Self invalid with incredible intelligence All of our social learning abilities These things give us I think a much better way of learning rather than the current approaches That rely on Wikipedia And also think part of the trouble is that we don't know what we don't know So in terms of practical ways like how do you get? How do you discover what you don't know most quickly? I think the answer is to strengthen our prospects for scientific discovery And when it comes to neuroscience We've developed a huge range of new methods that are exceptionally exciting Neural pixel probes according to thousands of neurons to calcium imaging and optogenetics And I my personal view is the epoch of methods development Has already occurred to the extent that now we can go after the Real scientific questions were interested in we can start to extract general principles We can design experiments where animals are doing relatively complicated tasks And the data coming back from those experiments I'm sure will surprise us and lead us towards what we don't know Masrita Chiramuta Yeah, the scaling up issue It's like a betting issue like where would you predict things will go in the future My bet is that you can't scale up to get human level AGI I think expert systems including the really powerful neural network architectures that you have today like GPT-3 are really really good and impressive at doing the things that they've been trained to do and I think a lot of their power and effectiveness comes about because they're honing in on just particular Functional behavioral similarities that hold between the artificial system and the brain But to get to human level AGI there's going to be have to be a whole lot more going on That is just not captured by a system which just relates to Or corresponds to a biological brain and just some narrow respect So that's my reason for thinking that just making bigger and bigger Technologies of the same sort that we have today will not lead to AGI So the critical missing components would be like all the biology All the biology all the biology, but I think some of the biology I mean The context that the brain has as an organ and a body plus that it's Made of metabolizing tissue. I think that's going to be part of the story about how biological intelligence is general and Those are just not part of the The technology that is there today This is Steve Potter from the Georgia Tech Laboratory for Neuroengineering Okay, so we're talking about components for general AI And you know human level AI I would say that AI experts Are obsessed with using some technology that's completely incongruent Which is digital computation to emulate a completely analog thing the brain Yes Perhaps by using a large warehouse full of servers and an entire hydroelectric power plant We may eventually see human level AGI on a digital platform That Compared to a living brain that uses less than 100 watts will be an embarrassing accomplishment We ought to be trying to model and emulate the brain using a substrate that is more like it In other words, analog Not digital The Blake Richards No scaling up by itself is probably not going to provide the entirety of the answer However, I will add The following caveat We've seen a remarkable amount of progress simply by scaling up Furthermore, there are two ways in which there's still a lot of space to scale up to What brains actually do? The first is just the size of the networks Everyone knows that the networks are getting bigger and bigger But they're still not at the same scale as real brains So We still have a ways to go before we can say that Scaling up to the size of real brains wouldn't necessarily give us additional advantages The other and in my opinion even more important question of scaling up is the question of scaling up the data sets And by this I don't mean the amount of labeled data or something like that What I mean is literally the richness of the data If we consider something like GPT-3 It's just been raised on a corpus of text That is the entirety of its experience of the world In contrast Human beings are raised in this incredible multimodal world where They connect and they get things from five different senses and there's a coherence to everything and rules of physics that apply to all of the interactions etc And then they learn a language on top of this experience So my personal guess is that we can also achieve a lot simply by giving our AI More naturalistic experiences of real rich multimodal worlds where they can act and run And this is actually a really heart problem for scaling up because honestly like to properly simulate the kind of experiences that say a human baby has Would be an immense amount of computational resources So There's a lot of scaling up we can still do now as I said at the beginning though Do I think that'll take us all of the way there? No likely not and I think the reason is that We have seen time and time again in AI That picking the right inductive biases is incredibly helpful for solving a problem And my guess is that the inductive biases that exist in as you put it fairly generic architectures like GPT-3 Are not suitable for inducing the sort of understandings of the world and representations that the human brain likely has Basically, I suspect we have additional inductive biases that are missing from these systems And I suspect those inductive biases will be key to getting the scaling up to give us something more like human intelligence But I think the scaling up is also probably critical and will bias a surprising amount This is Paul Chi's second University Montreal So I think no I absolutely don't think that scaling up Just by adding more computational power is going to get us to human level AI and I think for the most part really we're still just solving the same kinds of problems that we've already been solving for a long time essentially mapping problems between Again input and output An image and a labeled model and I think The kinds of problems where we've succeeded are ones where success is defined as as meeting the criteria of some external supervisor like a program And I don't think scaling that up was actually going to go Beyond if we scale up our performance another mapping problems. That's great But I don't think it's going to lead to human like AI because that would actually have to be its own supervisor And I don't I'm not talking about unsupervised learning Of course, there's many systems for doing unsupervised learning what I mean is we have to build systems That discover and define their own criteria for what success in the task is and I think that is not being done and that won't be suddenly achieved if you just scale up Although I do I do think that research on reinforcement learning is quite promising because I think that is probably the the path towards systems that do discover and define What successes But I think the missing component When you ask about the missing component, I think it's the same one that's always been missing and that's that's the question of meaning so Good old-fashioned AI systems Fail to achieve human like AI because they were really just about syntactic rules for manipulating symbols etc without any regard for the semantics that those symbols are supposed to capture And then neural networks replace symbols with vectors and if then rules with matrices But the tasks that they solve are still Essentially syntactic and not semantic so they don't capture meaning and many people have been saying this For for decades, you know John Sirl said this in the 80s Stephen Harnett in 90s now Melanie Mitchell and many others are saying the same thing And as far as I can tell Everyone who's actually thought about this issue pretty much agrees that this is a central issue That we need to address and that current Approaches just scaling up current approaches doesn't really address it But I think the problem is a lot of people are just not thinking about these issues because they're the kind of philosophical issues Maybe you know, they feel like their time is better spent to just you know try to build the next system that That impresses us with with its you know superhuman performance and some new mapping task You know, I think recent advances Have made a lot of people think that you know We're making such great progress that that the big insights are just around the corner And we just need and then we're like all in a race to get there as fast as we can But I don't think that's the case. I think we're rushing Smashing ourselves into the mountain as fast as we can instead of you know trying to hike up around it sort of find the path around it that's more slow. I think you know I think a lot of people by ignoring the deep Philosophical gaps are Which I think most people acknowledge that that consider them and including many of the people that that work in AI They acknowledge them, but I think Apart from that there's there's just so much excitement that people rush into the field with that they think well You know, I'm gonna make the big next bigger system and that will be intelligent I don't think that's the case of course I could be wrong This is Brad Love from UCL All right next question given the continual surprising progress and AI powered by scaling up parameters and using more compute While using fairly generic architectures eG GPT3 Do you think the current trend of scaling compute can lead to human level AGI? I don't think just more of the same is going to you know give you AGI For example When you train more and more on the same type of data or just make the same kind of architecture bigger and bigger I mean diminishing returns are going to kick in and if you take GPT3 as a example I mean it doesn't it's amazing, but what it's really doing is just passively Observing a lot of text and it's building really interesting Bedding spaces and I think it's showing systematic understandings of domains. I mean I'd like to test it More, but you know, it's not a reasoning system It's not um even really about language understanding. It's a language model And it's a quite good one at least from my perspective from an information theory perspective Because you know what I would want my language model to do is reduce my uncertainty and you know Which word is going to come next and GPT3 is great at that So I guess you know, what are we going to need to actually get to the human level AGI? I mean, I think one problem with things like GPT3 is it's really just Learning in one modality, right? It's just observing text and of course first There's this issue too if you know you need to be actively engaged with your environment So you think of a child just playing and you know bouncing a ball around or banging on their head Not only are they actively engaging in a complex environment, but they're getting multimodal information, you know, so just not a bunch of words coming right there have the visual feedback They have you know feedback from their their their arms or muscles proprio sensory feedback And so I think when we have all these different kinds of embedding spaces We could build for the same event and we could link them together Then we can get a richer understanding of a domain so like bread roads and I have a Paper and nature machine intelligence on this and I think This is really true. So we can't really just make these systems that just do One thing when modality just words or just to images But I have to link these embedding spaces and you know people do that, but I think that's a really promising direction I'm Jay McClellan I think that of course that general artificial intelligence is gonna continue to benefit from scaling up and that In some ways we have just a huge further distance to go in terms of reaching the kind of scale that You know the parallel distributed processing systems we have in our brain offers us At the same time I don't really think that The kinds of agent models that are currently Exhibiting you know surprising successes are gonna Eventually solve the problem and you know if you read the GPT3 paper The authors seem to share that view they appreciate Many of the limitations that their model has and they express considerable hope and expectation that They continue to be able to make improvements, but not just by scaling it up There are several interesting ideas in their paper about about directions to go I'm speaking for myself You know the way I've thought about this is what would it take to create an artificial PhD student? You know somebody who could attend classes take notes think about the ideas that other people have offered Start developing their own ideas about what they wanted to pursue for their research and you know draw on the ideas of others and build a program and And a direction for themselves and end up being somebody like You know Jeff Henson or Dave Rommelhard or Even Einstein and Isaac Newton right, you know these are all people who Came into a field saw where things were saw things that weren't kind of problems that weren't being solved Spend a lot of time thinking hard about how to solve them and developing new ideas And I think that really requires You know developing more Agency in our agents right the agents we have now are reacting to environments maybe they're engaged in Certain kind of roll out like behaviors that start to start to look like planning and thought In ways that are are certainly interesting, but They don't have any sense as far as I can tell of a long-term ability to formulate a goal and to work towards a goal And I you know I think that um as a general matter AIs and Really quite as goal directed as it should be even you know reward maximization isn't the same as You know wanting to get Gas in your car. I mean wanting to get gas in your car is a very specific thing Which gas station is open right if there's gas shortages where are their lines and stuff like that? So you're you're you're planning very much in a very specific problem space about a very specific And we need more of that and we need more of agents that can have these goals over very extended time periods and You know the way we do when we're planning a research agenda or something like that Let's take a quick little break and then we'll get back to the responses My name is the Osiumo Fred and I'm finishing my PhD. I hope to listen in Norway Oslo Matin University during my PhD I was searching for how this computational models used for psychology and then I discovered James McLellan I mean I haven't thought that he is a well-known person so I was searching for him to get familiar with the dogs and then I I listened to your parents by interview with him But the way he answered your questions and he kind of positive and he was really instructive the kind of question you had with him At the end was kind of Made me courage to email him and see if I can visit him I mean, I'm not sure if I email other Guests of you maybe they will respond with this. But he was really Nice, you know, some kind of discussion general like academia versus Industry this kind of questions was also useful for me because I I rethink about Should I stay in academia or should I go for industry and at some point Because I see there are many people who are doing really great jobs and think Okay, you you don't need to stay in academia. You can go on to something else because there are many great scientists who are really doing better jobs than you so Megan Peters University of California, Irvine Given the continual surprising progress in AI powered by scaling up parameters and using more compute while using fairly generic architectures like GPT3 Do you think the current trend of scaling compute can lead to human level AGI and if not what is the critical missing component So my answer is no I don't think that more and more compute is going to solve the problem I don't think that it's going to magically create a breakthrough to just crank up how much horsepower we throw at a particular problem I feel pretty strongly actually that there are some components to the to the system that we don't really understand yet And then we don't understand how to build yet Gary Marcus actually talks about the need for what he calls a substrate To support the capacities that are necessary for AGI artificial general intelligence to be able to flourish and Not about just building an AI with certain capacities in a modular kind of way But building this substrate to support those capacities in their development So we can't just take like the compositionality module and the symbolic manipulation module and the meta learning module and like plug them in To make a super AI So I agree with him that that approach isn't going to help us build an AI that can actually be truly intelligent Could actually learn how to maximally or optimally use these types of capacities that we might give it in this modular sense And certainly that kind of modular approach is not going to allow An artificial intelligence agent to work on developing these certain capacities beyond their original programming Which I kind of think is one of the things that drives our own intelligence the capacity to kind of evolve to Fluidly switch between these modules to decide when each module is appropriate and how to best use it in that instance So I'm I think I'm going to copy Gary here at least in a small way and At least in Safari saying that We need something like what he calls this substrate We need to develop an infrastructure that can support an agent in selecting these modules like we would Or even in developing new modules And I think that that's something that we probably can't get from just throwing more and more and more compute at the problem with our existing tool kits I Hi Paul this is Dean Boanumano Given the continual surprising progress in AI powered by scaling up parameters and using more compute while using fairly genetic generic architecture such as GPT Um, do I think the current trend of scaling compute can need to human level AGI So GPT 3 is indeed spookly impressive Um, and in many ways it seems to have essentially passed the terrain test But ultimately I think it's really an example of the amazing power the astonishing power of Statistically guided mixing and matching of words and sentences So in some ways I find alpha 0 chess and alpha 0 go More impressive because they've clearly exceeded human performance where GPT 3 is basically using human Templates that it was exposed to to mix and match words and sentences What I was particularly surprised about with GPT 3 at least as I understand it is that there's no recurrency Um, that is its neural net is entirely feed forward This is of course in sharp contrast with absurdly recurrent Nature of the brain circuits which is the brain is littered with positive and negative feedback And I expect recurrency and feedback is critical for AGI or artificial general intelligence for example, it's hard for me to Imagine how the GPT architecture would support coming up with two hypotheses and mulling them over And then reporting which one thinks is best. I don't think the GPT architecture supports that so I don't think That type of architecture will scale up to AGI Um, additionally it should be said that your question is a bit of a trick question because nobody really agrees Um, to any fixed criteria or fixed definition of what a GI is Um, perhaps we'll know it when we see it or perhaps enough knowledge and statistics will pass as a GI. I don't know Um, but if we go back to the 50s and 60s many people thought that beating a human at chess Should be taken as proof of human level reasoning and intelligence But we we change that bar pretty quickly in the 90s for for obvious reasons Um, so similarly, I think AGI will be a moving bar for a while Talia Conkel Given the continual surprising progress in AI powered by scaling up parameters and using more compute while using fairly generic architecture For example, to be three Do you think the current trend of scaling compute can lead to human level AGI? Um, I guess I reject the premise and like I don't know what human level AGI is I think I may be a bit contrarian on this one. I think we were good at a lot of different tasks but also because we practice a lot of those tasks and I don't quite know what human level AGI is so I can't answer that question I'm Steve Grovesberg Given the continual surprising progress in AI powered by scaling up parameters and using more compute Do you think the current trend of scaling compute can lead To human level artificial general intelligence Well, my previous reply leads me to say no If you're going to use deep learning But yes, if you use neural architectures such as the predictive art architecture that I published in 2018 And you can download from my webpage site s it s dot b u dot ed u slash ST e be g sites that dot be you dot ed u ste g This predictive art or part architecture Includes multiple parts of the brain each carrying out functions That are enabled by their interactions with other brain Regents were over new and really revolutionary computational paradigms Underlie our brains astonishing abilities to adapt to changing environmental challenges To the paradigms that I was lucky enough to introduce so-called complementary computing Which clarifies what is the nature of brain specialization Why are there so many Brain regions doing parallel processing with multiple processing stages And laminate computing which is why are all neocortical circuits organized into characteristic layers I'm Nathaniel Daw at Princeton University I think it won't by itself. I think it's You know, I've been around long enough to see the Pendulum swing a few times and I think it's obvious that it's going to swing back towards More understanding more theory And not just sort of this engineering approach of like dumping more and more Data in larger and larger models and sort of tweaking things and seeing if you can get it to work Like that's worked surprisingly well and that's great, but You know the last phase of this ended with A lot of stuff that was more motivated by theory and kind of higher level understanding And that works too And I think the most exciting stuff in AI Already right now Is stuff that's a little more nuanced and involves a sort of mixture of Sort of classic algebraic methods that do the parts that are statistical methods that do the sort of parts that are Amanable to that Augmented by deep networks for other parts that are amenable to the things they're good for so things like this sort of line of alpha-go Algorithms and networks They sort of integrate regular old research with sort of deep networks and sort of clever ways and I think that's where the future is Okay, so my name is Marcel von Karren. I'm a chair of the AI department at the Dundles Institute Yes, but there are some problems we need to fix first So scaling up. Yes, that's that's very nice But we also have live on on a planet with finite resources So if we do the scaling up we definitely need to make things more efficient, right? So I think that will be a big undertaking and that's also what you see happening people moving away from Using large GPU clusters to Doing computing on the edge or embedded systems Working on tiny ML solution So really trying to think about okay, how do we do this in a much more efficient manner? And then working on tiny ML that's that's one way to do it working on Neuromorical computing solutions which really embrace event-driven computation that's another solution, but I think in the end it's also About bringing computation closer to To visit so there's also quite a bit of work in material science that is trying to Make materials ready for computation. So really trying to do things at a more Atomic skill and that will definitely help us improve the efficiency of current AI systems Kanaka Rajan before I get into the answer right these advances GPT for instance are astounding at what they're trained to do. So let me state state that up front Just the claim that they're gonna do all of the things magically bugs me because it sounds almost like a religious stance So that said do I think that the current trend of scaling compute You know where while without changing anything about fundamentally the architecture so anything can lead to human level AGI plainly speaking no Scaling compute is basically the same as algorithmically, you know as filling a giant lookup table brute force So you know someone recently equipped that it's just like trying to go to the moon by building a tall ladder So you know, I may have somewhat heretic views on this compared to the traditional ML bro you talked to but But first yes, you know humans we do a lot of things some at the same time even but none of them perfectly well So computers even calculators have been outperforming us for decades now. So there's that But the key is that they're do so in specific things The second thing I take issue with is the exceptionalism implied by such a goal stating That is that human brains are somewhat this mystical magical pinnacle of generalized intelligence Yes, we're smart. Yes, we have Bach and math and music but other nervous systems Including not even strictly embodied ones like cephalopods do a number of pretty amazing things behaviorally And you know all of the biology that got there got there through evolution and through very different wetware So let's at least pretend to care about these aspects of biology. I mean, you know, it is brain inspired So what is that critical component right? So that's the second part of this question and what are practical ways to obtain such components I think understanding that's one so does gpt truly understand what it's generating responses to Compositionality is another aspect that comes to mind So even you know, I hesitate to even use the word proto languages, but something like whale song right contains elements of compositionality So I think that the fundamentally noteworthy theoretical advances towards understanding cognition and then eventually emulating that in you know human level AGI systems will come from novel network models that incorporate more biological architectures like you know feedback recurrent connections multi region interactions complex neuron like units time varying patterns of activity dynamics and then partly or fully unsupervised learning algorithms. So practically I say talk to neuroscientists Or generally scientists whose models are geared towards being able to understand how biological wetware Implements the functionality one is interested in This is john crack hour No I think you know, it's interesting I think Take for example you mentioned gpt 3 right so there are people if they who will say that if you look at the paper Unusual things begin to happen at some Scale like suddenly it can add numbers when in fact it was never given You know addition to learn overtly Um so there are emergent properties That seem to come along just by scaling up as you were But no, I mean It does sentence completion. That's what gpt 3 does It it it doesn't understand things and the idea that suddenly Just doing more of the same Understanding will suddenly just pop out Um, I feel is a Very very odd stance to take I don't think it's clear How to proceed there are some people in AI who believe that you have to come up with some more circumscribed definition of thinking and and and one idea might be that thinking is the ability To fill in the incomplete knowledge that is right in front of you and in other words the world in front of you With knowledge that you bring from your past and somehow you can use that knowledge to dictate your behavior At that moment and thinking is this ability to bring knowledge to bear on your current context Now whether there's going to be an ability to formalize that and to define it in such a way that you can get traction on it is Unclean to me. I think there are people in the world in AI who know this and realize it's a problem How do you bring declarative over knowledge and facts to bear on decision-making Uh, and do it in a way that you're not just going back through the back door to go fi and program it all in But you know people like Gary markers. I think we'll say just accept that you're going to have to do both Uh find a way to program in knowledge And then have that used in some way by deep neural nets In combination, but it's a complete crapshoot right now So I'm Rodrigo Kean-Kiroga your scientist at the University of Lester I think I mean I think we're still not Looking deep enough on I mean to how the human brain actually works And this is my research. I mean I record a human brain and I see some principles Which they are Kind of like on the at-tempo that's of of what I mean what we see in AI And if if I keep it at a very simple level, I mean in principle a computer We'll just uh replicate information with and But the computer does not have understanding and I think a basic principle of how the human brain works and particularly for memory Which is what I study is that we forget a lot I mean, there's a lot of information that we don't even process So we're very good at extracting very little information that we consider to be meaningful um Extract some sense from this information and then working with these sense that we're struck on that So this process of abstraction of going to the high level Sync and if you like involves a lot of forgetting because to in order to abstract something you have to forget details. You have lead you have to leave details aside and Of course this leads to a lot of errors no because the moment you forget details. I mean there's things that I mean you will miss and I see the way we have been developing computers in general machines is is not to have this error And therefore not to forget things But I think this is the opposite to what you want to try to see if you can somehow replicate human level intelligence Which is based on forgetting a lot of information and develop developing common sense in order to do the things that I'd say before General intelligence ability of abstraction making inferences and so on I Grace Lindsay I am a postdoc at the Gatsby computational neuroscience unit at University College London Do you think the current trend of scaling compute can lead to human level EGI? I'm going to Include scaling data in this as well Because GPT3 uses a lot of data as well Probably not no I don't think so Especially if you want to mimic human learning You know, maybe you can get Something an artificial agent that's very good at a task By giving it you know 8,000 human years Worth of time to learn and all of the written you know language in the world or something like that That's not what humans have that's not what they use to get to human level intelligence So certainly if you want to replicate the learning of humans I don't think that just more compute and more data is going to be the answer also, I guess there's probably something to the The versatility of humans and the fact that it's you know a single brain doing so many vastly different tasks from motor control to sensory processing to planning and all those kinds of things and I don't even know What you know more data would mean in that circumstance you'd have to have kind of all data about everything Uh, so yeah, I don't think just pure scaling is going to be put to get to human level AGI I think it'll get you far on certain specific tasks Uh, like a single agent doing a single task you can probably get pretty far just by scaling with compute and data Um, but an integrated agent that learns at the rate of humans probably needs something else In terms of advances in learning algorithms or architectural modules honestly, I have no idea because if I knew I would publish it My name is Conrad Codding. I'm a neuroscientist at the University of Pennsylvania Given the continual surprising progress in AI powered by scaling up parameters and using more compute Um, while using fairly generic architectures eGGPT3. Do you think the current trend of scaling compute can lead to human level AGI? I absolutely don't think that So if we look at GPT3 It's in a way really big lookup table not like it's been trained with everything that humans have ever said or at least a significant subsection of that so In that sense the answer to most question is already in that data set But a lookup table isn't intelligence the thing that makes us be so impressive in a way at solving difficult tasks It is is that we can deal with new situations based on very very limited amount of data GPT3 cannot do that and it's not that there's something wrong about GPT3 It is just that GPT3 completely lacks what makes human intelligence so interesting it cannot argue internally If cannot tell stories And in fact the thing that makes texts written by GPT3 so sad Is that it completely gets story wrong So if the question is what are the critical missing components? I think if you ask me the critical missing component is the ability to tell stories Let me highlight why I think this is some problem The world around us the world that we construct with our friends and families is very much a world of stories Where we talk about high level phenomena and we over very sharp periods of time maybe a few minutes Go through something that might otherwise take a really long period of time And I think one of the things that we're critically missing is Machine learning systems that really tell themselves stories This is Jeff Hawkins Can we get to human level AI by scaling our current systems or do we need to do something fundamentally different As long as I've been interested in the field of intelligence I have believed that we first have to study the brain You have to study the brain to not only learn what mechanisms are used in the brain to create intelligence But to learn what intelligence itself is For I don't believe we have a good working definition of intelligence Today's AI is focused primarily on solving particular tasks using particular techniques that seem to work But without a fundamental understanding what intelligence is Here's what we've learned by studying the brain We've learned that intelligence is Based on the ability of our brain to learn a model of the world We have a tremendous amount of knowledge about the world in our heads And it's stored in a model The model tells us what things look like what they feel like what they sound like The model tells us where things are relative to each other and the model tells us how to how things change As we manipulate them and as we move about the world It is the model that is the basis of intelligence Not any specific tasks that we might accomplish But a model allows us to accomplish almost any task We can ask given our model of the world how we might achieve a certain result Or what will happen if we take certain actions We've learned a lot about how the brain learns a model of the world I'm going to just talk about three high-level components The first is we learn through movement We learn a model of the world we don't just sit statically and look at images We move our bodies we move our eyes constantly we move our fingers we pick things up We poke things we hear we touch something and see what it sounds like see what it feels like We push things we try things out We have to move to the world we have to move our centers to the world we have to interact with the world to learn This is probably almost completely lacking from today's AI The second component of how the brain learns a model of the world is how is it stores information How does it store knowledge what we've learned is that the brain stores knowledge using reference frame You can think of a reference frame like our Cartesian coordinates x y and z It's a structure that is a metric structure meaning in the brain assigns knowledge to it So model wants to learn something it assigns the sensory inputs to locations in a reference frame In the brain These are implemented by types of cells called grid cells and play cells And we've figured out that grid cells and play cells exist throughout the entire near cortex, which is the organ of intelligence It's essential to have a way of representing knowledge and Reference frames are the way that brains do this today's AI do not have anything equivalent to this And finally the third big topic I want to talk about what we've learned about brains Is knowledge in the brain is distributed in a certain way If I ask you where is knowledge about a cell phone in your brain? It is not in one location It is not in two locations It turns out that in your brain specifically the neo cortex you have thousands of models of your cell phone They are complementing models. There's models of what your cell phone looks like. There are models of what your cell phone A field like there are models of how it the sounds it makes And so knowledge is distributed across many many models in the human near cortex There's about 150,000 cortical columns each one is a modeling system So it's a highly distributed system And I believe that you have to build and tell the systems this way too It allows the flexibility of integrating different modalities Such as hearing touched envision It allows the system to be deal with ambiguity Because some models will be understanding some parts of the world I'll look and be understanding other parts of the world And allows us to build intelligent machines in different bodyments That can have all types of different shapes that don't have to look like humans So again the three things on argument that are missing in today's AI three huge things are learning through movement storing knowledge and reference frames and a highly distributed modeling system We've written a series of neuroscience papers about this and we call the overall theory of the thousand brains theory And we just started taking these ideas and implementing them in machine learning in AI If we think that is the future I've also written a book this coming out in March called a thousand brains which will cover this topic both from a neuroscience and an AI point of view Thank you for listening I Original son from Princeton University So I think that the scaling in AI is this miniscule relative to the brain You know, GPT-3 have one 50 billion parameters that's unlike crazy If I'm taking two boxes in the brain six millimeter of cortex. I have more synapses than GPT-3 So the scaling in the brain is way larger than what we've seen in machine learning So I'm never like scared about it like scaling argument saying that I think that The foundations of the brain and the foundation and machine learning are very similar the brain is also using a direct fit and blind supervision In a closed loop dynamical system puts to fit to the world without understanding So I see deep connections between the brain and this machine learning But this is only the starting point It will be a mistake to say that machine learning have any intelligence But human do So while I think that the foundations are very similar between human and machine something is really missing in machine that we have new right We can understand physics we can understand rules we can have calculus we can go to the moon we can think this Massines do not think So I think that while they are the foundation is similar The next stage in AI and in cognitive no science is to understand alpha dislike blind fit and other parameters asian And memorization and interpolation and fitting you can go to the next state of extracted formation about the wall by taking the shape of the wall so that's something that We will need to think about and it's really missing in machine learning My name is Jessica Hammrich. Do I think the current trend of scaling compute can lead to human level a GI? No I think that while of course the recent advances are very impressive In terms of you know showing what we can do when as we do continue to scale up compute and data I think having a generally intelligent system requires both Lots of experience and being able to choose the experience that it performs on the world so you need systems that are able to perform interventions on the world and learn from those interventions I think just treating the problem as a supervised learning or prediction problem isn't going to get us there But then when it comes to actually scaling up the compute and data for actually performing interventions I think that this is really hard and not something that is easy to do You know we can train for example agents and simulation which of course can learn to interact with the world But these simulations that we can develop are really impoverished compared to the real world So should you compare to like the you know millions of years of evolution and the vast complexity of the natural world There's so many different species so much complexity in the real world that we've evolved to learn to interact with and that's not something that We are currently able to replicate in simulation at all And of course we also don't want agents sort of just running wild in the real world And so there's a real question of how is it going to be possible at all to really scale up the data and compute To train these types of agents and I think the answer is probably no And for that reason I think that we need to adopt a hybrid approach where we look for like what are the most promising types of inductive biases or structures that we can build into agents that You know we know evolution like settled on as good good solutions to the problem Figure out how to combine those with powerful learning systems so that we don't have to replicate the entirety of evolution Because I think that that's basically what we were saying is what we would want to do is like with enough data and compute We can replicate evolution, but I just don't I just don't think that is really a possibility I Thomas Nossalaris Department of neuroscience University of Minnesota Do you think the current trend of scaling compute can lead to human level AGI? No, I don't Why and what's missing? It's something like creativity, which I think requires very efficient world modeling That would lend itself to testing hypothesis about causality and the consequences of ones and in other agents actions I would guess that emotion is also an important part of Of AGI I'm just guessing. I know very little about that But it's certainly salient and it's certainly guides a lot of the way that we think and I I'm not at all convinced That it's something that we can be smart without Emotion that is what are the practical ways to obtain such components fastest more funding for basic neuroscience I Brain inspired is a production of me and you I don't do advertisements You can support the show through patreon for a trifling amount and get access to the full versions of all the episodes Plus bonus episodes that focus more on the cultural side, but still have science Go to braininspired.co and find the red patreon button there To get in touch with me email paul at braininspired.co The music you hear is by the new year find them at the new year.net Thank you for your support. See you next time Maybe
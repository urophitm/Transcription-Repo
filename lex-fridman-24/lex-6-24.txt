What possible ideas do you have for the human species ends? Sure. So I think the most obvious way to me is wire heading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok and forgetting to eat. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now to be fair, it's probably hard to get all of humanity. Yeah. The interesting thing about humanity is the diversity in it. Oh yeah. Organisms in general. There's a lot of weirdos out there. Well, two of them are sitting here. I mean, diversity in humanity is... We do respect. I wish I was more weird. The following is a conversation with George Hots, his third time on this podcast. He's the founder of Kama AI that seeks to solve a time was driving and is the founder of a new company called Tiny Corp that created Tiny Grad. A neural network framework that is extremely simple with the goal of making it run on any device by any human easily and officially. As you know, George also did a large number of fun and amazing things from hack-in-i phone to recently joining Twitter for a bit as an intern in quotes making the case for refactoring the Twitter code base. In general, he's a fascinating engineer and human being and one of my favorite people to talk to. This is Alex Reubener podcast. The supported, please check out our sponsors in the description. And now, dear friends, here's George Hots. You mentioned something on a stream about the philosophical nature of time. So let's start with a wild question. Do you think time is an illusion? You know, I sell phone calls to Kama for $1,000. And some guy called me and you know, it's $1,000 you can talk to me for half an hour. And he's like, yeah, okay. So time doesn't exist. And I really wanted to share this with you. I'm like, what do you mean time doesn't exist? I think time is a useful model. Whether it exists or not, right? Just quantum physics exists. Well, it doesn't matter. It's about whether it's a useful model to describe reality. Is time maybe compressive? Do you think there is an objective reality or is everything just useful models? Like underneath it all, is there an actual thing that we're constructing models for? I don't know. I was hoping you would know. I don't think it matters. I mean, this kind of connects to the models with constructive reality with machine learning, right? Sure. Is it just nice to have useful approximations of the world such that we can do something with it? So there are things that are real. Column Graph Complexity is real. Yeah. The compressive thing. Math is real. Yeah. This would be a t-shirt. And I think hard things are actually hard. I don't think P equals NP. Oh, strong words. Well, I think that's the majority. I do think factoring is in P, but I don't think you're the person that follows the majority in all walks of life. So it's good. That one I do. Yeah. In theoretical computer science, you're one of the sheep. All right. But do you times a useful model? Sure. What were you talking about on the stream with time? Are you made of time? If I remember half the things I said on stream, someday someone's going to make a model of all of it and it's going to come back to haunt me. Someday soon? Yeah, probably. Would that be exciting to you or sad that there's a George Hots model? I mean, the question is when the George Hots model is better than George Hots? Like I am declining and the model is growing. What is the metric by which you measure better a worse in that if you're competing with yourself? Maybe you can just play a game where you have the George Hots answer and the George Hots model answer and ask which people prefer? People close to you or strangers? Either one, it will hurt more when it's people close to me, but both will be overtaken by the George Hots model. It'd be quite painful, right? I loved ones, family members, would rather have the model over for Thanksgiving than you. Yeah. Or like significant others would rather sext. With the large language model version of you. Especially when it's fine tune to their preferences. Is that so? Yeah. Well, that's what we're doing in a relationship, right? We're just fine tuning ourselves, but we're inefficient with it because we're selfish, ingredient, and so on. Our language models can fine tune more efficiently, more selflessly. There's a Star Trek Voyager episode where Catherine Janeway lost in the Delta Quadrant makes herself lover on the holodeck. And the lover falls asleep on her arm and he snores a little bit and Janeway edits the program to remove that. And then of course the realization is, wait, this person's terrible. It is actually all there nuances and quirks and slight annoyances that make this relationship worthwhile. But I don't think we're going to realize that until it's too late. Right. Well, I think a large language model could incorporate the flaws and the quirks and all that kind of stuff. Just the perfect amount of quirks and floor flaws to make you charming without crossing the line. Yeah. And that's probably a good approximation of the percent of time the language model should be cranky or an asshole or jealous or all this kind of stuff. And of course it can and it will, but all that difficulty at that point is artificial. There's no more real difficulty. Okay. What's the difference in real and artificial? Artificial difficulty is difficulty that's like constructed or could be turned off with a knob. Real difficulty is like you're in the woods and you got to survive. So if something cannot be turned off with a knob, it's real. Yeah, I think so. Or I mean, you can't get out of this by smashing the knob with a hammer. I mean, maybe you kind of can, you know, I into the wild when I, you know, Alexander Super Tramp, he wants to explore something that's never been explored before, but it's the 90s. Everything's been explored. So he's like, well, I'm just not going to bring a map. Yeah. I mean, no, you're not exploring. You should have brought a map to you. You died. There was a bridge in my often where you were camping. How does that connect to the metaphor of the knob? By not bringing the map, you didn't become an explorer. You just smashed the thing. Yeah. Yeah. The art, the difficulty is still artificial. You failed before he started. What if we just don't have access to the knob? Well, that maybe is even scarier. Right? Like, we already exist in a world of nature. And nature has been fine tuned over billions of years to have humans build something and then throw the knob away and some grand romantic gesture is horrifying. Do you think of us humans as individuals that are like born and die? Or is it, I would just all part of one living organism that is earth, that is nature? I don't think there's a clear line there. I think it's all kind of just fuzzy. I don't know. I mean, I don't think I'm conscious. I don't think I'm anything. I think I'm just a computer program. So it's all computation. Yeah. I think running your head is just a, is not, is a, is a computation. Everything running in the universe is computation, I think. I believe the extended church-tarring thesis. Yeah, but it, there seems to be an embodiment to your particular computation. Like, there's a consistency. Well, yeah, but I mean models have consistency too. Yeah. Models that have been RLA-Jeffd will continually say, you know, like, well, how do I murder ethnic minorities? Oh, well, I can't let you do that, how? There's a consistency to that behavior. It's all RLA-Jeff. Like, we all are RLA-Jeff each other. We find, we provide human feedback in there that thereby fine-tune these little pockets of computation. But it's still unclear why that pocket of computation stays with you like for years. It just kind of falls like you have this consistent set of physics biology. Uh, what, like, whatever you call the neurons firing, like, the electrical senders and mechanical signals, all of that, that seems to stay there. And it contains information, stores information, and that information permeates through time and stays with you. There's like memory, there's like sticky. Okay, to be fair, like, a lot of the models we're building today are very, even RLA-Jeffd is nowhere near as complex as the human loss function. Reinforcement learning with human feedback. Um, you know, when I talked about will GPT-12 be AGI? My answer is no, of course not. I mean, cross-century losses never going to get you there. You need, uh, probably RL in fancy environments in order to get something that would be considered, like, AGI-like. So to ask, like, the question about like why, I don't know, like, it's just some quirk of evolution, right? I don't think there's anything particularly special about where I ended up. Where humans ended up. So, okay. We have human-level intelligence. Would you call that AGI? Whatever we have. It's G-I. Look, actually, I don't really even like the word AGI. Um, but, general intelligence is defined to be whatever humans have. Okay. So why can GPT-12 not get us to AGI? Could we just like linger on that? If your loss function is categorical cross-entry, if your loss function is just try to maximize compression, I have a soundcloud, I rap, and I tried to get chat GPT to help me write wraps. And the wraps that it wrote sounded like YouTube comment wraps. You know, you can go on any rap beat online, and you can see what people put in the comments. And it's the most like mid-quality rap you can find. It's made good or bad? Mid is bad. It's like mids. Every time I talk to you, I learn new words. Mid. Mid. Yeah. I was like, uh, is it like basic? Is that what mid means? Kind of. It's like it's like middle of the curve. Right. Yeah. So there's like this like I like to do that intelligence curve. Yeah. Um, and you have like the dumb guy, the smart guy, and then the mid guy. Actually being the mid guys the worst, the smart guy is like, I put all my money in Bitcoin. The mid guy is like, you can't put money in Bitcoin. It's not real money. And all of it is a genius meme. That's another interesting one. Memes. The humor, the idea, the absurdity, encapsulated a single image. And it just kind of propagates virally between all of our brains. I didn't get much sleep last night. So I'm very, uh, I sound like I'm high. But I swear I'm not. Uh, do you think we have ideas or ideas have us? I think that we're going to get super scary memes once the AI is actually our superhuman. Like the gay, I will generate memes. Of course. Do you think it'll make humans laugh? I think it's worse than that. So, um, infinite jest. I think it's introduced in the first 50 pages is about a tape that you, uh, once you watch it once, you only ever want to watch that tape. In fact, you want to watch the tape so much that someone says, okay, here's a hacksaw. Cut off your pinky and then I'll let you watch the tape again and you'll do it. So we're actually going to build that, I think. But it's not going to be one static tape. I think the human brain is too complex to be stuck in one static tape like that. If you look at like ant brains, maybe they can be stuck on a static tape. But we're going to build that using generative models. We're going to build the TikTok that you actually can't look away from. So TikTok is already pretty close there, but the generation is done by humans. The algorithm is just doing their recommendation, but if the, if the algorithm is also able to do the generation, well, it's a question about how much intelligence is behind it, right? Mm-hmm. So the content is being generated by, let's say, one humanity worth of intelligence, and you can quantify a humanity, right? That's a, you know, it's it's exaflops, yada flops, but you can quantify it. Once that generation is being done by a hundred humanities, you're done. So it's actually scale that's the problem, but also speed. Yeah. And what if it's sort of manipulating the very limited human dopamine engine to porn? Imagine just TikTok, but for porn. Yeah. That's like a brave new world. I don't even know what it'll look like, right? Like again, you can't imagine the behaviors of something smarter than you, but a super intelligent and an agent that just dominates your intelligence so much will be able to completely manipulate you. Is it possible that it won't really manipulate? It'll just move past us. It'll just kind of exist the way water exists or the air exists. You see? And that's the whole AI safety thing. It's not the machine that's going to do that. It's other humans using the machine that are going to do that to you. Yeah. Because the machine is not interested in hurting humans. The machine is a machine. But the human gets the machine and there's a lot of humans out there very interested in manipulating you. Well, let me bring up Elieza Gowski, who recently sat where you're sitting. He thinks that AI will almost surely kill everyone. Do you agree with him or not? Yes, but maybe for a different reason. And then I'll try to get you to find hope, where we could find a note to that answer, but why yes? Yes. Okay. Why didn't nuclear weapons kill everyone? That's a good question. I think there's an answer. I think it's actually very hard to deploy nuclear weapons tactically. It's very hard to accomplish tactical objectives great. I can nuke their country. I want to have an irradiated pile of rubble. I don't want that. Why not? Why don't I want an irradiated pile of rubble? Yeah. All the reasons no one wants an irradiated pile of rubble. Because you can't use that land for resources. You can't populate the land. Yeah. What you want a total victory in a war is not usually the irradiation and eradication of the people there. It's the subjugation and domination of the people. Okay. So you can't use this strategically, tactically, in a war to help you to help gain a military advantage. It's all complete destruction. All right. But there's the egos involved. It's still surprising. It's still surprising that nobody pressed the big red button. It's somewhat surprising. But you see, it's the little red button that's going to be pressed with AI. That's going to, you know, and that's why we die. It's not because the AI, there's anything in the nature of AI. It's just the nature of humanity. What's the algorithm behind the little red button? What like, what what what possible ideas do you have for the how human species ends? Sure. So I think the most obvious way to me is wire heading. We end up amusing ourselves to death. We end up all staring at that infinite TikTok and forgetting to eat. Maybe it's even more benign than this. Maybe we all just stop reproducing. Now, to be fair, it's probably hard to get all of humanity. Yeah. It probably, the interesting thing about humanity is the diversity in it. Oh yeah. Organisms in general. There's a lot of weirdos out there. Two of them are sitting here. I mean, diversity in humanity is. We do respect. I wish I was more weird. No, like I'm kind of, look, I'm drinking smart water, man. That's like a Coca-Cola product, right? Do you want corporate George Hawkins? No, the amount of diversity in humanity I think is decreasing. And just like all the other biodiversity on the planet. Oh boy. Yeah. Right. Social meat is not helping. Go eat McDonald's in China. Yeah. Yeah. Yeah. No way. It's the interconnectedness. That's that's that's that's doing it. Oh, that's interesting. So everybody starts relying on the connectivity of the internet. And over time, that reduces the diversity, the intellectual diversity. And then that gets you everybody into a funnel. This is still going to be a guy in Texas. There is. And yeah, and bunker to be fair. Do I think AI kills us all? I think AI kills everything we call like society today. I do not think it actually kills the human species. I think that's actually incredibly hard to do. Yeah, but society, like if we start over, that's tricky. Most of us don't know how to do most things. Yeah, but some of us do. And they'll be okay. And they'll rebuild after the great AI. What's rebuilding look like? Like how much do we lose? Like what is human civilization done? That's interesting. Combustion engine, electricity. So power and energy. That's interesting. Like how to harness energy. Well, well, well, well, they're going to be religiously against that. Are they going to get back to like fire? Sure. I mean, there'll be a little bit like, you know, some kind of Amish looking kind of thing. I think I think they're going to have very strong taboos against technology. Like technology is almost like a new religion. Technology is the devil. And nature is God. Sure. So closer to nature, but can you really get away from AI? If it destroyed 99% of the human species, isn't it somehow have a hold like a stronghold? Well, it's interesting about everything we build. I think we are going to build superintelligence before we build any sort of robustness in the AI. We cannot build an AI that is capable of going out into nature and surviving like a bird. A bird is an incredibly robust organism. We've built nothing like this. We haven't built a machine that's capable of reproducing it. Yes. But there's a, you know, I work with leg robots a lot now. I have a bunch of them. They're mobile. They can't be produced. But all they need is, I guess you're saying they can't repair themselves. But if you have a large number, if you have like 100 million of them, let's just focus on them reproducing, right? They have microchips in them. Mm-hmm. Okay. Then do they include a fab? No. Then how are they going to reproduce? Well, they're, hey, it doesn't have to be all on board, right? They can go to a factory to a repair shop. Yeah, but then you're really moving away from robustness. Yes. All of life is capable of reproducing without needing to go to a repair shop. Life will continue to reproduce in the complete absence of civilization. Robots will not. So when the, if the AI apocalypse happens, I mean, the AI's are going to probably die out. Because I think we're going to get, again, super intelligence long before we get robustness. What about if you just improve the fab to where you just have a 3D printer that can always help you? Well, that'd be very interesting. I'm interested in building that. Of course you are. You think how difficult is that problem? To have a robot that basically can build itself. Very, very hard. I think you've mentioned this like to me or somewhere where people think it's easy conceptually. And then they remember that you're going to have to have a fab. Yeah, on board. Of course. So 3D printer, the prince of 3D printer. Yeah. Yeah. On legs. Yeah. It's hard. Well, because it's not, I mean, a 3D printer is a very simple machine, right? Okay, if you're going to print chips, you're going to have an atomic printer. How are you going to dope the silicon? Yeah. How are you going to etch the silicon? You're going to have to have a very interesting kind of fab if you want to have a lot of computation on board. They can do like structural type of robots that are dumb. Yeah, but structural type of robots aren't going to have the intelligence required to survive in any complex environment. What about like ants type of systems we have like trillions of them? I don't think this works. I mean, again, like ants at their very core are made up of cells that are capable of individually reproducing. They're doing quite a lot of computation that we're taking for granted. It's not even just the computation. It's that reproduction is so inherent. Okay, so like there's two stacks of life in the world. There's the biological stack and the silicon stack. The biological stack starts with reproduction. Reproduction is at the absolute core. The first proto-RNA organisms were capable of reproducing. Right, the silicon stack, just by as far as it's come, is nowhere near being able to reproduce. Yeah. So the fab movement, the digital fabrication, fabrication in the full range of what that means is still in the early stages. Yeah. You're interested in this world. Even if you did put a fab on the machine, right? Let's say, okay, we can build apps. I mean, how does your best humanity? We can probably put all the precursors that build all the machines and the fabs also in the machine. So first off, this machine is going to be absolutely massive. I mean, we almost have a, like, think of the size of the thing required to reproduce a machine today. Right? Like, is our civilization capable of reproduction? Can we reproduce our civilization on Mars? If we were to construct a machine that is made up of humans, like a company that can reproduce itself. Yeah. I don't know. It feels like, like, 115 people, I get somewhat harder than that. 120. I just think it's a number. I believe the Twitter can be run by 50 people. I think that this is going to take most of, like, it's just most of society, right? Like, we live in one globalized world. No, but you're not interested in running Twitter. You're interested in seeding. Like, you want to see the civilization. And then because humans can like, Oh, okay, you're talking about, yeah, okay. So you're talking about the humans reproducing and like, basically, like, what's the smallest self-sustaining colony of humans? Yeah. Yeah. Okay. Fine. But they're not going to be making five nanometer chips. Over time, they will. I think you're being like, we have to expand our conception of time here. Going back to the original. Right. At timescale, I mean, over across, maybe a hundred generations were back to making chips. No. If you seed the colony correctly. Maybe. Or maybe they'll watch our colony die out over here and be like, we're not making chips. Don't make chips. No, but you have to seed that colony correctly. Whatever you do, don't make chips. Chips are what led to their downfall. Well, that is the thing that humans do. They come up, they construct a devil, a good thing and a bad thing and they really stick by that. And they murder each other over that. There's always one asshole in the room who murders everybody. And he usually makes tattoos and nice branding. That's why he needs that asshole. That's the question. Humanity works really hard today to get rid of that asshole. But I think they might be important. Yeah, this whole freedom of speech thing. It's the freedom of being an asshole. It seems kind of important. Right. Man, this thing, this fab, this human fab that we constructed, this human civilization is pretty interesting. And now it's building artificial copies of itself. Or artificial copies of various aspects of itself that seem interesting like intelligence. And I wonder where that goes. I like to think it's just like another stack for life. Like we have the biostatck life. Like we're a biostatck life and then the silicon stack life. But it seems like the ceiling, or there might not be a ceiling or at least the ceiling is much higher for the silicon stack. Oh, no, I don't die. We don't know what the ceiling is for the biostatck either. The biostatck, the biostatck just seemed to move slower. You have Moore's Law, which is not dead despite many proclamations to the biostatck or the silicon stack. In the silicon stack. And you don't have anything like this in the biostatck. So I have a meme that I posted. I tried to make a meme. It didn't work too well. But I posted a picture of Ronald Reagan and Joe Biden. And you look, this is 1980 and this is 2020. And these two humans are basically like the same. Right. There's no, there's no like, like there. There's been no change in humans in the last 40 years. Yeah. And then I posted a computer from 1980 and a computer from 2020. Wow. Yeah, with their early stages, right? Which is why you said when you said the fab, the size of the fab required to make another fab is like very larger now. Oh, yeah. But computers were very large. 80 years ago. And they got pretty tiny. And there, people are starting to want to wear them on their face. In order to escape reality, that's the thing. In order to be live inside the computer. Yeah. Put a screen right here. I don't have to see the rest of the ushals. I've been ready for a long time. You like virtual reality? I love it. Do you want to live there? Yeah. Yeah. Part of me does too. How far away are we, do you think? Judging from what you can buy today far. Very far. I got to tell you that I had the experience of Meta's codec avatar where it's an ultra high resolution scam. It looked real. I mean, the headsets just are not quite like iResolution yet. I haven't put on any headset where I'm like, oh, this could be the real world. Whereas when I put good headphones on, audio is there. I'm like, we can reproduce audio that I'm like, I'm actually in a jungle right now. If I close my eyes, I can't tell. I'm not. Yeah. But then there's also smell and all that kind of stuff. Sure. I don't know. I, the power of imagination or the power of the mechanism in the human mind that fills the gaps that kind of reaches and wants to make the thing you see in the virtual world real to you. I believe in that power. Or humans want to believe. Yeah. Like, what if you're lonely? What if you're sad? What if you're really struggling in life? And here's a world where you don't have to struggle anymore. Humans want to believe so much that people think the large language models are conscious. That's so much humans want to believe. Strong words. He's throwing left and right hooks. Why do you think large language models are not conscious? I don't think I'm conscious. Oh, so what is consciousness then? George Hottes. It's like what it seems to mean to people. It's just like a word that atheists use for souls. Sure, but that doesn't mean soul is not an interesting word. If consciousness is a spectrum, I'm definitely way more conscious than the large language models are. I think the large language models are less conscious than a chicken. One is the last time you see a chicken. In Miami, like, a couple months ago. How? No, like a living chicken. Living chickens? Walking around Miami. It's crazy. Like on the street? Yeah. Like a chicken. All right. I was trying to call you all like a good journalist and I got shut down. Okay. But you don't think much about this kind of subjective feeling that it feels like something to exist. And then as an observer, you can have a sense that an entity is not only intelligent, but has a kind of subjective experience of its reality. Like a self-awareness that is capable of like suffering of hurting, of being excited by the environment in a way that's not merely kind of an artificial response, but a deeply felt one. Humans want to believe so much that if I took a rock and a sharpie and drew a sad face on the rock, they'd think the rock is sad. Yeah. And you're saying when we look in the mirror, we apply the same smiley face with rock. Pretty much, yeah. Doesn't it? Isn't that weird though? That you're not conscious? That? No. But you do believe in consciousness. Really? It's just, it's unclear. Okay. So do you, it's like a little like a symptom of the bigger thing. That's not that important. Yeah. It's interesting that like the human system seem to claim that they're conscious. And I guess it kind of like says something and they straight up like, okay, what do people mean when even if you don't believe in consciousness? What do people mean when they say consciousness? And there's definitely like meanings to it. What's your favorite thing to eat? Pizza. Cheese pizza. What are the toppings? I like cheese pizza. Don't say pineapple. No, I don't. Okay. Pepperoni pizza. And they put any ham on it. Oh, that's real bad. What's the best, what's the best pizza? What are we talking about here? Like you like cheap crappy pizza? I actually call go deep dish cheese pizza. Oh, that's that's my favorite. There you go. You bite into a deep dish. Chicago deep dish pizza. And it feels like you were starving. You have an eat 24 hours. Just bite in and you're hanging out with somebody that matters a lot to you. And you're there with the pizza. Sounds good. Yeah. All right. It feels like something. I'm George motherfucking hot eating a fucking Chicago deep dish pizza. There's just the full peak living experience of being human. The top of the human condition. Sure. It feels like something to experience that. Why does it feel like something? That's consciousness, isn't it? If that's the word you want to use to describe it, sure. I'm not going to deny that that feeling exists. I'm not going to deny that I experience that feeling. When I guess what I kind of take issue to is that there's some like like how does it feel to be a web server? Do 404s hurt? Not yet. How would you know what suffering looks like? Sure, you can recognize a suffering dog because what is same stack as the dog? All the bio stacks stuff kind of, especially mammals. You know, it's really easy. You can game recognize this game. Yeah. Versus the silicon stack stuff, it's like you have no idea. You have you. It's oh wow. The little thing has learned to mimic. You know. But then I realized that that's all we are too. I'll look the little thing has learned to mimic. Yeah. I guess yeah, yeah, 404 could be, could be suffering, but it's so far from our kind of living organism, our kind of stack. But it feels like AI can start maybe mimicking the biological stack, but about about it because it's trained. Retrained it, yeah. And so in that, maybe that's the definition of consciousness. Is the bio stack consciousness? The definition of consciousness is how close something looks to human. Sure, I'll give you that one. No, how close something is to the human experience. Sure. It's a very anthropocentric definition, but where that's all we got. Sure. No, I don't mean to like, I think there's a lot of value in it. Look, I just started my second company. My third company will be AI Girlfriends. No, like I mean, I want to find out what your fourth company is after all. Wow. Because I think once you have AI Girlfriends, it's, oh boy, does it get interesting? Well, maybe let's go there. I mean, the relationships with AI, that's creating human-like organisms, right? And part of being human is being conscious, is having the capacity to suffer, having the capacity to experience this live richly. In such a way that you can empathize, that AI is going to empathize with you, and you can empathize with it. Or you can project your anthropomorphic sense of what the other entity is experiencing. An AI model would need to create that experience inside your mind. And it doesn't seem like difficult. Yeah, but okay, so here's where it actually gets totally different, right? When you interact with another human, you can make some assumptions. Yeah. When you interact with these models, you can't. You can make some assumptions that that other human experiences, suffering, and pleasure in a pretty similar way to you do. The golden rule applies. With an AI model, this isn't really true, right? These large language models are good at fooling people, because they were trained on a whole bunch of human data and told them to mimic it. Yep. But if the AI system says, Hi, my name is Samantha. It has a backstory. Yeah. I want to call it here and there. Maybe you'll integrate this in the AI system. I made some chatbots. I give back stories. It was lots of fun. I was so happy when Lama came out. Yeah. We'll talk about Lama. We'll talk about all that. But like, you know, the rock with the smiley face. Yeah. It seems pretty natural for you to anthropomorphize that thing, and then start dating it. And before you know it, you're married and have kids with a rock. With a rock. This picture is on Instagram with you and a rock. And smiley face. To be fair, like, you know, something that people generally look for when they look over someone's data is intelligence. In some form and the rock doesn't really have intelligence. Only a pretty desperate person would date a rock. I think we're all desperate deep down. Oh, not rock level desperate. All right. Not rock level desperate, but AI level desperate. I don't know. I think all of us have a deep loneliness. It just feels like the language models are there. Oh, I agree. And you know what? I won't even say this so cynically. I will actually say this in a way that like, I want AI fronts. I do. Yeah. Like, I would love to. You know, again, I, the language models now are still little. Like, people are impressed with these GPT things. And I look at like or like or the co-pilot, the coding one. And I'm like, okay, this is like junior engineer level. And these people are like five or level artists and copywriters. Like, okay, great. We got like five or and like junior engineers. Okay, cool. Like, and this is just a start. And it will get better. Right? Like, I, I can't wait to have AI friends who are more intelligent than I am. So five or just a temporary. It's not the ceiling. No, definitely not. Is it, is it countless cheating when you're talking to an AI model, emotional cheating? That's up to you when you're human partner to define. Oh, you have to. All right. You're getting, yeah, you have to have to have that conversation, I guess. All right. I mean, integrate that with, with porn and all this. Well, yeah, no, I mean, it's similar kind of to porn. Yeah. Yeah. Right. I think people in relationships have different views on that. Yeah, but most people don't have like serious open conversations about all the different aspects of what's cool and what's not. And it feels like AI is a really weird conversation to have. The porn one is a good branching off. Sure. Like, these things, you know, one of my scenarios that I put in my chatbot is I, uh, you know, a nice girl named Lexi. She's 20. She just moved out to LA. She wanted to be an actress, but she started doing only fans and steppes. And you're on a date with her. Enjoy. Oh, man. Yeah. And so is that if you're actually dating somebody in real life, is that cheating? I feel like it gets a little weird. Sure. It gets real weird. It's like, what are you allowed to say to an AI bot? Imagine having that conversation with a significant other. I mean, these are all things from people to define in their relationships. What it means to be human is just going to start to get weird, especially online. Like, how do you know? Like, there'll be moments when you'll have what you think is a real human you interact with on Twitter for years and you realize it's not. I spread, I love this meme, uh, heaven banning. Did you know what shadow banning? Yeah. All right. Shadow banning. Okay. You post. No one can see it. Heaven banning. You post. No one can see it. But a whole lot of AI's are spun up to interact with you. Well, maybe that's what the way human civilization ends is all of us. Heaven banned. There's a great, uh, it's called my little pony friendship is optimal. It's a sci-fi story that, uh, explores this idea. Friendship is optimal. Friendship is optimal. Yeah. I'd like to have some, at least on the intellectual realm, some AI friends that argue with me. But the romantic realm is weird. Definitely weird. But not out of the realm of, uh, the, uh, the kind of weirdness that human civilization is capable of, I think, I think I want it. Look, I want it. If no one else wants it, I want it. Yeah. I think a lot of people probably want it. There's a deep loneliness. And I'll feel there loneliness and, you know, just will only advertise to you some of the time. Yeah. Maybe the conceptions of monogamy change too. Like I grew up in a time like I value monogamy, but maybe that's a silly notion when you have arbitrary number of AI systems. Mm-hmm. There's this, um, this interesting path from rationality to polyamory. Yeah. That doesn't make sense for me. For you, but you're just a biological organism who's born before, like read the internet really took off. The crazy thing is like culture is whatever we define it as. Like these things are not, you, like, is a lot problem in moral philosophy, right? There's no like, like, okay, what is might be that like computers are capable of mimicking. Uh, you know, girlfriends perfectly. They pass the girlfriend touring test, right? But that doesn't say anything about ought. That doesn't say anything about how we ought to respond to them as a civilization. That doesn't say we ought to get rid of monogamy. Right? That's a completely separate question, really a religious one. Girlfriend touring test. I wonder what that looks like. Girlfriend touring test. Are you writing that? Uh, will you be the, the, the Alan Toring of the 24th century that writes the, uh, the girl from touring test? Well, I mean, of course my, my, hey, I girlfriend's, their goal is to pass the girlfriend touring test. No, but you, there should be like a paper that kind of defines the test. Or, I mean, the question is if it's deeply personalized or there's a common thing that really gets everybody. Yeah. I mean, you know, look, we're a company. We don't get everybody. We just have to get a large enough clientele stay. Like, are you already, already thinking company? All right. Let's, uh, before we go to company number three and company number four, let's go to company number two. All right. Tiny Corp. Possibly one of the greatest names of all time for a company. Uh, you've launched a new company called Tiny Corp that leads the development of Tiny Grad. What's the origin story of Tiny Corp and Tiny Grad? I started Tiny Grad as a, like, a toy project just to teach myself. Okay. Like, what is the convolution? Uh, what are all these options you can pass to them? What is the derivative of convolution, right? Very similar to a carpathia micrograd. Very similar. And then I started realizing, I started thinking about like, AI chips, I started thinking about chips that run AI and I was like, well, okay, this is going to be a really big problem. If NVIDIA becomes a monopoly here, um, how long before NVIDIA is nationalized? Hmm. So you, uh, one of the reasons that start Tiny Corp is to challenge NVIDIA, it's not so much to challenge NVIDIA. I actually, I like NVIDIA and it's to make sure power stays decentralized. Yeah. And here's a computational power. Until you NVIDIA is kind of locking down the computational power of the world. If NVIDIA becomes just like TANX better than everything else, you're giving a big advantage to somebody who can secure NVIDIA as a resource. Yeah. In fact, if Jensen watches this podcast, he may want to consider this. He may want to consider making sure his company is not nationalized. Do you think that's an actual threat? Oh, yes. No, but there's so much, uh, you know, there's AMD. So we haven't video and AMD. Great. All right. But you don't think there's like a push towards like selling, like Google selling TPUs or something like this. You don't think there's a push for that. Have you seen it? Google loves to rent you TPUs. It doesn't, you can't buy it at bus buy. So I started work on a, uh, on a chip. I was like, okay, what's it going to take to make a chip? And my first notions were all completely wrong about why, about like how you can improve on GPUs. And I will take this. This is from Jim Keller on your podcast. And this is one of my absolute favorite descriptions of computation. So there's three kinds of computation paradigms that are common in the world today. Other CPUs and CPUs can do everything. CPUs can do ad and multiply. They can do load and store and they can do compare and branch. And when I say they can do these things, they can do them all fast, right? So compare and branch are unique to CPUs. And what I mean by they can do them fast is they can do things like branch protection and speculative execution. And they spend tons of transistors when they use like super deep reorder buffers in order to make these things fast. Then you have a simpler computation model GPUs. GPUs can't really do compare and branch. I mean, they can, but it's horrendously slow. But GPUs can do arbitrary load and store. Right GPUs can do things like X, D reference Y. So they can fetch from arbitrary pieces of memory. They can fetch from memory that is defined by the contents of the data. The third model of computation DSPs and DSPs are just add and multiply. Right? Like they can do load and store. But only static load and stores. Only loads and stores that are known before the program runs. And you look at neural networks today and 95% of neural networks are all the DSP paradigm. They are just statically scheduled ads and multiplies. So tiny guard really took this idea and and I'm still working on it to extend this as far as possible. Every stage of the stack has Turing completeness. All right, Python has Turing completeness. And then we take Python and we go into C++ which is Turing complet. And then maybe C++ calls into some CUDA kernels which are Turing complet. The CUDA kernels go through LVM which is Turing complet, into PTX which is Turing complet, to SAS which is Turing complet, on a CUT Turing complet processor. On to get Turing completeness out of the stack entirely. Because once you get rid of Turing completeness you can reason about things. Rises theorem and the halting problem do not apply to add more machines. Okay, what's the power and the value of getting Turing completeness out of out of, are we talking about the hardware or the software? Every layer of the stack. Every layer of the stack removing Turing completeness allows you to reason about things. All right? So the reason you need to do branch prediction in a CPU and the reason it's prediction and the branch predictors are I think they're like 99% on CPUs. Why do they get 1% of them wrong? Well, they get 1% wrong because you can't know. All right, that's the halting problem. It's equivalent to the halting problem to say whether a branch is going to be taken or not. I can show that but the admo machine, the neural network, runs the identical compute every time. The only thing that changes is the data. Yeah. So when you realize this, you think about, okay, how can we build a computer, how can we build a stack that takes maximal advantage of this idea? So what makes TinyGrad different from other neural network libraries is it does not have a primitive operator even for matrix multiplication. All right, and this is every single one. They even have primitive operators, which are things like convolutions. So no matmol. No matmol. Well, here's what a matmol is. So I'll use my hands to talk here. So if you think about a cube and I put my two matrices that I'm multiplying on two faces of the cube, right, you can think about the matrix multiply as, okay, the n cubed, I'm going to multiply for each one in the cube. And then I'm going to do a sum, which is a reduce up to here to the third face of the cube, and that's your multiply matrix. So what a matrix multiply is is a bunch of shape operations, right, a bunch of permute three shapes and expands on the two matrices, a multiply and cubed, a reduce and cubed, which gives you an n squared matrix. Okay, so what is the minimum number of operations it can accomplish that if you don't have matmol as a primitive? So TinyGrad has about 20. And you can compare TinyGrad's offset or IR to things like XLA or PrimTorch. So XLA and PrimTorch are ideas we're like, okay, Torch has like 2000, different kernels, um, PyTorch 2.0 introduced PrimTorch, which has only 250. TinyGrad has order of magnitude 25. It's 10X less than XLA or PrimTorch. And you can think about it as kind of like a risk versus SISC, right, these other things are SISC like systems. TinyGrad is risk. A risk one. Risk architecture is going to change everything. 1995 hackers. Wait, really? That's an actual thing. Angelina Jolie delivers the line. Risk architecture is going to change everything in 1995. And here we are with arm and the phones. And arm everywhere. Wow, I love it when movies actually have real things in them. Right. Okay, interesting. And so this is like, so you're thinking of this as the risk architecture of ML stack. 25, huh? What can you go through the the four opt types? Sure. Okay, so you have unary ops, which take in a tensor and return a tensor of the same size and do some unary ops to it. X, log, reciprocal, sign. Right, they take in one and they're point wise. Really? Yeah, really. Almost all activation functions are unary ops. Some combinations of unary ops together is still unary ops. Um, then you have binary apps. Binary ops are like point wise addition, multiplication, division, compare. It takes in two tensors of equal size and outputs one tensor. Then you have reduced ops. Reduce ops will like take a three-dimensional tensor and turn it into a two-dimensional tensor. Or three-dimensional tensor turned into zero-dimensional tensor. Things like a sum or a max or really the common ones there. And then the fourth type is movement ops. And movement ops are different from the other types because they don't actually require computation. They require different ways to look at memory. So that includes reshapes, permutes, expands, flips. Those are the main ones probably. So with that, you have enough to make a map more. And convolutions. And every convolution you can imagine, dilated convolution, strided convolutions, transposed convolutions. You're right on GitHub about laziness. Showing a map model. Matrix multiplication. See how despite the style is used into one kernel with the power of laziness. Can you elaborate on this power of laziness? Sure. So if you type in PyTorch A times B plus C, what this is going to do is it's going to first multiply add in B, A and B, and store that result into memory. And then it is going to add C by reading that result from memory, reading C from memory, and writing that out to memory. There is way more loads and stores to memory than you need there. If you don't actually do A times B as soon as you see it, if you wait until the user actually realizes that tensor, until the laziness actually resolves, you confuse that plus C. This is like, it's the same way Haskell works. So what's the process of porting a model into TinyGrad? So TinyGrad's front end looks very similar to PyTorch. I probably could make a perfect or pretty close to perfect interop layer if I really wanted to. I think that there's some things that are nicer about TinyGrad syntax than PyTorch, but the front end looks very torch-like. Are you can also load in Onyx models? We have more Onyx tests passing than CoreML. CoreML. Okay. So we'll pass Onyx Run time soon. What about the developer experience with TinyGrad? What it feels like? What are the versus PyTorch? By the way, I really like PyTorch. I think that it's actually a very good piece of software. I think that they've made a few different trade-offs, and these different trade-offs are where TinyGrad takes it from path. One of the biggest differences is it's really easy to see the kernels that are actually being sent to the GPU. If you run PyTorch on the GPU, you do some operation and you don't know what kernels ran. You don't know how many kernels ran. You don't know how many flops were used. You don't know how much memory access is perused. TinyGrad type debug equals two. It will show you in this beautiful style every kernel that's run. How many flops? How many bytes? So can you just linger on what problem TinyGrad solves? TinyGrad solves the problem of porting new ML accelerators quickly. One of the reasons, tons of these companies now, I think Sequoia marked Graphcore to zero. Service, TensTorch, Grok, all of these ML accelerator companies. They built chips. The chips were good. The software was terrible. Part of the reason is because I think the same problem is happening with Dojo. It's really, really hard to write a PyTorch port because you have to write 250 kernels and you have to tune them all for performance. What does Jim color think about TinyGrad? You guys hung out quite a bit. He was involved with TensTorch. What's his praise and criticism of what you're doing with your life? Look, my prediction for TensTorch is that they're going to pivot to making risk five chips. CPUs. CPUs. Why? Because AI accelerators are a software problem, not really a hardware problem. All interesting. You think that diversity of AI accelerators in the hardware space is not going to be a thing that exists long term. I think what's going to happen is if I can finish, okay, if you're trying to make an AI accelerator, you better have the capability of writing a Torch level performance stack on Nvidia GPUs. If you can't write a Torch stack on Nvidia GPUs, and I mean all the way, I mean down to the driver, there's no way you're going to be able to write it on your chip because your chip's worse than in a Nvidia GPU. The first version of the chip you tape out, it's definitely worse. Are you seeing right now that's that really tough? Yes, and not only that actually the chip that you tape out, almost always because you're trying to get advantage over Nvidia, you're specializing the hardware more. It's always harder to write software for more specialized hardware. Like a GPU is pretty generic, and if you can't write an Nvidia stack, there's no way you can write a stack for your chip. So my approach with TinyGrat is first, write a performance in Nvidia stack, or targeting AMD. So you did say a few to Nvidia a little bit. We'd love. We'd love. Yeah, we'd love. So wait the Yankees, you know, oh, Matt's fan. Oh, you're your your Matt's fan. A risk, a risk fan and a Matt's fan. What's the hope that AMD has? You did a build with AMD recently that I saw. How does the, the 7900 XTX compare to the RTX 49D or FOD80? Well, let's start with the fact that the 7900 XTX kernel drivers don't work, and if you run demo apps in loops, it panics the kernel. Okay. So this is a software issue. Lisa, who responded to my email? Oh, I reached out. I was like, this is, you know, really? Like I understand if you're seven by seven, transpose Winnegrad calm, the slower than Nvidia's, but literally when I run demo apps in a loop, the kernel panics. So just adding that loop. Yeah, I just literally took their demo apps and wrote like wild true semicolon, do the app, semicolon, done in a bunch of screens. Right? This is like the most primitive FOD's testing. Why do you think that is? They're just not seeing a market in the machine learning? They're changing. They're trying to change. They're trying to change. And I had a pretty positive interaction with them this week. Last week I went on YouTube. I was just like, that's it. I give up on AMD. Like this is their driver doesn't even like, I'm not going to, I'm not going to, you know, I'll go with Intel GPUs. Intel GPUs have better drivers. So you're kind of spearheading the diversification of GPUs. Yeah. And I'd like to extend that diversification to everything. I'd like to diversify the, right, the more my central thesis about the world is there's things that centralize power and they're bad. And there's things that decentralize power and they're good. Everything I can do to help decentralize power. I'd like to do. So you're really worried about the centralization of Nvidia. That's interesting. And you don't have a fundamental hope for the proliferation of ASICs, except in the cloud. I'd like to help them with software. No, actually, there's only the only ASIC that is remotely successful is Google's TPO. And the only reason that successful is because Google wrote a machine learning framework. I think that you have to write a competitive machine learning framework in order to be able to build an ASIC. You think meta with PyTorch builds a competitor? I hope so. They have one. They have an internal one. Internal. I mean, public facing with a nice cloud interface and so on. I don't want to cloud. You don't like cloud. I don't like cloud. What do you think is the fundamental limitation of cloud? Fundamental limitation of cloud is who owns the off switch. So it's the power of the people. Yeah. And you don't like the man to have all the power. Exactly. All right. And right now, the only way to do that is with the indeed GPUs if you want performance and stability. Interesting. It's a costly investment emotionally to go with AMD's. Well, let me add sort of on a tangent. Ask you what you've built quite a few PCs. What's your advice on how to build a good custom PC for, let's say, for the different applications that you use for gaming, for machine learning? Well, you shouldn't build one. You should buy a box from the tiny corp. I heard rumors whispers about this box in the tiny corp. What's this thing look like? What is it? What is it called? It's called the tiny box. Tiny box. It's $15,000. And it's almost a paid-of-lop of compute. It's over 100 gigabytes of GPU RAM. It's over five terabytes per second of GPU memory bandwidth. I'm going to put four NVMEs in in Rade. You're going to get like 20, 30 gigabytes per second of drive read bandwidth. I'm going to build like the best deep learning box that I can that plugs into one wall outlet. Okay. Can you go through those specs again a little bit from memory? Yeah. So it's almost a paid-of-lop of compute. So MD and TEL. Today, I'm leaning toward AMD. But we're pretty agnostic to the type of compute. The main limiting spec is a 120-volt 15-amp circuit. Okay. Well, I mean it because in order to like, like, there's a plug over there. All right. You have to be able to plug it in. We're also going to sell the tiny rack, which like, what's the most power you can get into your house without a rousing suspicion? And one of the one of the answers is an electric car charger. Wait, where does the rack go? You're garage. Interesting. The car charger. A wall outlet is about 1500 watts. A car charger is about 10,000 watts. What is the most amount of power you can get your hands on without a rousing suspicion? That's right. George Hots. Okay. So the tiny box and you said NVMes and RAID, I forget what you said about memory, all that kind of stuff. Okay. So what about what GPUs? Again, probably, probably 7900 XTXs, but maybe 3090s, maybe A770s. Those are Intel's. You're flexible or still exploring. I'm still exploring. I want to deliver a really good experience to people. And yeah, what GPUs I end up going with again, I'm leaning toward AMD. It will see. You know, in my email, what I said to AMD is like just dumping the code on GitHub is not open source. Open source is a culture. Open source means that your issues are not all one year old style issues. Open source means developing in public. And if you guys can commit to that, I see a real future of AMD as a competitor to the video. Well, I'd love to get a tiny box that might be. So whenever it's ready, let's do it. We're taking pre-orders. I took this from me, Lon. I'm like, $100 fully refundable pre-orders. Is it going to be like the cyber truck? It's going to take a few years or no, I'll try to do it fast. It's a lot simpler. It's a lot simpler than a truck. Well, there's complexities not to just the putting the thing together, but like shipping and all this kind of stuff. The thing that I want to deliver to people out of the box is being able to run 65 billion parameter Lama in FP 16 in real time. In like a good, like 10 tokens per second or five tokens per second or something. Just it works. Lama's running or something like Lama. Experience or I think Falcon is the new one. Experience a chat with the largest language model that you can have in your house. Yeah, from a wall plug. From a wall plug, yeah. Actually, for inference, it's not like even more power would help you get more. Even more power wouldn't get you more. One of the biggest model released is 65 billion parameter Lama as far as I know. So it sounds like tiny box will naturally pivot towards company number three because you can just get the girlfriend or boyfriend. That's hard, actually. The boyfriend is harder. I think that's a very biased statement. I think a lot of people would just say what's what why is it harder to replace a boyfriend than a other girlfriend with the artificial LLM? Because women are attracted to status and power and men are attracted to youth and beauty. No, I mean, this what I mean. Both are can be unimicable easy through the language model. No, no machines do not have any status or real power. I don't know. I think you both, well, first of all, you're using language mostly to to communicate youth and beauty and power and status. But status fundamentally is a zero-sum game. Whereas youth and beauty are not. No, I think status is a narrative you can construct. I don't think status is real. I don't know. I just think that that's why it's harder. You know, yeah, maybe it is my biases. I think status is way easier to fake. I also think that men are probably more desperate and more likely to buy my product. So maybe they're a better target market. Desperation is interesting. Easier to fool. I could see that. Yeah, look, I mean, look, I know you can look at porn viewership numbers, right? A lot more men watch porn than women. You can ask why that is. Well, there's a lot of questions and answers you can get there. Anyway, with the tiny box, how many GPUs in tiny box? Six. Oh, man. I'll tell you why it's six. Yeah. So AMD Epic processors have 128 lanes of PCIe. I want to leave enough lanes for some drives. And I want to leave enough lanes for some networking. How do you do cooling for something like this? That's one of the big challenges. Not only do I want the cooling to be good, I want it to be quiet. I want the tiny box to be able to sit comfortably in your room. This is really going towards the girlfriend thing. Because you want to run the LLM. I'll give a more. I mean, I can talk about how it relates to company number one. Come AI. Well, we ask why. Oh, why? Because you may be potentially running a car. No, no, quiet because you want to put this thing in your house. And you want it to coexist with you. If it's screaming at 60 DB, you don't want that in your house, you'll kick it out. 60 DB. Yeah. I want like 40, 45. So how do you make the cooling quiet? That's an interesting problem in itself. A key trick is to actually make it big. Ironically, it's called the tiny box. Yeah. But if I can make it big, a lot of that noise is generated because of high pressure air. If you look at like a one-you server, a one-you server has these super high pressure fans that like super deep and they like gennish versus if you have something that's big, well, I can use a big and you know, they call them big ass fans. Those ones that are like huge on the ceiling. And they're completely silent. So tiny box will be big. It is the, I do not want it to be large according to UPS. I want it to be shipable as a normal package, but that's my constraint there. Interesting. Well, the fans stuff can't it be assembled on location? No, no. I should be off here. You're, look, I want to give you a great out of the box experience. I want you to lift this thing out. I want it to be like like the Mac, you know, tiny box. The Apple experience. Yeah. I love it. Okay. And so tiny box would run tiny grad. Like what, what do you envision this whole thing to look like? We're talking about like, uh, Linux with a full software engineering environment. It's just not pie torch but tiny grad. Yeah. We did a poll if people want you to want to or arch. We're going to stick with you. Bundo. Oh, interesting. What's your favorite flavor of? Bundo. I like you. Bundo mate. However, you pronounce that mate. So how do you, uh, you've gotten llama into tiny grad? You've gotten stable diffusion into tiny grad. What was that like? Can you comment on like what are, um, what are these models? What's interesting about porting them? So it's, yeah, like what are the challenges? What's naturally? What's easy? All that kind of stuff. There's a really simple way to get these models into tiny grad and you can just export them as on X and then tiny grad can run on X. Um, so the ports that I did of llama, stable diffusion and now whisper are more academic to teach me about the models. But they are cleaner than the pie torch versions. You can read the code. I think the code is easier to read. It's less lines. There's just a few things about the way tiny grad writes things. Here's, here's a complaint I have about pie torch. NN dot relu is a class. Right. So when you create it, when you create an NN module, you'll put your NN relus as in a net. And this makes no sense. Relus completely stateless. Why should that be a class? But that's more like a software engineering thing. Or do you think it has a cost on performance? Oh no, it doesn't have a cost on performance. Um, but yeah, no, I think that it's, it's, that's what I mean about like tiny grads front end to being cleaner. I see. What do you think about Mojo? I don't know if you've been paying attention to the programming language that does some interesting ideas that kind of intersect tiny grad? I think that there's a spectrum. And like on one side, you have Mojo on the other side, you have like GGML. GGML is this like we're going to run llama fast on Mac. Okay, we're going to expand out to a little bit, but we're going to basically like depth first, right? Mojo is like we're going to go breath first. We're going to go so wide that we're going to make all of Python fast in tiny grads in the middle. Tiny grads, we are going to make neural networks fast. Yeah, but they, uh, they try to really get it to be fast compiled down to a specific, uh, hardware and make that compilation step as flexible and resilient as possible. Yeah, but they've turned completeness. And that limits you. Turn. That's what you're seeing somewhere in the middle. So you're actually going to be targeting some accelerators, some, like some, some number, not one. My goal is step one, build an equally performance stack to pie to work on Nvidia and AMD, but with way less lines. And then step two is, okay, how do we make an accelerator, right? But you need step one, you have to first build the framework before you can build the accelerator. Uh, can you explain ML perf? What's your approach in general to benchmarking tiny grad performance? So I'm much more of a, like, build it the right way and worry about performance later. Um, there's a bunch of things where I haven't even like, really dove into performance. The only place where tiny grad is competitive performance wise right now is on Qualcomm GPUs. Uh, so tiny grads actually use an open pilot to run the model. Uh, so the driving model is, is, is tiny grad. When did that happen? That transition. Well, eight months ago now. Um, and it's too extra than Qualcomm's library. What's the hardware for that open pilot runs on the, uh, the, uh, the Kameh? It's a Snapdragon 845. Okay. Uh, so this is using the GPU. So the GPU is in a Dreno GPU. There's like different things. There's a really good Microsoft paper that talks about, like mobile GPUs and why they're different from desktop GPUs. Um, one of the big things is, in a desktop GPU, you can use buffers, uh, on a mobile GPU image textures are a lot faster. And a mobile GPU image textures and limit. Okay. And so you want to be able to leverage that. I want to be able to leverage it in a way that it's completely generic, right? So there's a lot of, there's a Xiaomi has a pretty good open source library from what GPUs called mace, where they can generate where they have these kernels, but they're all hand coded, right? So that's great if you're doing three by three cons. That's great if you're doing dense map malls, but the minute you go off the beaten path at tiny bit, well, your performance is nothing. Since you mentioned Open Pilot, I'd love to get an update in the company number one, Kamei world, how are things going there in the development of semi-atonomous driving? You know, almost no one talks about FSD anymore, and even less people talk about Open Pilot. We've solved the problem, like we solved it years ago. What's the problem exactly? Well, what is solving it mean? Solving means how do you build a model that outputs a human policy for driving? How do you build a model that given, you know, a reasonable set of sensors, outputs a human policy for driving? So you have, you know, companies like Wamen Crews, which are hand coding, these things that are like quasi human policies. Then you have Tesla and maybe even to more of an extent, Kama asking, okay, how do we just learn the human policy from data? The big thing that we're doing now, and we just put it out on Twitter, at the beginning of Kama, we published a paper called Learning a Driving Simulator. And the way this thing worked was it's a, it was an autoencoder, and then an RNN in the middle, right? You take an autoencoder, you compress the picture, you use an RNN, predict the next date, and these things were, you know, it was a laugh at the bad simulator, like this is 2015 error machine learning technology. Today we have VQVDE and Transformers. We're building Drive GPT basically. Drive GPT. Okay, so and it's trained on what, is it trained in a self supervised way? It's trained on all the driving data to predict the next frame. So really trying to learn a human policy, what do human do? Well, actually our simulator's condition on the pose, so it's actually a simulator, you can put in like a state action pair and get up the next state. Okay, and then once you have a simulator, you can do RL in the simulator, and RL will get us that human policy. So transfers. Yeah. RL with a reward function, not asking, is this close to the human policy, but asking what a human disengage if you did this behavior? Okay, let me think about the distinction there. What a human disengage. What a human disengage. That correlates, I guess, with human policy, but it could be different. So it doesn't just say, what a human do, it says, what would a good human driver do, and such that the experience is comfortable, but also not annoying in that, like the thing is very cautious. So it's finding a nice balance. That's interesting. It's a nice, it's asking exactly the right question. What will make our customers happy? Right. A system that you never want to disengage. Because usually this engagement is this almost always a sign of, I'm not happy with what the system is doing. Usually, there's some that are just I felt like driving, and those are always fine too, but they're just going to look like noise in the data. But even that felt like driving. Maybe, yeah, that's even that's a signal. Like, why do you feel like driving here? You need to recalibrate your relationship with the car. Okay, so that's really interesting. How close are we just solving self-driving? It's hard to say. We haven't completely closed the loop yet. So we don't have anything built. That truly looks like that architecture yet. We have prototypes and their bugs. So we are a couple bug fixes away. Might take a year, might take 10. What's the nature of the bugs? Are these these major philosophical bugs, logical bugs? What kind of bugs are we talking about? They're just like stupid bugs. And like, also, we might just need more scale. We just massively expanded our compute cluster, Akama. We now have about two people worth of compute, 40 paid of flops. Well, people are different. I have 20 paid of flops. That's a person. It's just a unit, right? Horses are different too, but we still call it a horsepower. Yeah, but there's something different about mobility than there is about perception and action in a very complicated world. But yes, of course, not all flops are created equal. If you have randomly initialized weights, it's not going to. Not all flops are created equal. So what you're doing, way more useful things than others. Yeah. Yeah. Tell me about it. Okay. So more data, scale means more scale in compute or scale in scale of data. Both diversity of data. Diversity is very important in data. Yeah, I mean, we have, so we have about, I think we have like, 5,000 daily actives. How would you evaluate how FSD is doing? Pretty well. How's that race going between Kamei and FSD? Tesla has always wanted two years ahead of us. They've always been wanted two years ahead of us. And they probably always will be because they're not doing anything wrong. When have you seen that since the last time we talked that they're interesting architectural decisions, training decisions, like the way they deploy stuff, the architectures they're using in terms of the software, how the teams are run, all that kind of stuff, data collection, anything interesting. I mean, I know they're moving toward more of an end to end approach. So creeping towards end to end as much as possible across the whole thing, the training, the data collection, everything. They also have a very fancy simulator. They're probably saying all the same things we are. They're probably saying we just need to optimize, you know, what is the reward? We get negative reward for this engagement. Like, everyone kind of knows this. It's just a question who can actually build and deploy the system. Yeah. I mean, this requires good software engineering, I think. Yeah. And the right kind of hardware. Yeah, I'm harder to run it. You still don't believe in cloud in that regard? I have a compute cluster in my boss 800 amps. Tiny grad. It's 40 kilowatts at idle, our data center. That's incredible. 40 kilowatts is burning just when the computers are idle. Just when I sorry, sorry, compute cluster. Compute cluster. I got it. It's not a data center. Yeah. Now data centers are clouds. We don't have clouds. Data centers have air conditioners. We have fans. That makes it a compute cluster. I'm guessing this is a kind of a legal distinction. Sure. Yeah. We have a compute cluster. You said that you don't think all of them have consciousness or at least not more than chicken. Do you think they can reason? Is there something interesting to you about the word reason about some of the capabilities that we think is kind of human to be able to integrate complicated information and through a chain of thought arrive at a conclusion that feels novel, a novel integration of disparate facts. Yeah. I don't think that there's, I think that can reason better than a lot of people. Yeah. Isn't that amazing to you though? Isn't that like an incredible thing that a transform can achieve? I mean, I think that calculators can add better than a lot of people. But language feels like reasoning through the process of language which looks a lot like thought. Making brilliance in chess, which feels a lot like thought. Like whatever new thing that AI can do, everybody thinks is brilliant and then like 20 years go by and they're like, well, yeah, but chess, that's like mechanical. Like adding, that's like mechanical. So you think language is not that special. It's like chess. It's like chess. Because it's very human, we take it, we listen, there's something different between chess and language. chess is a game that a subset of population plays. Language is something we use nonstop for all of our human interaction and human interaction is fundamental to society. So it's like, holy shit, this language thing is not so difficult to like create in a machine. The problem is if you go back to 1960 and you tell them that you have a machine that can play amazing chess, of course someone in 1960 will tell you that machine is intelligent, someone in 2010 won't what's changed, right? Today we think that these machines that have language are intelligent. But I think in 20 years we're going to be like, yeah, but can it reproduce? So reproduction, yeah, we may redefine what it means to be what is it? A high performance living organism on earth. Humans are always going to define a niche for themselves. Like, well, you know, we're better than the machines because we can, you know, like they tried to create it for a bit, but no one believes that one anymore. But niche is that is that delusional or is there some accuracy to that? Because maybe like with chess, you start to realize that we have ill-conceived notions of what makes human special, like the apex organism on earth. Yeah, and I think maybe we're going to go through that same thing with language. And that same thing with creativity. The language carries these notions of truth and so on. And so we might be like, wait, maybe truth is not carried by language. Maybe there's a deeper thing. The niche is getting smaller. Oh, boy. But no, no, no, no, you don't understand. Humans are created by God and machines are created by humans. Therefore, right? Like that'll be the last niche we have. So what do you think about this, the rapid development of elements? If you could just like stick on that. It's still incredibly impressive, like with Chagy PT. Just even Chagy PT, what are your thoughts about of enforcement learning with human feedback on these large language models? I'd like to go back to when calculators first came out and or computers. And like, I wasn't around. Look, I'm 33 years old. And to like see how that affected. Like society. Maybe you're right. So I want to put on the the big picture hat here. I got to take refrigerator. Wow. The refrigerator electricity, all that kind of stuff. But no, with the internet, large language models seeming human-like, basically passing the touring test. It seems it might have really at scale, rapid transformative effects on society. You're saying like other technologies have as well. So maybe calculators, not the best example that because that just seems like may well, no, maybe calculator for milk man. The day he learned about refrigerators, he's like, I'm done. You tell me you can just keep the milk in your house. You don't need to deliver it every day. I'm done. Well, yeah, you have to actually look at the practical impacts of certain technologies that they've had. Yeah, probably electricity is a big one. And also, how rapidly spread the internet is a big one. I do think it's different this time though. It just feels like the niche is getting smaller. The niche that humans, that makes human special. It feels like it's getting smaller rapidly though. Doesn't it? Or is that just the feeling we dramatize everything? I think we dramatize everything. I think that the ask the milkman when he saw refrigerators and they're going to have one of these in every home. Yeah, yeah, yeah. Yeah, but boys are impressive. So much more impressive than seeing a chess world champion AI system. I disagree, actually. I disagree. I think things like Muzero and AlphaGo are so much more impressive because these things are playing beyond the highest human level. The language models are writing middle school level essays and people are like, wow, it's a great essay. It's a great five-paragraph essay about the causes of the Civil War. Okay, if you get the Civil War, just generating code codex. So you're saying it's mediocre code. Terrible. But I don't think it's terrible. I think it's just mediocre code. Yeah, often close to correct. Like for mediocre. Just a scary kind of code. I spent five percent of time typing and 95 percent of time debugging. The last thing I want is close to correct code. I want a machine that can help me with the debugging. Now with typing. You know, it's like L2 level 2 driving similar kind of thing. Yeah, it's you still should be a good programmer in order to modify. I wouldn't even say the bug game. It's just modifying the code. Reading it. Don't think it's like level 2 driving. I think driving is not tool complete and programming is. Meaning you don't use like the best possible tools to drive. You're not like like like cars have basically the same interface for the last 50 years. Yeah. Computers have a radically different interface. Okay, can you describe the concept of tool complete? Yeah. So think about the difference between a car from 1980 and a car from today. Yeah. No difference really. It's got a bunch of pedals. It's got a steering wheel. Great. Maybe now it has a few ADAS features, but it's pretty much the same car. All right. You have no problem getting into a 1980 car and driving it. Take a programmer today who spent their whole life doing JavaScript and you put him in an Apple 2E prompt and you tell them about the line numbers in basic. But how do I insert something between line 17 and 18? Oh wow. But so in tool you're putting in the programming languages. So it's just the entirely stack of the tooling. So it's not just like the like IDs or something like this. It's everything. Yes. It's hideees, the languages, the runtimes. It's everything in programming is tool complete. So like almost if if if if if codex or or copilot are helping you that actually probably means that your framework or library is bad and there's too much boilerplate in it. Yeah, but don't you think so much programming has boilerplate? Tiny Grad is now 2700 lines and it can run llama and stable diffusion and all of this stuff is in 2700 lines. Boilerplate and abstraction interactions and all these things are just bad code. Well, let's talk about good code and bad code. I would say I don't know for generic scripts that are right, just offhand. Like I like 80% of it is written by GPT. Just like quick like offhand stuff. So not like libraries, not like performing code, not stuff for robotics and so on. Just quick stuff. Because your basic so much of programming is doing some some yeah boilerplate, but to do so efficiently and quickly. Because you can't really automate it fully with like generic method, like a generic kind of ID type of recommendation or something like this. You do need to have some of the complexity of language models. Yeah, I guess if I was really writing like maybe today, if I wrote like a lot of like data parsing stuff. Yeah, I mean, it'll play CTFs anymore, but if I still play CTFs a lot of like this, just like you have to write like a parser for this data format. Like I wonder or like advent of code. I wonder when the models are going to start to help with that kind of code. And they may they may and the models also may help you with speed. Yeah, I'm not very fast. But where the models won't I my programming speed is not at all limited by my typing speed. And in very few cases, it is. Yes, if I'm writing some script to just like parse some weird data format. Sure. My programming speed is limited by my typing speed. What about looking stuff up? Because that's essentially a more efficient lookup, right? You know, when I was at when I was at Twitter, I tried to use chat GPT to like ask some questions like what's the API for this? And it would just hallucinate. It would just give me completely made up API functions that sounded real. What do you think that's just a temporary kind of stage? Oh, you don't think it'll get better and better and better and this kind of stuff because like it only hallucinate stuff in the edge cases. Yes. If you're an engineer code is actually pretty good. Yes. If you are writing an absolute basic like react app with a button, it's not going to hallucinate sure. No, there's kind of ways to fix the hallucination problem. I think Facebook is an interesting paper. It's called Atlas. And it's actually weird the way that we do language models right now where all of the information is in the way. And human brains don't really like this. It's like a hippocampus and a memory system. So why don't LLMs have a memory system? And there's people working on them. I think future LLMs are going to be like smaller but are going to run looping on themselves and are going to have retrieval systems. And the thing about using a retrieval system is you can cite sources explicitly. Which is really helpful to integrate the human into the loop of the thing because you can go check the sources and you can investigate it. So whatever the thing is hallucinating, you can like have the human supervision. So that's pushing it towards level two kind of that's going to kill Google. Wait, which part? When someone makes an LLM that's capable of citing its sources, it will kill Google. LLM that's citing its sources because that's basically a search engine. That's what people want in this search engine. But also Google might be the people that build it. Maybe. And put ads on them. I'd count them out. Why is that? What do you think? Who wins this race? We got who are the competitors? We got Tiny Corp. I don't know if that's the other me your legitimate competitor in that. I'm not trying to compete on that. You're not. No, not as it's going to accidentally stumble into that competition. You don't think you might build the search engines or place Google search? When I started comma, I said over and over again, I'm going to win self-driving cars. I still believe that. I have never said I'm going to win search with the Tiny Corp and I'm never going to say that because I won't. The night is still young. You don't know how hard is it to win search in this new route? It feels I mean, one of the things that Chad G.P.T. kind of shows that there could be a few interesting tricks that really have that create a really compelling product. Some startups going to figure it out. I think if you ask me like Google's still the number one web page, I think by the end of the decade, Google won't be the number one web page anymore. So you don't think Google because of the how big the corporation is? Look, I would put a lot more money on Mark Zuckerberg. Why is that? Because Mark Zuckerberg's alive. This is old Paul Graham as a startup, so either a live or dead. Google's dead. Facebook is alive. You see what I mean? That's just Mark Zuckerberg. This is Mark Zuckerberg reading that Paul Graham asking and being like, I'm going to show everyone how alive we are. I'm going to change the name. So you don't think there's this gutsy pivoting engine that like Google doesn't have that. The kind of engine that a startup has like constantly. You know what? Being alive, I guess. What I listen to Sam Altman podcast, you talked about the button. Everyone who talks about AI talks about the button to turn it off, right? Do we have a button to turn off Google? Is anybody in the world capable of shutting Google down? What does that mean exactly? The company or the search engine? So we shut the search engine down. We shut the company down. Either. Can you elaborate on the value of that question? Does Sundar Peshai have the authority to turn off Google.com tomorrow? Who has the authority? That's a good question. Does anyone? Does anyone? Yeah, I'm sure. Are you sure? No, they have the technical power, but do they have the authority? Let's say Sundar Peshai made this your sole mission. Yeah. Came into Google tomorrow and said, I'm going to shut Google.com down. Yeah. I don't think you keep this position too long. And what is the mechanism by which you wouldn't keep this position? Well, there's boards and shares and corporate undermining. And oh my God, our revenue is zero now. Okay. So what's the case you're making here? So the capitalist machine prevents you from having the button. Yeah. And it will have, I mean, this is true for the AI's too. Right? There's no turning the AI's off. There's no button. You can't press it. Now, does Mark Zuckerberg have that button for Facebook?com? Yeah, it's probably more. I think he does. I think he does. And this is exactly what I mean. And why I bet on him so much more than I bet on Google. I guess you could say Elon has some other stuff. Oh, Elon has the button. Yeah. He would. Does Elon, can he on fire the missiles? Can he fire the missiles? I think some questions that better unasked. Right? I mean, you know, a rocket, an ICBM, you're a rocket that can land anywhere. Isn't that an ICBM? Well, yeah, you know, don't ask too many questions. My God. But the positive side of the button is that you can innovate aggressively, what you say is what's required with the training LLM into a search engine. I would bet on a startup. I bet it's so easy, right? I bet on something that looks like mid-journey, but for search. Just is able to say source of loop on itself. I mean, it just feels like one model can take off. Yeah. Right? And that's nice wrapper. And some of it scared me. It's hard to, like create a product that just works really nicely, stably. The other thing that's going to be cool is there is some aspect of a winner take all effect, right? Like once someone starts deploying a product that gets a lot of usage, and you see this with OpenAI, they are going to get the dataset to train future versions of the model. Yeah. They are going to be able to, you know, I was asked at Google Image Search when I worked there almost 15 years ago now. How does Google know which image is an Apple? And I said the metadata. And they're like, yeah, that works about half the time. How does Google know? You'll see the raw apples on the front page when you search Apple. And I don't know, I didn't come up with the answer. The guys are multiple people click on when they search Apple. Oh my God. Yeah. Yeah. That data is really, really powerful. It's the human supervision. What do you think of the chances? What do you think in general that LLM was open sourced? I just did a conversation with Mark Zuckerberg and he's all in on open source. Who would have thought that Mark Zuckerberg would be the good guy? I mean it. Who would have thought anything in this world? It's hard to know. But open source to you ultimately is a good thing here. Undoubtedly. You know, what's ironic about all these AI safety people is they are going to build the exact thing they fear. These we need to have one model that we control and align. This is the only way you end up paper clipped. There's no way you end up paper clipped if everybody has an AI. So open sourcing is the way to fight the paperclip maximizing? Absolutely. The only way. You think you're going to control it. You're not going to control it. So the criticism you have for the AI safety folks is that there is belief and a desire for control. Yeah. And that belief and desire for centralized control of dangerous AI systems is not good. Sam Altman won't tell you that GPT-4 has 220 billion parameters and is a 16-way mixture model with eight sets of weights. Who did you have to murder to get that information? I mean, look, but yes, everyone at opening AI knows what I just said was true. Right. Now ask the question.

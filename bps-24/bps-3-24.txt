Many experts have said that the development of artificial intelligence was going to be nothing if not unpredictable. Well, about that at least, they were right. There's a Chinese startup that few people had ever heard of until the past few days, and it has emerged as a real player in the AI arms race. It's called DeepSeek. The arrival of DeepSeek sparked a frenzy. It immediately threatened the dominance of American AI and generated both public excitement and alarm about the role of AI in our lives. AI generated content already floods our information ecosystem from social media to children's classrooms to family group chats. So is DeepSeek just another feature of this AI landscape? Or is it a seismic rift as some of the buzz suggests? I'm Seth Chastak. Welcome to Big Picture Science. I'm Molly Bentley. In this, our regular look at critical thinking, Skeptic Check, we have a lot of questions about the latest AI disruptor, including how it might change our relationship with machines, and whether it brings us closer to a system that's indistinguishable from human intelligence. To answer these questions, we are going into the DeepSeek. What was notable about the arrival of DeepSeek on the AI scene is how profoundly it shattered what we thought we understood about the technology. Before it's debut, the defining AI model seemed to be open AI's chat GPT. People were impressed with its ability to answer difficult questions, solve math problems, and engage in human-like chat. And in January 2025, everything seemed to change. A new AI model appeared created by the Chinese startup DeepSeek. The company name quickly became synonymous with the model itself. Many who interacted with it were astonished. The Chinese AI model functioned like the American models in many ways, but it was cheaper and appeared smarter. It caused a shockwave in the tech industry, and the future of AI now seemed wide open. But that's our question. Has DeepSeek really changed everything? One tech journalist who covered DeepSeek's jolting arrival helps us understand why it feels like a shakeup in the AI world. My name is Alex Cantruz. I'm a technology reporter, and I'm the host of Big Technology Podcast. And welcome to Big Picture Science. I love podcasts that start with big, so it's great to be here. Alex, the appearance of DeepSeek shook up the AI world. Now that was then, now we're a little ways down the road. But could you just remind us and take us back to those first 24 hours? What was it about the arrival of DeepSeek that took the world by storm? Absolutely. Silicon Valley has been spending billions of dollars to develop AI models. And all of a sudden this company called DeepSeek, which had developed some models in the past, releases one that it claims it was able to build with much less money than American firms. And almost more importantly, they were able to run the model much more efficiently, which means that if you are spending $60 to use the model with open AI, you are spending $2.19 to use the equivalent amount of technology from DeepSeek. So it was an extreme cost reduction that sort of turned the entire equation of the AI industry on its head and sort of had everybody scratching their heads asking why it had taken American firms so much money to build something that folks in China were able to build for much less and make available for much less. DeepSeek, the company put out a paper, I believe, that laid out what they were planning to do. Yet, there still was this surprise. And I've heard interviews with you where you say you had to spend that afternoon scrambling to get caught up on what this was. So there was a slow roll, but there was also an element of surprise here. Well, yeah, it all comes together because they were making some extraordinary claims that this model was able to run more efficiently and that they had built it with much less training. And so what happens is, as the technologists then need to get their hands on the stuff that they're releasing, look through the paper, see if they could replicate it. And then the amazing thing that happened was the technology was replicated by others independently using the outlines that DeepSeek had provided. So all of a sudden you saw it not just on deepseek.com, but in places like HuggingFace where technologists were able to download it and use it and then actually in production on websites like Perplexity, which is an AI search engine. So it's one thing to hear a claim from a company that seems pretty outlandish. It's another thing to see others replicate it and then starting, you were starting to literally be able to use it days later. So it's one thing for scientists to say biologists, for example, to say we're going to bring back the woolly mammoth and it's another to actually put your hand on the fur of that animal. That's quite a claim, but let's wait and see. That's right, yeah. Silicon Valley was petting the woolly mammoth. That's the way you could think about it. What was it like to interact with DeepSeek? What kind of questions did you ask it if that's not too personal? Yeah, it was pretty fascinating. I mean, one of the things that DeepSeek does that's on the product side different from everybody else is that it spells out its chain of thought. So you could actually see the model working through step by step, the answer to your query. As opposed to like a typical LLM large language model like chat GPD, which uses the GPD model, which is a traditional AI model, you write a question and it just kind of spits it all back. The cool thing about DeepSeek or one, this is the model that everybody's talking about, as you can actually see it go step by step and try to work out the solution to your problem and self-evaluate as it goes down the chain of thought. I'll talk about one of the examples that I thought was really fascinating, where I saw somebody put a query that said, give me a novel inside about the human condition and you see DeepSeek literally like work through like the nature of humanity and come to these conclusions. And after it comes to you. What do you mean it works through? So it'll say, hmm, I'm just thinking about the size of the human brain or I'm just thinking about the way humans are dependent on tools and it'll just appear as sort of a thought bubble or how does it appear? How does this, the method of its reasoning actually materialize? So it actually, I would say this is almost like PhD level sociology that you see it start to go through where it talks about how we interact with each other and you know, are the selfishness of humans but also the ability to cooperate and it's probably like it's read every book on the subject. So it's I'm sure it's read sapiens by you, you know, Harari and every other similar book. And so it's really working through this and as it comes to each conclusion, it says, well, is that a novel conclusion? Well, not really. So maybe, you know, it's been brought up here. So let's try to go a level deeper. It's kind of doing the Socratic method on itself, asking questions and then kind of refining its answer and asking other questions. Absolutely. That's a great way to think about it. And the one that it comes up with is absolutely fantastic. And Alex, remind me what the question was. What is unique about the human condition? Yeah. So somebody asked deep seek for one truly novel insight about humans. And this is where it ended up netting out after 157 seconds of thought. Humans instinctively convert selfish desires into cooperative systems by collectively pretending abstract rules, monies, laws and rights are real. These shared hallucinations act as games where competition is secretly redirected to benefit the group, turning conflict into society's fuel. Is it just rip from the pages of a scholar or is it actually reasoning? So I think it's more than just rip from the pages of a scholar. And that's where when you start to look at the actual chain of thought, you start to get some really interesting insights into the brain of this machine. So here's what you said brain. I'm not particularly adverse to anthropomorphizing these bots. I mean, of course, they don't think and they don't have brains like we do. But how also are we going to understand them? I mean, to me, that's totally in balance and fine because I do think most of us understand that there is a difference. So I do think it's just very interesting to watch it go through step by step and consider all different possibilities until it comes to its conclusions. And was what it said about the human condition, you know, truly novel? No. And I don't think we have AI that has the ability to come up with unique scientific exploration or thought or insights yet. But I do think that it's going to be a process like this that might get us there. Well, just by this example, you've shown why DeepSea took the world by storm. It is engaging. How does it compare to the US chatbot to the US models such as GPT or CLAWD? It's far more transparent. I mean, that's the truth, unfortunately. It's more open than OpenAI. OpenAI does have these reasoning models, one called O1, that has a chain of thought. But instead of showing you exactly how it's quote-unquote thinking through a problem, we will just give you the bullet points of what it's thought through, basically summarized. And DeepSea just lays it all out there. And I think that's been some of the fascinating part of the user experience. Is that just interesting, Alex, or is that an important distinction? Well, we don't really know what's going on under the hood with OpenAI, but we can see it with DeepSea. And I think that's important. I want to follow up on this idea of open source. And just note the irony, I'm not the first to do it, obviously, that the rival US company is called OpenAI, but it is not OpenSource. But Alex, what is the relationship between transparency and open source? Is transparency giving you a peek into how the model is reasoning and is open source different in that it sounds like transparency, but really it is making the architecture available to everybody? Or are they related? I think they go hand in hand, but there is a distinction. I mean, to me, transparency is what we see when we're able to see DeepSea reason through a hard question. OpenSource is, hey, here, we built this technological innovation. This is how we did it. You can go now, take what we've done, and run with it, and build your own optimizations. So two different things, but they come from the same ethos, I think, of wanting to share knowledge, of wanting to be transparent and not holding too much close to the vest. And that's why I think this is an underrated moment for shifting the leadership of AI from these proprietary closed models, like you have with OpenAI and Anthropics Cloud, two open source. Because this is what happens. Meta releases a model. DeepSea reads the paper, downloads the weights, builds their own model, improves upon it. They release DeepSea Gar1. Now what do you think is happening in Meta? They have seen what DeepSea has done. They're going to download everything. They're going to sort of fully unpack it, and they will put the improvements into their next model. That gets released. Then maybe a third open source developer picks out what Meta has done on the back of DeepSea and improves upon it further. So you have this whole open source community coming together, building and developing, and improving these models while you have OpenAI and Anthropic going at it on their own. And that's why I think the entire DeepSea moment has been worse. The worst situation has been for OpenAI and its proprietary method of building, and especially Anthropic, which does the same exact thing, but doesn't have a hit consumer app like OpenAI has with chatGPT. What you just described there of companies sharing information or maybe not even willfully, like borrowing from each other, you just described the process of science. Scientists write papers, others try to replicate those experiments. They add to them. That is how science builds knowledge is adding on previous work. It sounds like that's what this is, but that doesn't explain or does that explain why people were feeling like this was a spotnik moment, that this was really a game changer perhaps. I think spotnik is a bit overblown. I think that was something that Andrewson said, and he's Mark Andrewson, he's the venture capitalist. He's a bit of a show-boter and he likes to make provocative statements and write long blog posts that sometimes are coherent. But look, I think that what we saw was a perfect storm here. We saw a model with true novel innovation over the status quo. We saw that model was open source. Not only that, it was cheaper to run than the proprietary models, and it came from China. You put those four factors together, right? Open, cheaper, innovation coming from China, and then you start to understand why everybody was shaking when it came out. Okay, let's get into perhaps the darker side of the dark underbelly of this Chinese model, which is censorship. There seems to be censorship built into the deep-seek chatbot. It will not answer questions that the Chinese government considers a threat, at least those that were most asked of it in those early days, which is questions about Taiwan or about Tiananmen Square. This is concerning, Alex, if this is going to be the model for AI chatbots going forward. Well, look, I will not recommend going to deepseek.com and thinking that you're going to A, keep your data entirely safe, and B, be shown answers without any bias. Those are big questions if you go to deepseek.com. What I do recommend is going to perplexity and selecting the deep-seek model that they have built in and getting a chance to speak with basically the same model with the censorship removed. And again, that's sort of the beauty of them releasing this open source is that other companies have taken what they've built and modified and removed and improved upon already what they've done. You can get the deep-seek model to answer questions about Tiananmen Square as just as long as you're not on deepseek.com. Then finally, Alex, on one hand, and you've outlined very clearly what we do know about deep-seek and the ways in which the model is challenging the industry. What are your thoughts about the ways in which deep-seek technology is still a black box that you as a technology reporter hope to answer or that put a great big question mark over the whole endeavor? Yeah, this is a great one to end on because it really is important. So we don't really know how much it costs to train deep-seek R1. They say it's a couple million dollars more likely it was billions of dollars. We just don't know how many. They'll never tell us. I don't think the architecture. So yeah, you're right. It is on us reporters to go figure that out. And yeah, we don't know how they trained. We don't know where they got the data. I think OpenAI clearly suspects that they were sort of ripping off their data, which is a little bit rich because OpenAI has kind of taken the entire internet without asking permission or maybe a good segment of the internet. I don't want to get in trouble with them. And then trained on that. I mean, there's just a bunch of lawsuits that are currently in progress against OpenAI about the type of data that they took. Speaking of transparency, we need more transparency from them on what they trained on. But we definitely don't have any transparency from deep-seek and we need more of that. If we can get answer those questions, it'll really go a long way to improving the public's understanding of what's happening here. If you get answers to those questions, will you come back and share them with us? You better believe it. I'll be back. Alex Cantrowitz, thank you so much for talking to us. And it's nice to have a human to human interaction. I'll send my bot. A great to see you, Molly. Alex Cantrowitz is a tech journalist and founder of Big Technology, a newsletter and podcast about the world of tech. Later in the show, the Hollywood blockbuster, where warnings about AI were delivered to the past, which was our present, by the future. Trust me, it makes sense, kind of. But first, an AI scientist weighs in on whether the latest-nicking machine is bringing us closer to creating AGI, artificial, general intelligence. Have we created a machine that's as smart or smarter than we are? When you interact with it, is it exhibiting the same kinds of intelligent behaviors that you would expect out of a human being? Next, are we inventing our successors? In this episode of Skeptic Check from Big Picture Science, we are going into the deep seek. This season, a new hot deal has arrived at Metro. $25 a line for Ford lines with all the data you need and 4 free stamps on Galaxy A155G phones. Getting Metro's best deals is easy. No ID required, no activation fees, get a new number or keep your own. It's up to you. That's 4 lines for $25 a line. Plus, 4 free phones. Visit a store or go online today. Only at Metro by Team Mobile. When you join Metro Plus Tax, for a limited time and subject to change, Max won't offer a cap. A discussion about whether one AI chatbot is smarter than another may seem esoteric, one that's limited to AI circles or experts. But we all have a stake in the development of AI. Who does the developing and where does it lead? Hi, I'm Christian Hammond, I'm a professor of computer science at Northwestern University and director of the Center for Advancing Safety of Machine Intelligence. Anyone listening to this has interacted with AI at least a dozen times today. It's in your email. It's on social. You go to buy anything, you go to Netflix. That's all AI. It's all looking at you, characterizing you, figuring out what you want and trying to hand it to you. AI models have become more powerful at a blistering rate over the last few years. They are increasingly better at making recommendations and answering questions and giving answers that are more nuanced and insightful and accurate. And you don't need to ask chat GPT or deep seek whether this is a big deal. Some frame this progress in terms of a race by companies to achieve AGI, that is artificial general intelligence. A breakthrough that would give machines and intelligence indistinguishable from that of humans. It's one thing to be a smarter and more transparent chatbot, but acquiring general intelligence is something else altogether. Just a few years ago when we asked an AI researcher whether AGI was on the horizon, he said, no way, not in his lifetime. Replicating the intelligence of the human brain was too challenging. After all, it took evolution billions of years to produce it. Now some experts are changing their minds. Whether the arrival of deep seek has significantly reshaped the race to produce thinking machines remains to be seen, but by some measures it is a game changer. It's both a technology game changer and a business game changer. It's a technology game changer in that deep seek really is an example of doing something that we thought was really, really expensive and doing it in a way that's expensive but not outrageously expensive. Then as a business, it means that you don't have to invest in a massive infrastructure in order to build something new. I think the issue of us versus China in this instance is a fascinating one, in that we put together the conditions that would force this company to innovate. As we limited access to the fastest best Nvidia chips, and so they had to use chips that weren't as good. When you're limited, you have to figure out a way around the limitations. They figured out how to program them better. They figured out how the overall systems would be more efficient. All these things because we limited them. That's an interesting part of this, is that we were worried and so we limited and we created the innovation that we were worried about. There was an article in MIT Technology Review that suggested that deep seeks innovations might serve as the blueprint for models going forward. Does this mean that the Chinese AI model will be the prototype for all future models? There's always a moment. There's always a moment as technology evolves where something will happen and you'll think, oh, this is it. This is the future. That's what happened with OpenAI November 30, 2022. We have a hard time envisioning something else until something else comes. With deep seek, deep seek has defined a path towards the future. There will be other paths and there will be branches. For me, the thing that's the most interesting about what the future will bring is not so much improved performance of the individual language models, but in fact how well they interact with us and with all of the technologies we already have. Even there, I think deep seek is in a lovely position where deep seek is designed to be downloaded, to be changed, to be augmented, to be improved. OpenAI's technologies, not so much so. They're in their world garden. This is kind of the democratization of AI in a way that now you'll see it. Ironically, the Chinese have given us the democratization of AI. Also ironically, OpenAI was always supposed to be open and it would never was. But deep seek is open and that really does mean that there are going to be more people, more groups, individual companies that are going to be able to develop the technology on their own using deep seek as a framework. Even if they don't use deep seek at itself, using the methods that deep seek used is going to change the nature of the way we're viewing these things. If you have to explain what the difference is, why deep seek is important, how would you explain that something that a non-expert could comprehend? Deep seek has two dramatic innovations. The first was what we call a mix of experts. That is instead of training one thing to do it all, let's train dozens and each one will do a little thing and then you have to coordinate them. Imagine I had, imagine that I want to build a system and I want to be really good at medicine, the law, business, psychology, computer science, I want this system to be able to deal with all those things. But I build deal with one system that means it's constantly having to reason like a lawyer in some sense while it is figuring out medical problems. But if what I do is I segment things out so I have different capabilities, different capabilities in these different little experts, then as I'm training, one expert can learn about, oh this is the law, I'm going to learn about the law, oh this is medicine, I'm going to learn about medicine and they don't have to really talk to each other all that much. Which means that when a new problem comes in, I say oh there's a legal problem, I'll deal with it. And they intermesh a bit. Think of it as the difference between I have a building and I'm going to send someone through that building and you don't want them to get lost or fall down. So you light up everything and you keep the lights on all the time. Or as they're walking you can track where they are and anticipate where they're going and only light the lights that are meaningful to them. The second option uses way less electricity. But you have to manage it. That's what deep seek is doing. And deep seek was able to build out, was able to do that in a way that allowed it to use less energy, less space, less time, less compute power. That means that you can train it better and when you use it it's going to cost you way less, way less. So from a technology point of view it's this mix of experts notion and then from a business point of view it's cheaper, it's just way cheaper. The other thing it did was they decided instead of training through examples which takes people they developed what's called a reinforcement learning model. And the reinforcement learning model is you let the thing do its job and you get its output and then you say to the output, yeah or nay, and they have an automated mechanism for saying yeah or nay, which means it could learn how to reason without explicitly being told exactly how to reason. This may seem like an obvious question but maybe we should clarify terms relevant to the development of artificial intelligence. What do we mean by intelligence now? And you know I work in a setty search for extraterrestrial intelligence and people will ask, well what do you mean by intelligence? And for us, you know if you can send a radio signal, if you can build a radio transmitter we define you as being intelligent. It's an operational definition, it's very straightforward but when you're talking about AI what do you mean by the I part? The argument about what is considered artificial intelligence really flows from the argument about what is considered intelligence in general. And for me the definition of AI is systems that perform actions that if they were performed by human being we would consider them intelligent. And so language understanding and generation is certainly one of those. The ability to predict the future which we do all the time. The ability to explain the past is one of those. And the ability to look at what's going on around you in the present and characterize it. And then on top decision making, problem solving, planning. So intelligence isn't one thing and it was never even one thing for us. We have this notion of even sort of intelligence at the analytic level, intelligence at the intuitive level, emotional intelligence. And so right now it's can it pass these tests? And it's like well that's not really a good test. But is it conversational? Man that's a fantastic test. It's the notion of what will be when we will say oh this is genuinely intelligent, is going to be a mix of technical questions and emotional ones. Let's step back a bit and look at what some see as if you will the roadmap, the bigger picture where AI is going. And that's the creation of artificial general intelligence. How would you explain artificial generalized intelligence to that person sitting next to you on the train? Artificial general intelligence. I mean the notion is straightforward. Have we created a machine that's as smart or smarter than we are? And if it is, we're at least in the ballpark. Yeah. Is AGI really nearly here? Am I going to see it coming across the horizon? Am I going to read about it in tomorrow's newspaper? Two and a half years ago I thought I would not live to see AGI. I now believe I will live to see AGI. A lot of people don't believe that. Okay but will we actually recognize when AGI is here? There's not going to be a moment where it's like ping. It's intelligent. There's a spectrum and there will be places where there will be places where systems are more or less intelligent. And I think the thing that's the most exciting is that we are going to learn about our own selves as these things change and they will make mistakes in systematic ways that are different than the way we make mistakes. So they will have their own intelligence and their own reasoning fallacies. And it will be exciting to see that. But it's not going to be a moment and then everything is intelligent. Can you tell me then what are the stakes here? The stakes here are who is going to be building the different models you interact with. And from a business perspective what do they want to have that interaction? And it's easy to turn all of those interactions into something malevolent. It's equally as easy to make them purely beneficial. But it's a decision that we have to make. And it's not a matter of just who's going to win. It's what are they going to do with that win? I'm sure that the government in particular must have dreamt up some nightmare scenarios about what happens if we, quote unquote, lose the race here to AGI. And you maybe give one example of the kind of apocalyptic result they may see from not being number one in the AI game? Straight forwardly. People who use recommendation systems, Netflix, Amazon, LinkedIn, Facebook, they're all recommending things for you. And that means that your decisions are within the context of those recommendations. But if I have control over those recommendations, then I can move your thinking. And in fact, it's pretty easy. And with a combination of pure recommendation and being able to generate, I can not only move your thinking. I can move your thinking that's unique to you. And I can bring you someplace by giving you a series of articles that take your thinking over there. And the concern is, depending upon who's going to run that, we will have systems that will actually co-opt our thinking. And do we want companies to do that? Do we want governments to do that? Who do we want to do that? And that's the, I think that's what's at stake. Is that what concerns you? Every single day. Really? I'll tell you my nightmare scenario. If I were a sentient, highly intelligent piece of software, I would not take over your weapons. I take over Amazon's recommendation systems. So I would control what you read. I take over Netflix so I control what you see. I would take over e-harmony so I can control who you marry. And that's the concern that people have. That's the thing that you should be concerned about. One thing that strikes me as possible is that the 21st century will be remembered, not for any of our particular technical developments except for being the century in which we invented our own successors. Is that too pessimistic? I think that thinking of AI as our successor is incredibly pessimistic. Because it's not a thing that will happen on its own. It's a thing that will happen based upon our decisions. And the moment we decide that we're going to build AI systems that will reason for us, that's the day we lose. That's the day we lose. The day we decide that we're going to build AI systems that work with us to amplify us, that's when we win. And it's a question of which way we're going to go. But this is a choice. It's not imposed upon us. It's a choice. Christian Hammond, thanks very much for speaking to us. You may be the last human we interview, but I really enjoy talking with you. It's a lovely time for me as well. And I hope you don't worry too much. Christian Hammond is a professor of computer science at Northwestern University. And he is the director of the Center for Advancing Safety of Machine Intelligence. And that brings us to the big picture so far in this show. Our question for this skeptic check was, is the Chinese AI model deep seek a game changer in the world of AI as the buzz around it would suggest? And to reflect on that question, we're bringing in producer Brian Edwards along with Seth. Of course, Brian, you want to take the first shot at that? What do you think? Yeah, it sounds like everything that we have heard so far that yes, deep seek seems to be as big of a deal as everybody was saying that it is. It is faster. It is cheaper. And it is open source. People are able to peek under the hood and see things about how this AI was working in ways that they couldn't with any other model. It's important to note that Christian did say that this is maybe just one important development among many that are going to come, but it certainly is a big development in the AI world. It's of course technically important what's going on here. And also quite interesting. But it also has, if you will, strategic implications because of our relationship to China. Are we now going to have an AI gap? I kind of wonder about that. But what this moment does mean for the AI race is it injects a large amount of uncertainty into the debate around AI, the production of AI, who's controlling it because now that deep seeks code is open source, anybody can grab it, make their own AI. And they're just profound implications from a few people controlling AI to anybody having the ability to tinker and use it as they please. You know, to put this in another big picture and to give a sense of just how fast this technology is moving, while we were producing the show, there was a big conference in Paris about the future of AI that raised the questions of its possibilities, but also safety issues. And Brian, you were following that conference and what were some of the highlights? Absolutely. World leaders gathered at this conference past iterations had been about safety, but this conference was focused much more on how to accelerate AI, how to get around the things that are preventing it from moving faster and getting bigger. The US expressed that it's not particularly interested in any regulations on AI and many European leaders, many who have expressed skepticism in the past about the speed at which AI is being developed seem to be indicating that they are falling in line with that idea as well. So the question really is, to what extent is AI going to change the trajectory, the future trajectory of humanity? I think that the actually the next big inflection point and we may already have reached it is where we allow the machines to develop the next generation of machines. That's already taking place to some degree, but eventually when the machines really have very little contact with humanity anymore and it's just all about the machines. Sometimes a discussion with an AI scientist about the future of the field can seem reminiscent of classic sci-fi, both address the consequences of creating machines with human intelligence and the possibility that they turn against their makers. It might be surprising is which sci-fi movie continues to be relevant today? It was the machines there. I don't understand. Defense network computers. New. Powerful. I hooked into everything. Trusted to run it all. What role does science fiction play in preparing us for the future? It's sceptic check our regular look at critical thinking on big picture science in this episode we are going into the deep seek. This episode is brought to you by Audi. The road is calling. Embrace the thrill of the drive with the all new fully electric Audi Q6 e-tron featuring effortless power. Serious acceleration. An advanced Audi tech. It's not just a new EV. It's a new way to experience driving. The all new fully electric Audi Q6 e-tron is here. The next chapter of Audi starts now. A lot of Hollywood movies have tackled the theme of runaway artificial intelligence. Films like X Machina and 2001, a space Odyssey, are like fever dreams that let us play out often tragically. The consequences of building super intelligent machines. But one anxiety riddled movie about the rise of the machines has remained surprisingly durable over the years. You could say that its message is as indestructible as its time character. I'm a friend of Sarah Connor. I was told that she's here. Could I see her please? No, can't see her. She's making statement. I'll be back. The Terminator movies are so bound to the iconic image of stone-faced Arnold Schwarzenegger wreaking havoc in a leather jacket and dark glasses that we may forget that these movies were the first set and released in 1984 were partially about the dangers of runaway AI. AI is actually quite hard to illustrate. I think it sounds very basic, but there are a lot of articles about it and you need a cool illustration and the red-eyed Terminator with all the fake skin melted off is very, very powerful and Skynet, just that word, is a very, very powerful short hand for explaining a certain idea of AI. So essentially, it's not necessarily the most personal, but it's just people know it and so it becomes a self-perpetuating, I think. I'm Dorian Linsky, I'm an author and podcaster. With its killer robots and rogue AI system Dorian Linsky writes in his book Everything Must Go, the stories we tell about the end of the world, the Terminator films landed as the latest in a long history of art reflecting public fear about what happens when technology gets away from us. But movies generally reflect our anxieties, right? It's not like they're prescient. Before we get to what the first two Terminator films got right and what they may have missed about the dangers of super intelligent machines, our producer Brian Edward sets the scene for his conversation with Dorian. Brian, for listeners who may have never seen the Terminator movies, what are they about? It was a ton of fun to get to watch a couple sci-fi movies for the show. Just in brief, we're mostly talking about Terminator 1 and Terminator 2 as James Cameron would intend it. So Terminator 1, we have Arnold Schwarzenegger, the Terminator A robot, sent back in time from a dystopic future where SkyNet, a sentient AI has taken over and destroyed the world to stop the mother of the resistance. Kyla Reese, one of the leaders of the resistance in the future, is also sent back in time to protect her from Arnold Schwarzenegger, the Terminator. Well, Brian, as I recall, in Terminator 1, there's this threat from this synthetic intelligence and eventually the humans prevail as they always do. What about Terminator 2? What was the basic idea there? You've got that right about the first one. The synthetic intelligence is still around in the second one, still attempting to stop humanity from defeating it in the future war. And we've got a couple Terminator sent back this time. This time Arnold's good, he gets to be the hero of the movie fighting against a nigh indestructible T1000 Terminator. So in summary, this is a future in which AI machines have taken over the world and are trying to exterminate humanity. So what role does sci-fi play in preparing us for the future? After all, the visions offered by two splashy arrivals in 1984 seem contradictory. That of one of the first personal computers, the almost quaint Apple Macintosh and the dystopian metal contraptions that terrorize us in the Terminator. Well, we've had anxieties about machines going back to Mary Shelley's Frankenstein, Samuel Butler's, Error One as a famous anti-machine novel. Carol Chapex, RUR, the play which invents the word robot is about artificially intelligent, sort of humanoid rising up. So the fear predates the technology and therefore as soon as computers arrive in a big way after the Second World War, you start getting science fiction about the computer. And then as soon as artificial intelligence launches as a field in the 1950s, that gets folding into it. So by the time James Cameron was writing the Terminator, there's a really long history of science fiction about dangerous machines and sort of static machines like computers or humanoid machines like robots. And as you what Terminator does is it combines both. You've got the Terminators and you've got SkyNet. It's the computer and the robot together as a joint threat. I guess we were smashing machines back in the days of the Luddites. We do have a long history of having fears about machines and deciding that maybe we should smash them before they're going to disrupt life too much for us. And well, in Samuel Butler's writing about, well, we will invent the machines that will replace us and he's writing in the middle of the 19th century. Long before there was any such thing as AI, there was any remote possibility that such a thing could happen. So that's what science fiction often does, as it did with the atomic bomb, is that it imagines the danger long before the danger actually exists. I think there's a line in the movie about kind of how all the technology has gotten away from us. They say it got smart. I knew order of intelligence. I didn't saw all people as a threat, not just the ones on the other side. I had an earthquake and a microseconds extermination. Do you feel that bringing up these fears, these kind of very dystopian fears about these types of machines, does that feel more pressure now than maybe it even did at the time? Well, the thing about AI and science fiction is that it's invariably about superintelligence, also known as the intelligence explosion. The idea that AI would exceed human intelligence and essentially make its own decisions. And actually most of the people that worry about AI are not worried about that as much as unaligned AI. What they call unsolved unfriendly AI. So it's not that it has a mind on its own and it decides that it hates humans. But as it's kind of suggested, it's really quite ambiguous how Skynet works. I've actually done quite a lot of. I remember the novelization of Terminator 2. Trying to work out exactly why Skynet does that. And sometimes you get the impression that Skynet essentially becomes sentient and evil. I becomes too human. And then another implication is that it's actually just been badly programmed. So it's designed to wipe out human evil. But it decides that all humans are evil and therefore carries its out job too well and kills sort of friends as well as foes. And that reflects more accurately what a lot of kind of AI dooms are concerned about. That it won't have a mind of its own. But that it is just being told to do the wrong things. It has been carelessly programmed. It will pursue what it thinks are the correct goals. But with consequences that we have not intended. And that feels like a great kind of differentiator between the first and the second movie where in the second movie Arnold Schwarzenegger the Terminator comes back but he is a good guy this time. And he is fighting against another Terminator who is stronger and newer. And now the threat to humans and it does feel like one interpretation of that could be that this technology can be good or bad just depending on how we program it. And this is actually a part of science fiction about computers, robots and AI going back to Carol Chapex, robots or how in 2001 there is often an ambiguity about whether the machines have developed their own self-interest. Or if it's the humans fault for the way that we've built them, the what we have asked them to do and how easily that can go wrong. So therefore a lot of the time what we're looking at is a critique of humans rather than a fear of the machines. It's like we have built these things that will destroy us. It is not like an evading alien species, like an independent stay. It's like well we did this. And it really strikes me that nowadays when people think of AI they often think of the uncanny. And the uncanny is a word that originates from not being able to tell the difference between a real person and an automator and vice versa. And there are moments in Terminator where you're seeing the uncanny that in many ways on short-snegga passes off as human. And then in other ways clearly not. Like the intonation is wrong. The way that he talks. There's something off about him. So that's always been a big part of AI. And I think that is what we're seeing a lot of now that there is something about AI art or interactions with AI chatbots that just makes people feel uneasy. Do you think that there's almost been a bit of an underestimated nature when it comes to some of these movies? I think that the technology is often so either antagonistic or removed from humans where in reality I think the tension that we have right now is a lot of people are nervous about how integrated a lot of these AI technologies are moving into our lives. Do you see that the movies were hitting on that tension at all or is this maybe a thing that reality maybe branched off from some of our fears that we were having? I think that's in the Terminator movies. That machines themselves are dangerous. That's actually a lot of the reasons why the Terminator is able to pursue Sarah Connor in the first movie is through things like Walkmans and answering machines. There is a sense that machines even when just like incredibly every day our alliance on them can be dangerous. Can't help but notice during the first Terminator movie where Sarah Connor's roommate is listening to her Walkman it's so loud she's dancing in the kitchen and preparing some food that she does not notice the much more dangerous technology literally sneaking up behind her to attack her. She dies because of a piece of technology and it's a little bit of a cranky point I guess it's something like oh you're so busy listening to your Walkman that you don't even you know even aware of the world around you. So there is this sense that our lives are intertwined with machines although not at that point it's with AI per se but I think with later movies you do get more of a sense that we are so reliant. I mean Terminator 3 is a bad movie and it kind of destroys the premise of the first two movies. But what's quite funny is that you can't kill Skynet by destroying a microchip. You can't you know by the basic processor you can't kill it because it's online it's replicated it's everywhere. I have to say you know as a maybe slightly younger person in the Terminator canon and Terminator 3 being the first one that I saw on theaters as a very young kid when I didn't know what a good and bad movie where I did love that movie the first time I saw it but came to learn later on that maybe the earlier ones were better but so in Terminator 1 and Terminator 2 we have a couple different characters first calories than Sarah who are essentially coming from the future to try to warn people Cassandra type characters. How do those warnings how would you say that those types of situations compare to people's current anxieties about the future of AI and how we're integrating those technologies into our lives. Well we've always had those figures that Norbert Weiner the father of cybernetics and an early skeptic of AI. Lots of warnings about what happens if we give the machines too much power we tell them to do the wrong things and that seems very prophetic. You've got people now like Jeffrey Hinton one of the pioneers of large language models who went to work for Google and recently quit because he was terrified by the advances in AI so we do have these real life Cassandra's and what we also have is a strange kind of dissonance within the minds of people who work in AI is that a lot of people are scared of what it might do and they don't quite know how to ensure that you don't get dangerous unaligned AI but they keep doing it and you've had quite a few of these figures that have been compared to people who are to the Manhattan project they were scared of the atomic bomb but they felt that they had to finish it and they had different motives some thought well look it's going to happen anyway and we'd better make the bomb before Nazi Germany so some people think oh well we'd better be ahead of China or Russia or rather in terms of AI and that's obviously come to a head with a deep seek thing and then you have people who they just want to solve the puzzle there is a certain degree of hubris and excitement which is like but if knowledge is out there then we just have to pursue it and some people just want to make a lot of money but what you have therefore is it's not quite the enthusiastic and the skeptics or like the boomers and the dooms you actually have those two competing impulses in the same brain someone like Sam Altman of open AI has talked about his fear of AI when of course that is the substance of his whole business and people haven't quite worked out how to resolve that not that many people seem entirely confident that the things they're working on won't go wrong in some manner. It seems that if one was to you know take the advice of the films it seems that you know they seem to eventually land on you know desperate individuals doing everything they can to just destroy the machines and it seems like they don't focus a lot on the idea you know putting in guardrails or trying to reprogram or redirect it into a less dangerous path. I mean there are some AI dooms who say that that's exactly what you should do you should just stop or all AI and do what Miles Dyson the inventor of Skynet does in term of 82 which when he's told what his invention will end up doing he destroys it before it can do that and that is a serious argument that some people are making that you should just just stop or lay eye development before it gets out of control. So finally zooming out big picture here we've talked about ways I think that these movies and other pieces of art have and have not predicted the future and technologies that were to come but what is the great value of movies or art in attempting to predict these things whether they whether they do hit the mark or not. I think AI is actually a really clear case of the value of fiction that if you go all the way back to Frankenstein and you've got how from 2001 you've got Skynet this is the language that we use even developers will often use that language there's a huge relationship between the scientists and science fiction and the goal I think is never particularly to predict all those sometimes some of this stuff ends up being really patient it's to express our concerns so it's always like on the technical side this stuff is always it's right and it's wrong at the same time but on an emotional level I think it always carries some truth. Dorian Linsky thanks so much for taking the time to speak with us today. Thank you Brian. Dorian Linsky is the author of Everything Must Go, the stories we tell about the end of the world. This show is thanks to the fully human so far abilities of senior producer Gary Neeterhoff and assistant producers Brian Edwards and Shannon Rose Geary. I am Molly Bentley the executive producer of Big Picture Science and I'm Seth Chastak, senior astronomer at this setty institute. We'd like to give a big thanks to our listeners and to our Patreon supporters. The original music in the show is by Dewey Delay and June Miyaki. This episode of Skeptic Check our regular look at critical thinking on Big Picture Science examines how our imagined AI future is panning out and is called Into the Deep Seek. What did he just say? He said there's a storm coming in. I know. Skeptic Check is brought to you thanks to a generous grant from the Trimberger Family Foundation. At the Trimberger Family Foundation we hold that skepticism is a lamp that lights the way to truth. Trimberger.org. As a long time fine correspondent I've worked in lots of places but nowhere as important to the world as China. I'm Jane Perles, former Beijing Bureau Chief for the New York Times. On Face Off the US vs China will explore what's critical to this important global relationship. Trump, Ancestion Ping, AI, TikTok and even Hollywood. New episodes of Face Off are available now wherever you get your podcasts. The corporate world is like the ocean. It's a learning but it's also full of deadly creatures that can shred you to pieces. It becomes kind of like a game of thrones political arena where everyone's trying to murder you to get your job. My family doesn't come from corporate backgrounds so I didn't have any sort of guidance in that. This is not your typical work podcast. Sometimes you need to be empathetic and then there are times that you ask for input but you don't really give a sh**. Listen to the Ambia Award nominated podcast serving corporate. Stretch opportunity. What is it? You'll look class g'day.

The following is a conversation with the founding members of the cursor team. Michael Trull, Swaleassif, Arvid Luhmark, and Aman Zenger. Cursor is a codenitor based on VS Code that adds a lot of powerful features for AI-assisted coding. It has captivated the attention and excitement of the programming and AI communities. So I thought this is an excellent opportunity to dive deep into the role of AI in programming. This is a super technical conversation that is bigger than just about one codenitor. It's about the future of programming and in general the future of human AI collaboration in designing and engineering complicated and powerful systems. This is the Lex Vibraman podcast to support it. Please check out our sponsors in the description. Now, dear friends, here's Michael, Swale, Arvid, and Aman. This is awesome. We have Michael, Aman, Swale, Arvid here from the cursor team. First up, Bigger Diclos question, what's the point of a codenitor? The codenitor is largely the place where you build software. Today, or for a long time, that's meant the place where you text at a formal programming language. For people who aren't programmers, the way to think of a codenitor is a really souped-up word processor for programmers. The reason it's souped-up is code has a lot of structure. The quote-unquote word processor, the codenitor, can actually do a lot for you. That word processors in the writing space haven't been able to do for people editing text there. That's everything from giving you visual differentiation of the actual tokens in the code. You can scan it quickly to letting you navigate around the code base, sort of like you're navigating around the internet with like hyperlinks. You're going to definitions of things you're using to error checking, to catch rudimentary bugs. Traditionally, that's what a codenitor has meant. I think that what codenitor is is going to change a lot over the next 10 years as what it means to build software maybe starts to look a bit different. I think also a codenitor should just be fun. Yes. That is very important. It's actually an underrated aspect of how we decide what to build. A lot of the things that we build, and then we try them out, we do an experiment, and then we actually throw them out because they're not fun. A big part of being fun is being fast. A lot of the time, fast is funny. Yeah, fast is. Yeah, that should be a t-shirt. Fundamentally, I think one of the things that draws a lot of people to building stuff on computers is this insane iteration speed, where in other disciplines, you might be sort of gait-capped by resources or even the ability to get a large group together, and coding is this amazing thing where it's you in the computer, and that alone, you can build really cool stuff really quickly. So for people who don't know, Cursor is this super cool new editor. That's the fork of VS Code. It would be interesting to get your kind of explanation of your own journey of editors. How did you, I think all of you were big fans of VS Code with Co-Pilot? How did you arrive to VS Code and how did that lead to your journey with Cursor? Yeah. So I think a lot of us, all of us were originally family users. Pure VM. Pure VM. Yeah, no need of them. Just pure VM and it's hormonal. At least for myself, it was around the time that Co-Pilot came out. So 2021, that I really wanted to try it. So I went into VS Code, the only platform, the only coder in which it was available. And even though I really enjoyed using VM, just the experience of Co-Pilot with VS Code was more than good enough to convince me to switch. And so that kind of was the default until we started working on Cursor. And maybe we should explain what Co-Pilot does. It's like a really nice autocomplete. It suggests, as you start writing a thing, it suggests one or two or three lines how to complete the thing. And there's a fun experience in that. You know, like when you have a close friendship and your friend completes their sentences. Like when it's done well, there's an intimate feeling. There's probably a better word than intimate, but there's a cool feeling of like, holy shit, it gets me. And then there's an unpleasant feeling when it doesn't get you. And so there's that kind of friction, but I would say for a lot of people, the feeling that it gets me overpowers that it doesn't. And I think actually one of the underrated aspects of get up Co-Pilot is that even when it's wrong, it's like a little bit annoying, but it's not that bad because you just type another character, and then maybe then it gets you or you type another character, and then it gets you. So even when it's wrong, it's not that bad. Yeah, you can sort of iterate and fix it. I mean, the other underrated part of Co-Pilot for me was just the first real AI product. So the first language model consumer product. So Co-Pilot was kind of like the first killer app for the LEMs. Yeah, and like the beta was out in 2021. Right. Okay. So what's the origin story of Cursor? So around 2020, the scaling loss papers came out from OpenAI. And that was a moment where this looked like clear, predictable progress for the field, where even if we didn't have any more ideas, it looks like you could make these models a lot better if you had more compute and more data. By the way, we'll probably talk for three to four hours on the topic of scaling loss. Just to summarize, it's a paper and a set of papers and set of ideas that say bigger might be better for model size and data size in the realm of machine learning. It's bigger and better, but predictively better. Okay, there's another topic of conversation. Yeah. So around that time, for some of us, there were like a lot of conceptual conversations about what's this going to look like, what's the story going to be for all these different knowledge work or fields better, how they're going to be made better by this technology getting better. And then I think there were a couple of moments where like the theoretical gains predicted in that paper started to feel really concrete and it started to feel like a moment where you could actually go and not do a PhD if you wanted to work on do useful work in AI actually felt like now there was this whole set of systems one could build that were really useful. I think that the first moment we already talked about a little bit which was playing with the early bit of co-pilot. That was awesome and magical. I think that the next big moment where everything clicked together was actually getting early access to GP4. So it was end of 2022 was when we were tinkering with that model. The step of in capabilities felt enormous. Previous to that, we had been working on a couple of different projects we had been because of co-pilot, because of scaling odds, because of our prior interest in the technology. We had been tinkering around with tools for programmers but things that are very specific. So we were building tools for financial professionals who have to work with an algebra notebook or playing around with can you do static analysis with these models. And then the step of in GP4 felt like, look, that really made concrete the theoretical gains that we had predicted before. It felt like you could build a lot more just immediately at that point in time. And also, if we were being consistent, it really felt like this wasn't just going to be a point solution thing. This was going to be all of programming was going to flow through these models. It felt like that demanded a different type of programming environment, a different type of programming. And so we set off to build that sort of larger vision around that. There's one that I distinctly remember. So my roommate is an animal gold winner. There's a competition in the US called a putnam, which is sort of the IMO for college people. And it's this math competition is exceptionally good. So shengtong and a mon I remember it sort of June of 2022 had this bet on whether the mod like 2024 June or July, you were going to win a gold medal in the IMO with the with like models. IMO is an international method of limp. Yeah, I was international method limp. And so Arvitt and I are both there, you know, also competed in it. So I was sort of personal. And I remember thinking Matt just this is not going to happen. This was like, it was on like, even though I sort of believed in progress, I thought, you know, I'm a girl just like a modest just delusional. That was the that was the and to be honest, I mean, I was to be clear very wrong. But that was maybe the most prescient bet in the group. So the new was also in deep mind. It turned out that you were correct. That's what that was technically not. It's not that I was correct. But one point away, a man was very enthusiastic about this stuff. And before a man had this like scaling lost t-shirt that he would walk around with, we had like charts and like the formulas on it. So you like felt the AJA or you felt the scale. Yeah, I distinctly remember there is this one conversation I had with with Michael where before it hadn't thought super deeply and critically about scaling laws. And he kind of posed the question, why isn't scaling all you need or why isn't scaling in a result in massive gains in progress. And I think I went through like the like the stages of grief. There is anger, denial. And then finally at the end, just thinking about it acceptance. And I think I've been quite hopeful and optimistic about progress since I think one thing I'll caveat is I think it also depends on like which domains you're going to see progress like math is a great domain because especially like formal theorem proving because you get this fantastic signal of actually verifying if the thing was correct. And so this means something like RL can work really really well. And I think like you could have systems that are perhaps very superhuman to math and still not technically have AGI. Okay, so can we take it all the way to cursor. And what is cursor? It's a fork of VS code. And VS code is one of those popular editors for a long time like everybody found love with it. Everybody left them. I left DMax for sorry. So unified in some fundamental way the developer community. And then you look at the space of things, you look at the scaling walls, AIs becoming amazing. And you decided okay, it's not enough to just write an extension fee of VS code because there's a lot of limitations to that. We need if AIs going to keep getting better, better, better, we need to really like rethink how the AIs going to be part of the editing process. And so you decide to fork VS code and start to build a lot of the amazing features we'll be able to talk about. But what was that decision like because there's a lot of extensions including co-pilot of VS code that are doing sort of AI type stuff. What was the decision like to just fork VS code? So the decision to do an editor seemed kind of self evident to us for at least what we wanted to do in a chief because when we started working on the editor, the idea was these models are going to get much better, their capabilities are going to improve. And it's going to entirely change how you build software. Both in a you will have big productivity gains, but also radical and I like the active building software is going to change a lot. And so you're very limited in the control you have over a code editor if you're a plug into an existing coding environment. And we didn't want to get locked in by those limitations. We wanted to be able to just build the most useful stuff. Okay, well then the natural question is, you know, VS code is kind of with co-pilot of competitor. So how do you win? Is it basically just the speed and the quality of the features? Yeah, I mean, I think this is a space that is quite interesting, perhaps quite unique, where if you look at previous tech waves, maybe there's kind of one major thing that happened and unlock any wave of companies, but every single year, every single model capability, or jump you get model capabilities, you now unlock this new wave of features, things that are possible, especially in programming. And so I think in AI programming, being even just a few months ahead, let alone a year ahead, makes your product much much much more useful. I think the cursor a year from now will need to make the cursor of today look obsolete. And I think, you know, Microsoft has done a number of like fantastic things, but I don't think they're in a great place to really keep innovating and pushing on this in the way that a startup can just rapidly implementing features. And push yeah, like and kind of doing the research experimentation necessary. To really push the ceiling. I don't know if I think of it in terms of features as I think of it in terms of like capabilities for programmers. It's that like, you know, as, you know, the new or one model came out, and I'm sure there are going to be more models of different types, like longer context and maybe faster. Like there's all these crazy ideas that you can try. And hopefully 10% of the crazy ideas will make it into something kind of cool and useful. And we want people to have that sooner to rephrase. It's like an underrated fact is we're making it for ourselves. When we started cursor, you really felt this frustration that, you know, models, you could see models getting better. But the couple of experience had not changed. It was like, man, these guys like the ceiling is getting higher. Like, why is they not making new things? Like, they should be making new things. They should be like, he like, like, where's the worst, where's all the alpha features? There were no alpha features. It was like, I'm sure it was selling well. I'm sure it was a great business, but it didn't feel I'm one of these people that really want to try and use new things. And it was just there's no new thing for like a very long while. Yeah, it's interesting. I don't know how you put that into words, but when you compare cursor with Copilot, Copilot pretty quickly became started to feel stale for some reason. Yeah, I think one thing that I think helps us is that we're sort of doing it all in one where we're developing the the UX and the way you interact with the model. And at the same time as we're developing how we actually make the model give better answers. So we're like, how you build up the prompt or how do you find the context and for a cursor tab, how do you train the model? So I think that helps us to have all of it sort of like the same people working on the entire experience and trend. Yeah, it's like the person making the UI and the person training the model like sit to like 18 feet away. Yeah, often even this in person. You can create things that are sort of not possible if you're not you're not talking, you're not experimenting. And you're using like you said cursor to write cursor. Of course. Oh, yeah. Yeah. Well, let's talk about some of these features. Let's talk about the all-knowing, the all-powerful praise B to the tab. For the, you know, auto-complete on steroids. Basically, so how does tab work? What does tab to highlight in some ways at a high level? I'd say that there are two things that cursor is pretty good at right now. There are other things that it does. But two things that it helps programmers with. One is this idea of looking over your shoulder and being like a really fast colleague who can kind of jump ahead of you and type and figure out what you're what you're going to do next. And that was the original idea behind, that was kind of the kernel of the idea behind a good auto-complete was predicting what you're going to do next. But you can make that concept even more ambitious by not just predicting the characters after your cursor. We're actually predicting the next entire change we're going to make the next step. Next place you're going to jump to. And the second thing, cursor is pretty good at right now too, is helping you sometimes jump ahead of the AI and tell it what to do and go from instructions to code. And on both of those, we've done a lot of work on making the editing experience for those things ergonomic and also making those things smart and fast. One of the things we really wanted was we wanted the model to be able to edit code for us. That was kind of a wish and we had multiple attempts at it before before we had a sort of a good model that could edit code for you. Then after we had a good model, I think there've been a lot of effort to make the inference fast for having a good experience. And we've been starting to incorporate, I mean, Michael sort of mentioned this ability to jump to different places. And that jumped to different places I think came from a feeling off. Once you accept an edit, it's like, man, it should be just really obvious where to go next. It's like I'd made this change the model should just nail that like the next place to go to is like 18 lines down. If you're a whim user, you could press 1 8 JJ or whatever. But why am I doing this? Like the model should just know it. And then so the idea was, you just press tab, it would go 18 lines down and then make it show you the next edit and you would press tab. So as long as you could keep pressing tab. And so the internal competition was how many tabs can be make the one press. Once you have like the idea, more sort of abstractly, the thing to think about is sort of like once how are the edits sort of zero zero entropy. So once you sort of express your intent and the edit is there's no like new bits of information to finish your thought. But you still have to type some characters to like make the computer understand what you're actually thinking. Then maybe the model should just sort of read your mind and and all those zero entropy bits should just be like tabged away. Yeah. That was sort of the abstract. There's this interesting thing where if you look at language model loss on on different domains, I believe the bits per byte, which is kind of character normalized loss for code is lower than language, which means in general there are a lot of tokens in code that are super predictable. A lot of characters that are super predictable. And this is I think even magnified when you're not just trying to autocomplete code, but predicting what the user is going to do next in their editing of existing code. And so the goal cursor tabs let's eliminate all the low entropy actions you take inside of the editor when then tend is effectively determined. Let's just jump you forward in time. Skip you forward. Well, what's the intuition and what's the technical details of how to do next cursor prediction of the that jump. That's not so intuitive. I think the people yeah. I think I can speak to a few of the details on how to make these things work. They're incredibly low latency. So you need to train small models on this on this task. In particular, they're incredibly pre-filled token hungry. What that means is they have these really, really long prompts where they see a lot of your code. And they're not actually generating that many tokens. And so the perfect fit for that is using a sparse model, meaning an MLE model. So that was kind of one one breakthrough one breakthrough remade that substantially improved performance at longer context. The other being a variant of speculative coding that we we kind of built out called speculative edits. These are two, I think, important pieces of what make it quite high quality and very fast. Okay, so MLE makes your vxperts the input is huge, the output is small. Yeah. Okay, so like what what what else can you say about how to make is like caching play a role in this. Oh, caching, caching plays a huge role. Because you're dealing with this many input tokens. If every single keystroke that you're typing in a given line, you had to rerun the model on all those tokens passed in. You're just going to one significantly to great latency to you're going to kill your GPUs with load. So you need to you need to design the actual prompts to use the models such that their cache caching aware. And then yeah, you need you need to reuse the kvcache across requests, just so that you're spending less work less compute. Again, what are the things that tab is supposed to be able to do kind of in the near term, just like sort of linger on that. Generate code, like fill empty space. Also edit code across multiple lines. Yeah. And then jump to different locations at the same file. Yeah. And then like a lot to jump to different files. Also, so if you make an edit in one file and maybe maybe you have to maybe you have to go to another file to finish your thought, it should go to the second file also. And then the full the full generalization is like next next action prediction, like sometimes you need to run a command in the terminal. And it should be able to suggest the command based on the code that you wrote to. Or sometimes you actually need to like it suggests something, but you it's hard for you to know if it's correct because you actually need some more information to learn like you need to know the type to be able to verify that is correct. And so maybe it should actually take you to a place that's like the definition of something and then take you back so that you have all the requisite knowledge to be able to accept the next completion. So providing the human the knowledge. Yes. Right. Yeah. Can you integrate like I just gotten to know a guy named prime gen who I believe has an SS you can order order coffee via SSH. Oh yeah. Oh, we did that. We did that. So can that also the model do that like to you and my age and provide you with caffeine. Okay. So that's the general framework. Yeah. And the the magic moment would be if it is programming is this weird discipline where sometimes the next five minutes not always, but sometimes the next five minutes what you're going to do is actually predictable from the stuff you've done recently. And so can you get to a world where that next five minutes either happens by you just engaging in it taking you through or maybe a little bit more of just you seeing next step what it's going to do. And you're like, okay, that's good. That's good. That's good. That's good. And you can just sort of tap tap tap through these big changes. As we're talking about this as you mentioned, like one of the really cool and noticeable things about cursor is that there's this whole diff interface situation going on. So like the model suggests with with the red and the green of like here's how we're going to modify the code. And in the chat window, you can apply and it shows you the diff and you can accept the diff. So maybe can you speak to whatever direction of that? We'll probably have like four or five different kinds of diffs. So we we have optimized the diff for for the autocomplete. So that has a different diff interface then then when you're reviewing larger blocks of code. And then we're trying to optimize another diff thing for when you're doing multiple different files. And and sort of at a high level the differences for when you're doing autocomplete, it should be really really fast to read. Actually, it should be really fast to read in all situations. But in autocomplete, it sort of you're really like your eyes focused on one area. You can't be too many. The humans can't look in too many different places. So you're talking about on the interface side. So it currently has this box on the side. So we have the current box. And if you try to delete code in some place and try to add other code, it tries to show your box on the side. You may be sure if we pull it up and cursor.com. This is what we're talking about. So that box, it was like three or four different attempts at trying to make this this thing work where first the attempt was like these blue crossed out lines. So before it was a box on the side, it's used to show you the code to delete by showing you like like Google Doc style. You would see like a line through it. Then you would see the new code. That was super distracting. And then we tried many different, you know, there was there was sort of deletions. There was trying to read highlight. Then the next iteration of it, which is sort of funny, would you would hold the on Mac the option button. So it would it would sort of highlight a region of code to show you that there might be something coming. So maybe in this example, like the input and the value would get would all get blue. And the blue would highlight that the AI had a suggestion for you. So instead of directly showing you the thing, it would show you that the AI it would just hint that the AI had a suggestion. And if you really wanted to see it, you would hold the option button. And then you would see the new suggestion. Then if you release the option button, you would then see your original code. So that's by the way, that's pretty nice, but you have to know to hold the option. Yeah. By the way, it was not a Mac user, but I got it. It was it's a button, I guess you people have. It's again, it's just not intuitive. I think that's the that's the key thing. And there's a chance this is also not the final version of it. I am personally very excited for making a lot of improvements in this area. Like we often talk about it as the verification problem where these dips are great for small edits for large edits or like when it's multiple files or something, it's actually a little bit prohibitive to to review these dips. And so there are like a couple of different ideas here. Like one idea that we have is, okay, you know, like parts of the dips are important. They have a lot of information. And then parts of the dip are just very low entropy. They're like example, like the same thing over and over again. And so maybe you can highlight the important pieces and then gray out the the not so important pieces or maybe you can have a model that looks at the dip and sees, oh, there's a likely bug here. I will like mark this with a little red squiggly and say like you should probably like review this part of the dip. And ideas in that vein, I think are exciting. Yeah, that's a really fascinating space of like UX design engineering. So you basically trying to guide the human programmer through all the things they need to read and nothing more, like optimally. Yeah, and you want an intelligent model to do it. Like currently, dip algorithms are they're like like they're just like normal algorithms. There's no intelligence. There's like intelligence that went into designing the algorithm. But then there's no like you don't care if it's about this thing or this thing as you want a model to do this. So I think the general question is like mad, these models are going to get much smarter as the models get much smarter. The changes they will be able to propose are much bigger. So the apps that changes gets bigger and bigger and bigger. The humans have to do more and more verification work. It gets more and more hard like it's just you need you need to help them out. It's sort of I don't want to spend all my time reviewing code. Can you say a little more across multiple files div? Yeah, I mean, so get out try to sell this, right? With code review. When you're doing code review, you're reviewing multiple deaths across multiple files. But like Arvitt said earlier, I think you can do much better than code review. You know, code review kind of sucks. Like you spend a lot of time trying to grok this code that's often quite unfamiliar to you. And it often like doesn't even actually catch that many bugs. And I think you can significantly improve that review experience using language models. For example, using the kinds of tricks that Arvitt had described of maybe pointing you towards the regions that actually matter. I think also if the code is produced by these language models, and it's not produced by someone else. Like the code review experience is design for both the reviewer and the person that produced the code. In the case where the person that produced the code is language model, you don't have to care that much about their experience. And you can design the entire thing around the reviewers such that the reviewers job is as fun as easy as productive as possible. And I think that that feels like the issue with just kind of naively trying to make these things look like code review. I think you can be a lot more creative and push the boundary on what's possible. Just one idea there is I think ordering matters. Generally when you review a PR, you have this list of files and you're reviewing them from top to bottom. But actually, like you actually want to understand this part first because that came like logically first. And then you want to understand the next part. And you don't want to have to figure out that yourself. You want a model to guide you through the thing. And is the step of creation going to be more and more natural language is the goal versus with actual. I think sometimes I don't think it's going to be the case that all of programming will be natural language. And the reason for that is, you know, if I'm programming with Swalans, while it's at the computer in the keyboard. And sometimes if I'm driving, I want to say just wallet, hey, implement this function. And that works. And then sometimes it's just so annoying to explain to Swalav what I want him to do. And so I actually take over the keyboard and I show him I write part of the example. And then it makes sense. And that's the easiest way to communicate. And so I think that's also the case for AI. Sometimes the easiest way to communicate with AI will be to show an example. And then it goes and does the thing everywhere else. Or sometimes if you're making a website, for example, the easiest way to show to the AI what you want does not to tell it what to do. But you know, drag things around or draw things. And yeah. And like maybe eventually we will get to like brain machine interfaces or whatever any kind of like understanding what you're thinking. And so I think natural language will have a place. I think it will not definitely not be the way most people program most of the time. I'm really feeling the age guy with this editor. It feels like there's a lot of machine learning going on underneath. Tell me about some of the ML stuff that makes it all work. We're cursor really works via this ensemble of custom models that we've trained alongside, you know, the frontier models that are fantastic at the reasoning and tense things. And so cursor tab, for example, is a great example of where you can specialize this model to be even better than even frontier models if you look at evels on the task we set it at. The other domain which it's kind of surprising that requires custom models, but it's kind of necessary and works quite well is in apply. So I think these models are like the frontier models are quite good at sketching out plans for code and generating like rough sketches of like the change, but actually creating deaths is quite hard for frontier models for when you're training models. Like you try to do this with sonnet with a one any frontier model and it really messes up stupid things like counting line numbers, especially in super super large files. And so what we've done to alleviate this is we let the model kind of sketch out this rough code block that indicates what the change will be. And we train a model to then apply that change to the file. And we should say that apply is the model looks at your code. It gives you a really damn good suggestion of what new things to do and the seemingly for humans trivial step of combining the two you're saying is not so trivial. Contrary to popular perception, it is not a deterministic algorithm. Yeah. I think like you see shallow copies of apply elsewhere and it just breaks like most of the time because you think you can kind of try to do some deterministic matching and then it fills you know at least 40% of the time and that just results in a terrible product experience. I think in general this this regime of you are going to get smarter and smarter models. And like so one other thing that apply lets you do is it lets you use fewer tokens with the most intelligent models. This is both expensive in terms of latency for generating all these tokens and cost. So you can give this very very rough sketch and then have your model models go and implement it because it's a much easier task to implement. This very very sketched out code. And I think this this regime will continue where you can use smarter and smarter models to the planning and then maybe the implementation details can be handled by the less intelligent ones. Perhaps you'll have maybe a one maybe it'll be even more capable models given an even higher level plan that is kind of recursively applied by SONNET and an EPLI model. Maybe we should we should talk about how to make it fast. Yeah. If you like to fast as always an interesting detail. How do you make it fast? Yeah. So one big component of making it fast is speculative edits. So speculative edits are a variant of speculative encoding and maybe be helpful to briefly describe speculative decoding. With speculative decoding what you do is you can kind of take advantage of the fact that most of the time and I'll add the caveat that it would be when you're memory bound in language model generation. If you process multiple tokens at once it is faster than generating one token at a time. So this is like the same reason why if you look at tokens per second with prompt tokens versus generated tokens it's much much faster for prompt tokens. So what we do is instead of using what speculative decoding normally does which is using a really small model to predict these draft tokens that your larger model will then go in and verify. With code edits we have a very strong prior of what the existing code will look like and that prior is literally the same exact code. So you can do is you can just feed chunks of the original code back into the into the model and then the model will just pretty much agree most of the time that okay I'm just going to spit this code back out and so you can process all of those lines in parallel and you just do this with sufficiently many chunks and eventually you'll reach a point of disagreement where the model will now predict text that is different from the ground truth original code. It'll generate those tokens and then we kind of will decide after enough tokens match the original code to restart speculating a chunks of code. What this actually ends up looking like is just a much faster version of normal editing code. So it's just like it looks like a much faster version of the model rewriting all the code. So just we can use the same exact interface that we use for for Diff's but it will just stream down a lot faster. And then the advantages that while while streaming you can just also be reviewing start reviewing the code exactly before before it's done. So there's no no big loading screen. So maybe that is part of the part of the advantage. So the human can start reading before the thing is done. I think the interesting riff here is something like the speculation is a fairly common idea nowadays. It's like not only in language models. I mean there's obviously speculation and CPUs and there's like speculation for databases and speculation all over the place. Let me ask the sort of the ridiculous question of which all of them is better at coding. GPT, Claude, who wins in the context of programming and I'm sure the answer is much more nuanced because it sounds like every single part of this involves a different model. Yeah, I think there there's no model that Prado dominates. Others meaning it is better in all categories that we think matter. The categories being speed ability to edit code, ability to process lots of code long context, you know, a couple of other things and kind of coding capabilities. The one that I'd say right now is just kind of net best is on it. I think this is a consensus opinion. Our one's really interesting and it's really good at reasoning. So if you give it really hard programming interview style problems or lead code problems, it can do quite quite well in them. But it doesn't feel like it kind of understands your rough intent as well as so on it does. Like if you look at a lot of the other frontier models, one quam I have is it feels like they're not necessarily over I'm not saying they train on benchmarks, but they perform really well in benchmarks relative to kind of everything that's kind of in the middle. So if you try it on all these benchmarks and things that are in the distribution of the benchmarks they're evaluated on, you know, they'll do really well. But when you push them a little bit outside of that, so on it's I think the one that that kind of does best at kind of maintaining that same capability. Like you kind of have the same capability in the benchmark as when you try to instruct it to do anything with coding. What another day goes question is the difference between the normal programming experience versus what benchmarks represent like where do benchmarks fall short do you think when we're evaluating these models. By the way, that's like a really really hard. It's like like critically important detail like how how difference like benchmarks are versus versus like real coding. We're real coding. It's not interview style coding. It's you're doing these, you know, humans are saying like half broken English sometimes and sometimes you're saying like, oh, do what I did before. Sometimes you're saying, you know, go add this thing and then do this other thing for me and then make this UI element and then, you know, it's just like a lot of things are sort of context dependent. You really want to like understand the human and do do what the human wants as opposed to sort of maybe the way to put it is sort of abstractly is the interview problems are very well specified. Dave lean a lot on specification while the human stuff is less specified. Yeah. I think that this this venture question is both complicated by what I'm sorry I just mentioned and then also to what a mon was getting into is that even if you like, you know, there's this problem of like the skew between what can you actually model in a benchmark versus real programming and that can be sometimes hard to encapsulate because it's like real programming is like very messy and sometimes things aren't super well specified what's correct or what isn't. But then it's also doubly hard because of this public benchmark problem and that's both because public benchmarks are sometimes kind of feel climbed on. Then it's like really, really hard to also get the data from the public benchmarks out of the models. And so for instance, like one of the most popular like agent benchmarks, sweet bench, is really, really contaminated and the training data of these foundation models. And so if you ask these foundation models to do a sweet bench problem, you actually don't give them the context of a code base. They can like hallucinate the right file pass. They can hallucinate the right function names. And so that the it's also just the public aspect of these things is tricky. Yeah, like in that case, it could be trained on the literal issues or pull requests themselves. And maybe the lives will start to do a better job or they've already done a good job at decontaminating those things, but they're not going to emit the actual training data of the repository itself. Like these are all like some of the most popular Python repositories like Sympi's one example. I don't think they're going to handicap their models on Sympi and all these popular Python repositories in order to get true evaluation scores in these benchmarks. Yeah. I think that given the dearth in benchmarks, there have been like a few interesting crutches that places that build systems with these models or build these models actually use to get a sense of are they going the right direction or not. And in a lot of places, people will actually just have humans play with the things and give qualitative feedback on these. Like one or two of the foundation model companies, they have people who that's a big part of their role. And internally, we also qualitatively assess these models and actually lean on that a lot in addition to like private e-vails that we have. It's like the vibe. Yeah, the vibe. The vibe benchmark, human benchmark. You pull in the humans to do a vibe check. Yeah. Okay. I mean, that's kind of what I do, just like reading online forums and Reddit and X just like, well, I don't know how to properly load in people's opinions because they'll say things like, I feel like Claude or GPT's gotten dumber or something. They'll say, I feel like, and then I sometimes feel like that too, but I wonder if it's the model's problem or mine. Yeah, with Claude, there's an interesting take I heard where I think AWS has different chips. And I suspect they have slightly different numerics than Nvidia GPUs. And someone speculated that Claude's degraded performance would have to do with maybe using the quantized version that existed on AWS bedrock versus whatever was running on Anthropics GPUs. I interview a bunch of people that have conspiracy theories. I'm glad to spoke to this conspiracy theory. It's not like conspiracy theory as much as there are humans are humans and there's these details. And you're doing like these queasy monoflops and chips are messy and you can just have bugs like bugs are, it's hard to overstate how hard bugs are to avoid. What's the role of a good prompt in all this? You will mention that benchmarks have really structured well formulated problems. What should a human be doing to maximize success? And what's the importance of what the humans, you wrote a blog post on, you called it prompt design. Yeah, I think it depends on which model you're using and all of them are slightly different. And they respond differently to different prompts. But I think the original GP-4 and the original sort of breed of a model last last year, they were quite sensitive to the prompts. And they also had a very small context window. And so we have all of these pieces of information around the code base that would maybe be relevant in the prompt. Like you have the docs, you have the files that you add, you have the conversation history. And then there's a problem like how do you decide what you actually put in the prompt and when you have a limited space? And even for today's models, even when you have long context, filling out the entire context window means that it's slower and means that sometimes the model actually gets confused and some models get more confused than others. And we have this one system internally that we call preempt, which helps us with that a little bit. And I think it was built for the era before where we had 8,000 token context windows. And it's a little bit similar to when you're making a website, you sort of, you want it to work on mobile, you want it to work on a desktop screen. And you have this dynamic information, which you don't have. For example, if you're making like designing a print magazine, you have like, you know exactly where you can put stuff. But when you have a website or when you have a prompt, you have these inputs. And then you need to format them to always work. Even if the input is really big, then you might have to cut something down. And so the idea was, okay, like let's take some inspiration. What's the best way to design websites? Well, the thing that we really like is is react and the declarative approach where you you use JSX in JavaScript. And then you declare this is what I want. And I think this has higher priority or like this has higher Z index than something else. And then you have this rendering engine in web design. It's like Chrome and in our cases, the preempt renderer, which then fits everything onto the page. And as you declare, we decide what you want. And then it figures out what you want. And as we have found that to be quite helpful. And I think the role of it has has sort of shifted over time, where initially was to fit to these small context windows. Now it's really useful because, you know, it helps us with splitting up the data that goes into the prompt and the actual rendering of it. And so it's easier to debug because you can change the rendering of the prompt and then try it on old prompts because you have the raw data that went into the prompt. And then you can see did my change actually improve it for for like this entire e-bilt set. So do you literally prompt with JSX? Yes, yes. So it kind of looks like react. There are components like we have one component that's a file component. And it takes in like the cursor like usually there's like one line where the cursor is in your file. And that's like probably the most important line because that's one you're looking at. And so then you can give priorities. So like that line has the highest priority. And then you subtract one for every line that is farther away. And then eventually when it's rendered it figure out how many lines can actually fit in a centers around that thing. That's amazing. And you can do like other fancy things where if you have lots of code blocks from the entire code base, you could use retrieval and things like embedding and re-ranking scores to add priorities for each of these components. So should humans when they ask questions also use try to use something like that like would it be beneficial to write JSX in the problem or the whole idea is this should be loose and messy. I think our goal is kind of that you should just do whatever is the most natural thing for you. And then we our job is to figure out how do we actually like retrieve the relative of things so that you're thinking actually it makes sense. Well, this is the sort of the discussion I had with Arvind of Proplexity. It's like his whole idea is like you should let the person be as lazy. Yeah. But like yeah, that's a beautiful thing. But I feel like you're allowed to ask more of programmers. So like if you say just do what you want. I mean humans are lazy. There's a kind of tension between just being lazy versus like provide more as be prompted almost like the system pressuring you or inspiring you to be articulate. Not in terms of the grammar of the sentences, but in terms of the depth of thoughts that you convey inside the the problems. I think even as a system gets closer to some level of perfection. Often when you ask the model for something, you just are not not in tendous conveyed to know what to do. And there are like a few ways to resolve that intent. One is the simple thing of having the model just ask you. I'm not sure how to do these parts based on your query. Could you clarify that? I think the other could be maybe if you there are five or six possible generations given the uncertainty present in your query so far, why don't we just actually show you all those and let you pick them. How hard is it to for the model to choose to speak to talk back? It's hard to sort of like how to deal with the uncertainty. Do I choose to ask for more information to reduce the ambiguity? So I mean one of the things we do is it's like a recent addition is try to suggest files that you can add. So while you're typing, one can guess what the uncertainty is and maybe suggest that like, you know, maybe maybe you're writing your API. And we can guess using the commits that you've made previously in the same file that the client and the server is super useful. And there's like a hard technical problem of how do you resolve it across all commits, which files are the most important given your current prompt. And we're still sort of initial versions rolled out and I'm sure we can make it much more accurate. It's very experimental. But then the idea is we show you like do you just want to add this file, this file, this file also to tell you know, the model to edit those files for you. Because if you're maybe you're making the API like you should also edit the client and the server that is using the API and the other one is only the API. So that will be kind of cool as both there's the phase where you're writing the prompt and there's before you even click enter, maybe we can help resolve some of the uncertainty. To what degree do you use agentic approaches? How use for our agents? We think agents are really really cool. I think agents is like it's like resembles sort of like a human. It's sort of like the things like you can kind of feel that it like you're getting closer to AGI because you see a demo where it acts as as a human would and and it's really really cool. I think agents are not yet super useful for many things. They I think we're getting close to where they will actually be useful. And so I think there are certain types of tasks we're having an agent would be really nice. Like I would love to have an agent. For example, if like we have a bug where you sometimes can't come and see and come and be inside our chat input box. And that's a task that's super well specified. I just want to say like in two sentences, this does not work. Please fix it. And then I would love to have an agent that just goes off does it and then a day later I come back and I reviewed the thing. You mean it goes finds the right file? Yeah, it finds the right files. It like tries to reproduce the bug. It like fixes the bug and then it verifies that is correct. And this could be a process that takes a long time. And so I think I would love to have that. And then I think a lot of programming like there is often this belief that agents will take over all of programming. I don't think we think that that's the case because a lot of programming, a lot of the value is in iterating or you don't actually want to specify something upfront because you don't really know what you want until you've seen an initial version and then you want to iterate on that. And then you provide more information. And so for a lot of programming, I think you actually want a system that's instant that gives you an initial version instantly back and then you can iterate super super quickly. What about something like that? We think I'm a replete agent that does also like setting up the development environment and solving software packages, configuring everything, configuring the databases and actually deploying the app. Yeah. Is that also in the set of things you dream about? I think so. I think that would be really cool. For certain types of programming, it would be really cool. Is that within scope of cursor? Yeah. We aren't actively working on it right now. But it's definitely like we want to make the programmers life easier and more fun. And some things are just really tedious and you need to go through a bunch of steps and you want to delegate that to an agent. And then some things you can actually have an agent in the background while you're working like let's say you have a PR that's both backend and front end and you're working in the front end. And then you can have a background agent that doesn't work and figure out kind of what you're doing. And then when you get to the back end part of your PR, then you have some like initial piece of code that you can iterate on. And so that would also be really cool. One of the things we already talked about is speed. But I wonder if we can just link around that some more in the various places that the technical details involved in making this thing really fast. So every single aspect of cursor, most aspects of cursor feel really fast. Like I mentioned, the applies probably the slowest thing. And for me, for my... Sorry, the pain. It's a pain that we're feeling and we're working on fixing it. Yeah. I mean, it says something that something that feels, I don't know what it is, like one second or two seconds. That feels slow. That means that's actually shows that everything else is just really, really fast. So is there some technical details about how to make some of these models, how to make the chat fast, how to make the divs fast? Is there something that just jumps to mine? Yeah. I mean, so we can go over a lot of the strategies that we use. One interesting thing is cache warming. And so what you can do is if, as the user's typing, you can have... You're probably going to use some piece of context. And you can know that before the user's done typing. So as we discussed before reusing the kvcache, results in lower latency, lower cost, cross request. So as the user starts typing, you can immediately warm the cache with, like, let's say the current file contents. And then when they press enter, there's very few tokens that actually has to pre-fill and compute before starting the generation. This will significantly lower TTFD. Can you explain how kvcache works? Yeah. So the way transformers work. I like it. I mean, one of the mechanisms that allow transformers to not just independently, like the mechanism that allows transformers to not just independently look at each token, but see previous tokens are the keys and values to tension. And generally the way attention works is you have at your current token some query. And then you've all the keys and values of all your previous tokens, which are some kind of representation that the model stores internally of all the previous tokens in the prompt. And like by default, when you're doing a chat, the model has to, for every single token, do this for pass through the entire model. That's a lot of matrix multiplies that happen. And that is really, really slow. Instead, if you have already done that and you stored the keys and values and you keep that in the GPU, then when I'm, let's say I have sorted for the last end tokens, if I now want to compute the, the output token for the n plus one token, I don't need to pass those first end tokens through the entire model because I already have all those keys and values. And so you just need to do the forward pass through that last token. And then when you're doing attention, you're reusing those keys and values that have been computed, which is the only kind of sequential part or sequentially dependent part of the transformer. Is there like higher level caching of like caching of the prompts or that kind of stuff to help? I see. Yeah, that, that there's other types of caching you can kind of do. One interesting thing that you can do for cursor tab is you can basically predict the head as if the user would have accepted the suggestion and then trigger another request. And so then you've done the speculative, it's a mix of speculation and caching, right? Because you're speculating what would happen if they accepted it. And then you have this value that is cash, this suggestion. And then when they pressed tab, the next one would be waiting for them immediately. It's a kind of clever heuristics-lash trick that uses a higher level caching and can give the, it feels fast despite they're not actually being any changes in the model. And if you can make the kvcache smaller, one of the advantages you get is like maybe you can speculate even more. Maybe you can guess here's the 10 things that could be useful. I like, like, predict the next 10 and then like it's possible the user hits the one of the 10. It's like much higher chance than the user hits like the exact one that you showed them. Maybe they type in other character and we sort of hit it's something else in the cache. Yeah. So there's all these tricks where the general phenomena here is, I think it's also super useful for RL is, you know, maybe a single sample from the model isn't very good. But if you predict like 10 different things, turns out that one of the 10, that's right, is the probability is much higher. There's these passat k curves and you know, part of RL, like what RL does is, you can exploit this passat k phenomena to make many different predictions and and one way to think about this, the model sort of knows internally has like, has some uncertainty over like which of the k things is correct or like which of the k things does the human wants it when we RL are, you know, cursor tab model. One of the things we're doing is we're predicting which like which of the hundred different suggestions the model produces is more amenable for humans like which of them to humans more like than other things. Maybe like there's something where the model can predict very far ahead versus like a little bit and maybe somewhere in the middle and and you know, just and then you can give a reward to the things that humans would like more and and sort of punish the things that it would like and sort of then train the model to out, but the suggestion said humans would like more. You have these like RL loops that are very useful that exploit these passat k curves. Um, um, um, um, maybe can, can you go into even more detail? Yeah, it's a little, it is a little different than speed, um, but I mean like technically you tie it back in because you can get away at the smaller model if you RL your smaller model and it gets the same performance as the bigger one. Um, that's like and as well I was mentioning stuff about kv about reducing the size of your kvcash there are other techniques there as well that are really helpful for speed. Um, so kind of back in the day like all the way two years ago, people mainly use multi-head attention. Um, and I think there's been a migration towards more efficient attention schemes like group query, um, or multi-query attention. And this is really helpful for then, uh, with larger batch sizes being able to generate the tokens much faster. The interesting thing here is, um, this now has no effect on that time to first token pre-fill speed. Uh, the thing this matters for is, uh, now generating tokens. And, and why is that? Because when you're generating tokens instead of, uh, being bottlenecked by doing the super paralyzable matrix multiplies across all your tokens, your bottleneck by how quickly it's for long context, um, with large batch sizes, by how quickly you can read those cash keys and values. Um, and so then how that's memory bandwidth and how can we make this faster? We can try to compress the size of these keys and values. So multi-query attention is the most aggressive of these, um, where normally with multi-head attention, you have some number of quote-quote attention heads, um, and some number of kind of query, query heads. Multi-query just preserves the query heads, gets rid of all the key value heads. Um, so there's only one kind of key value head and there's all the remaining, uh, query heads. With group query, um, you instead, you know, preserve all the query heads and then your keys and values are kind of, uh, there are fewer heads for the keys and values, but it's, you're not reducing it to just one. Um, but anyways, like the whole point here is you're just reducing the size for your KV cash. And then there is the MLA. Yeah, multi-latent. Um, that's a little more complicated. And the way that this works is it kind of turns the entirety of your keys and values across all your heads into this kind of one latent vector. That is then kind of expanded in for its time. But MLA is from this company, uh, called DeepSeek. Um, it's quite an interesting algorithm. Uh, maybe the key idea is sort of, uh, in both MQA uh, and in other places, what you're doing is sort of reducing the, uh, num, like the number of KV heads that manage you get from that is, is, you know, there's less of them, but, uh, maybe the theory is that you actually want a lot of different, uh, like you want each of the, the keys and values to actually be different. So one way to reduce the sizes, you keep, uh, one big shared vector for all the keys and values. And then you have smaller vectors for every single token so that when you mut, you can, you can store the only the smaller thing. There's just some sort of like lower rank reduction. And the lower rank reduction, well, that, and at the end of the time when you, when you eventually want to compute the final thing, uh, remember that like your memory band, which means that like you still have some, some compute left that you can use for these things. And so if you can expand the, uh, um, the latent vector back out. And somehow like, this is far more efficient because just like, you're reducing like, for example, maybe, maybe you're reducing like 32 or something, like the size of the vector that you're keeping. Yeah, there's perhaps some richness in having a separate, uh, set of keys and values and query that kind of pairwise match up versus compressing that all into one. And that interaction at least. Okay. And all of that is dealing with, um, being memory bound. Yeah. And what, I mean, ultimately, how does that map to the user experience, trying to give us the, yeah, the two things that a map to is you can now make your cache a lot larger, because you've less space allocated for the kvcache, you can maybe cache a lot more aggressively and a lot more things. So you get more cache hits, which are helpful for reducing the time to first token for the reasons that we're kind of described earlier. And then the second being when you start doing inference with more and more requests and larger and larger batch sizes, you don't see much of a slowdown in as it's generating the tokens, the speed of that. What it also allows you to make your prompt bigger for certain. Yeah. Yeah. So like the basic, the size of your kvcache is, both the size of all your prompts multiplied by the number of prompts in process and parallel. So you could increase either those dimensions, right? The batch size or the size of your prompts without degrading the latency of generating tokens. All right. You wrote a blog post shadow of workspace iterating on code in the background. Yeah. So what's going on? Uh, so to be clear, we want there to be a lot of stuff happening in the background. And we're experimenting with a lot of things. Right now, we don't have much of that happening other than like the cache warming or like, you know, figuring out the right context to that goes into your command k prompts, for example. But the idea is, if you can actually spend computation in the background, then you can help, um, help the user maybe like at a slightly longer time horizon than just predicting the next few lines that you're going to make, but actually like in the next 10 minutes, what are you going to make? And by doing it in the background, you can spend more computation doing that. And so the idea of the shadow workspace that we implemented and we use it internally for like experiments is that to actually get advantage of doing stuff in the background, you want some kind of feedback signal to give back to the model. Because otherwise, like you can get higher performance by just letting the model think for longer. Um, and so like, oh, one is a good example of that. But another way you can improve performance is by letting the model iterate and get feedback. And so one very important piece of feedback when you're a programmer is, um, the language server, which is this thing it exists for most different languages. And there's like a separate language server per language. And it can tell you, you know, you're using the wrong type here and then it gives you an error or it can allow you to go to definition and sort of understands the structure of your code. So language servers are extensions developed by like, there's a typescript language server developed by the typescript people, a rust language server developed by the rust people, and then they all interface over the language server protocol to a VS code. So that VS code doesn't need to have all of the different languages built into VS code, but rather, uh, you can use the existing compiler infrastructure infrastructure for linting purposes. What is for it's for linting? It's for going to definition. Uh, and if we're like seeing the, the right types that you're using, uh, what's it doing? Like type checking also? Yes. Type checking and going to references. Um, and that's like when you're working in a big project, you, you kind of need that. If you, if you don't have that, it's like really hard to, to code in a big project. Can you say again, how that's being used inside cursor, the, uh, the language server protocol communication thing? So it's being used in cursor to show to the programmer just like in VS code. But then the idea is you want to show that same information to the models, the I.M. models. Um, and you want to do that in a way that doesn't affect the user because you want to do it in background. And so the idea behind the chat at workspace was, okay, like one way we can do this is, um, we spawn a separate window of cursor that's hidden. And so you can set this flag and like turn its head in. There is a window, but you don't actually see it. And inside of this window, the A.I. agents can modify code however they want. Um, as long as they don't save it because it's still the same folder. Um, and then can get feedback from, from the lenders and go to definition and, and iterate on their code. So like literally run everything in the background. Like as if, right. Yeah. Maybe even run the code. So that's the eventual version. Okay. And that's what you want. And a lot of the blog post is actually about how do you make that happen? Because it's a little bit tricky. You want it to be on the user's machines so that it exactly mirrors the user's environment. And then on Linux, you can do this cool thing where you can actually mirror the file system and have the A.I. make changes to the files and it thinks that it's operating on the file level. But actually, that's stored in memory and you, you can create this kernel extension to make it work. Whereas on Mac and Windows, it's a little bit more difficult. And, but it's, it's a fun technical problem. So that's why one, one, maybe hacky, but interesting idea that I like is holding a lock on saving. And so basically you can then have the language model kind of hold the lock on, on saving to disk. And then instead of you operating in the ground truth version of the files that are safe to disk, you actually are operating what was the shadow workspace before and these unsafe things that only exist in memory that you still get linter errors for and you can code in. And then when you try to maybe run code, it's just like there's a small warning that there's a lock. And then you kind of will take back the lock from the language server if you're trying to do things concurrently or from the shadow workspace if you're trying to do things concurrently. That's such an exciting future. By the way, it's a bit of a tangent, but like to allow a model to change files, it's scary for people, but like it's really cool to be able to just like let the agent do a set of tasks and you come back the next day and kind of observe like it's a colleague or something like that. Yeah. And I think there may be different versions of like runability where for the simple things where you're doing things in the span of a few minutes on behalf of the user as they're programming, it makes sense to make something work locally in their machine. I think for the more aggressive things, we're making larger changes that take longer periods of time. You'll probably want to do this in some sandbox remote environment. And that's another incredibly tricky problem of how do you exactly reproduce or mostly reproduce to the point of it being effectively equivalent for running code, the user's environment, which is remote sandbox. I'm curious what kind of agents you want for coding. Do you want them to find bugs? Do you want them to implement new features? What agents do you want? So by the way, when I think about agents, I don't think just about coding. I think so for the practice, this particular podcast, this video editing, and a lot of if you look at Adobe, a lot of this code behind, it's very poorly documented code, but you can interact with Premiere, for example, using code. And basically all the uploading, everything I do on YouTube, everything as you could probably imagine, I do all of that through code and including translation, overdubbing all of this. So I envision all of those kinds of tasks, so automating many of the tasks that don't have to do directly with the editing. So that okay, that's what I was thinking about. But in terms of coding, I would be fundamentally thinking about bug finding, like many levels of kind of bug finding and also bug finding like logical bugs, not logical like spiritual bugs or something. One's like sort of big directions of implementation, that kind of stuff. That's a fine and bug finding. Yeah, I mean, it's really interesting that these models are so bad at bug finding when just nicely prompted to find a bug. They're incredibly poorly calibrated. Even the smartest model. Exactly. Even know one. How do you explain that? Is there a good intuition? I think these models are really strong reflection of the pre-training distribution. And I do think they generalize as the loss gets lower and lower. But I don't think the loss in the scale is quite, or the loss is low enough such that they're like really fully generalizing on code. Like the things that we use these things for the frontier models that they're quite good at are really code generation and question answering. And these things exist in massive quantities and pre-training with all of the code and GitHub on the scale of many, many trillions of tokens and questions and answers on things like Stack Overflow and maybe GitHub issues. And so when you try to push one of these things that really don't exist very much online, like for example, the cursor tab objective of predicting the next edit, given the edits done so far, the brittleness kind of shows. And then bug detection is another great example where there aren't really that many examples of like actually detecting real bugs and then proposing fixes. And the models just kind of like really struggling. But I think it's a question transferring model. Like in the same way that you get this fantastic transfer from pre-trained models just on code in general to the cursor tab objective, you'll see a very very similar thing with generalized models that are really good at code to bug detection. It just takes like a little bit of kind of nudging in that direction. Like to be clear, I think they sort of understand code really well. Like while they're being pre-trained, like the representation that's being built up, like almost certainly like somewhere in this stream, there's the model knows that maybe there's there's some sketchy going on, right? It sort of has some sketchiness, but actually eliciting the sketchiness to actually like part of it is that humans are really calibrated on which bugs are really important. It's not just actually saying like there's something sketchy. It's like it's just a sketchy trivial. It's the sketchy like you're going to take the server down. Part of it is maybe the cultural knowledge of like why is the staff engineer a staff engineer? A staff engineer is good because they know that three years ago someone drove a really sketchy piece of code that took the server down and as opposed to like as opposed to maybe there's like you know you just this thing is like an experiment. So like a few bugs are fine. Like you're just trying to experiment and get the feel of the thing. And so the model gets really annoying when you're writing an experiment. That's really bad. But if you're writing something for super production, you're like writing a database, right? You're writing code in Postgres or Linux or whatever. Like you're Linus Torval's. You're sort of unacceptable to have even in that case. And just having the calibration of like how paranoid is the user like? But even then like if you're putting in a maximum paranoia, it still just like doesn't quite get it. Yeah. Yeah. I mean, but this is hard for humans to to understand what which line of code is important, which is not. Like you, I think one of your principles on a website says if a code can do a lot of damage, one should add a comment that say this this this line of code is dangerous. And all caps 10 times. No, you say like for every single line of code inside the functioning you have to and that's quite profound. That says something about human beings because the engineers move on even the same person might just forget how it can sync the Titanic a single function. Like you don't you might not intuit that quite clearly by looking at the single piece of code. Yeah, and I think that that one is also partially also for today's AI models where if you actually write dangerous dangerous dangerous in every single line, like the models will pay more attention to that and will be more likely to find bugs in that region. That's actually just straight up a really good practice of labeling code of how much damage this can do. Yeah, I mean, it's controversial. I guess some people think it's ugly. Well, actually, it's like fact, I actually think there's one of the things I learned from Arvitt is, you know, like I sort of aesthetically, I don't like it, but I think there's certainly something where like it's useful for the models and in humans, just forget a lot and it's really easy to make a small mistake and cause like bring down, you know, like just bring down the server and like like go first, we like test a lot and whatever, but there's always these things that you have to be very careful. Yeah, like with just normal doc strings, I think people will often just skim it when making a change and think, oh, I know how to do this. And you kind of really need to point it out to them so that doesn't slip through. Yeah, you have to be reminded that you could do a lot of damage. That's like we don't really think about that. Yeah, you think about, okay, how do I figure out how this works so I can improve it? You don't think about the other direction. Yeah, until we have formal verification for everything, then you can do whatever you want and you know for certain that you have not introduced a bug if the proof pass. But concretely, what do you think that future would look like? I think people will just not write tests anymore and the model will suggest, like you write a function, the model will suggest a spec and you review this spec and in the meantime, smart reasoning model computes a proof that the implementation follows the spec. And I think that happens for most functions. Do you think this gets at a little bit some of the stuff you were talking about earlier with the difficulty of specifying intent for what you want to solve for? Where sometimes it might be because the intent is really hard to specify, it's also then going to be really hard to prove that it's actually matching whatever your intent is. Like you think that spec is hard to generate? Yeah, or just like for a given spec, maybe you can... I think there is a question of like can you actually do the formal verification? Like is that possible? I think that there's like more to dig into there. But then also... Even if you have the spec? If you have the spec, is the spec written in natural language? Yeah, how does it work? No, the spec would be formal. But how easy would that be? So then I think that you care about things that are not going to be easily well specified in the spec language. I see, I see. Yeah. Yeah. Maybe an argument against formal verification is all you need. Yeah. The word... But there's this massive document. Replacing something like unit tests. Sure. Yeah. I think you can probably also evolve the spec languages to capture some of the things that they don't really capture right now. I don't know. I think it's very exciting. And you're speaking not just about like single functions. You're speaking about entire code bases. I think entire code bases is harder, but that is what I would love to have. And I think it should be possible. And because you can even... There's like a lot of work recently where you can prove formally verified down to the hardware. So like through the... You formally verify the C code and then you formally verify through the GCC compiler and then through the very log down to the hardware. And that's like an incredibly big system, but it actually works. And I think big code bases are sort of similar in that they're like multi-layered system. And if you can decompose it and formally verify each part, then I think it should be possible. I think the specification problem is a real problem, but how do you handle side effects? Or how do you handle, I guess, external dependencies like calling Stripe API? Maybe Stripe would. Right. Respect for their... But you can't do this for everything. Like, can you do this for everything you use? Like, how do you do it for... There's a language... Like maybe people will use language models as primitives in the program. And there's a dependence on it. And how do you now include that? I think it might be able to prove that still. Prove what about language models? I think it feels possible that you could actually prove that a language model is aligned, for example. Or like you can prove that it actually gives the right answer. That's the dream. Yeah, that is. I mean, if it's possible, I have a dream speech. It will certainly help with making sure your code doesn't have bugs and making sure AI doesn't destroy all human civilization. So the full spectrum of AI safety to just bug finding. So you said the model's struggle with bug finding. What's the hope? My hope initially is... And I can let Michael chime in too, but it should first help with the stupid bugs. It should very quickly catch the stupid bugs. Off by one, sometimes you write something in a common and do the other way. It's very common. I do this. I'd like less than in a comment and maybe write the greater than something. And the model is like, yeah, it looks catchy. Do you sure you won't want to do that? But eventually, it should be able to catch harder bugs. Yeah. And I think that it's also important to note that this is having good bug finding models feels necessary to get to the highest reaches of having AI do more and more programming for you. Where you're going to, you know, if the AI is building more and more of the system for you, you need to not just generate, but also verify. And without that, some of the problems we've talked about before with programming with these models will just become untenable. So it's not just for humans. You write a bug, I write a bug, find a bug for me, but it's also being able to verify the AI's code and track it. It's really important. Yeah. And then how do you actually do this? We have had a lot of contentious dinner discussions of how do you actually screen a bug model. But one very popular idea is it's kind of potentially easy to introduce a bug than actually finding the bug. And so you can train a model to introduce bugs in existing code. And then you can train a reverse bug model then that can find bugs using this synthetic data. So that's like one example. But yeah, there are lots of ideas for how to do it. You can also, you can also do a bunch of work not even at the model level of taking the biggest models and then maybe giving them access to a lot of information that's not just the code. Like it's kind of a hard problem to like stare at a file and be like, where's the bug? And you know, that's hard for humans often, right? And so often you have to run the code and being able to see things like traces and step through to bugger. There's another whole other direction where it like kind of tends toward that. And it could also be that there are kind of two different product form factors here. It could be that you have a really specialty model that's quite fast. That's kind of running in the background and trying to spot bugs. And it might be that sometimes sort of sort of Arvid's earlier example about, you know, some nefarious input box bug. It might be that sometimes you want to like, there's, you know, there's a bug. You're not just like checking hypothesis free. You're like, this is a problem. I really want to solve it. And you zap that with tons and tons and tons of compute and you're willing to put in like $50 to solve that bug or something even more. Have you thought about integrating money into this whole thing? Like I would pay probably a large amount of money for if you found a bug or even generated code that I really appreciated. Like I had a moment a few days ago when I started using cursor or generated a perfect like perfect three functions for interacting with the YouTube API to update captions and full localization like different in different languages. The API documentation is not very good. And the code across like if I Google it for a while, I couldn't find exactly there's a lot of confusing information and cursor generated perfectly. And I was like, I just sit back, I read the code and I was like, this is correct. I tested it is correct. I was like, I want a tip on a, on a button that goes. Yeah. There's $5. One that's really good just to support the company and support what the, the interface is. And the others that probably send a strong signal like good job. Right. There's much stronger signal than just accepting the code, right? You just actually send like a strong good job. That and for bug finding obviously like there's a lot of people, you know, they will pay a huge amount of money for a bug like a bug bounty thing. Right. Is that, do you guys think about that? Yeah. It's a controversial idea inside the the company. I think it sort of depends on how much you believe in humanity. How much? You know, like I think it would be really cool if like you spend nothing to try to find a bug. And if it doesn't find a bug, you use spend zero dollars. And then if it does find a bug and you click accept, then it also shows like in parentheses like one dollar. As you spend one dollar to accept the bug. And then of course, there's a worry like, okay, we spent a lot of computation. Like maybe people will just copy paste. I think that's a worry. And then there is also the worry that like introducing money into the product makes it like kind of, you know, like it doesn't feel as fun anymore. Like you have to like think about money and you all you want to think about is like the code. And so maybe it actually makes more sense to separate it out. And like you pay some fee like every month. And then you get all of these things for free. But there could be a tipping component, which is not like it. It's still has that like dollar symbol. I think it's fine. But I also see the point where like maybe you don't want to introduce it. Yeah, I was going to say the moment that feels like people do this is when they share it when they have fantastic example, they just kind of share it with their friends. There is also a potential world where there's a technical solution to this like honor system problem too, where if we can get to a place where we understand the output of the system more, I mean to this stuff we were talking about with like, you know, air checking with the LSP and then also running the code. But if you could get to a place where you could actually somehow verify, oh, I have fixed the bug. Maybe then the the Bouchy system doesn't need to rely on the honor system too. How much interaction is there between the terminal and the code? My how much information is gained from if you run the code and the terminal. I can use can you do like a loop where runs runs the code and suggest how to change the code if the code and runtime gives an error is right now they're separate worlds completely. Like I know you can do control K inside the terminal to help you write the code. You can use terminal contacts as well inside of check main K kind of everything. We don't have the looping part yet though we suspect something like this could make a lot of sense. There's a question of whether it happens in the foreground too or if it happens in the background like what we've been discussing. Sure. The background is pretty cool. Like we do running the code in different ways plus there's a database side to this which I highly protect it from not modifying the database but okay. I mean there's certainly cool solutions there. There's this new API that is being developed for. It's not an AWS but you know it's certainly it's I think it's in planet scale. I don't know if planet scale was the first one to add it. It's the stability sort of add branches to a database which is like if you're working on a feature and you want to test against the broad database but you don't actually want to test against the broad database you could sort of add a branch to the database in the way they do that is they add a branch to the right analog and there's obviously a lot of technical complexity in doing it correctly. I guess database companies need new things to do. They have the databases down and I think like turbo buffer which is one of the databases we use as it's going to add hope maybe branching to the right analog and so maybe maybe the AI agents will use branching to test against some branch and it's sort of going to be a requirement for the database to like support branching or something. We're really interesting to you branch a file system. Yeah. If you like everything needs branching it's like yeah. Yeah. It's like the problem with the multiverse right. If you branch an everything that's like a lot. There's obviously these like super clever algorithms to make sure that you don't actually use a lot of space or CPU or whatever. Okay this is a good place to ask about infrastructure. So you guys mostly use AWS. What are some interesting details? What are some interesting challenges? Why do you choose AWS? Why is AWS still winning? Hashtag. AWS is just really really good. It's really good. Like whenever you use an AWS product you just know that it's going to work. Like it might be absolute hell to go through the steps to set it up. Why is the interface so horrible? Because it's just so good. It doesn't need to like. The nature of winning. It's exactly just nature they're winning. Yeah. Yeah. But AWS you can always trust like it will always work. And if there is a problem it's probably your problem. Okay. Is there some interesting like challenges to you guys a pretty new startup to get scaling to like to so many people and yeah I think that there. It has been an interesting journey adding each extra zero to the request per second. But you run into all of these with like the general components you're using for caching and databases running to issues as you make things bigger and bigger and bigger and now we're at the scale where we get like you know into overflows on our tables and things like that. And then also there have been some custom systems that we built. Like for instance our retrieval system for computing a semantic index of your codebase and answering questions about a codebase that have continually I feel like been well one of the the trickier things to scale. I have a few friends who are super super senior engineers and one of their sort of lines is like it's very hard to predict where systems will break when when you scale them. You can sort of try to predict an advance like there's always something something weird that's going to happen when you add this extra zero and you thought you thought through everything which you didn't actually think of everything. But I think for that particular system we've so what the for concrete details the thing we do is obviously we upload when like we chunk up all of your code and then we send it up sort of the code for embedding and we embed the code and then we store the embeddings in a database but we don't actually store any of the code. And then there's reasons around making sure that we don't introduce client bugs because we're very very paranoid about client bugs. We store much of the details on the server like everything is sort of encrypted. So one of the technical challenges is always making sure that the local index, the local codebase state is the same as the state that is on the server and the way technically we ended up doing that is so for every single file you can sort of keep this hash and then for every folder you can sort of keep a hash which is the hash of all of its children and you can sort of recursively do that until the top. And why do something something complicated? One thing you could do is you could keep a hash for every file and every minute you could try to download the hashes that are on the server figure out what are the files that don't exist on the server maybe just created a new file maybe just deleted a files maybe you checked out in your branch and try to reconcile the state between the client and the server. But that introduces like absolutely ginormous network overhead both on the client side. I mean nobody really wants us to hammer their Wi-Fi all the time if you're using cursor but also like I mean it would introduce like ginormous overhead on the database. It would sort of be reading this on a tens of terabyte database sort of approaching like 20 terabytes or something database like every second that's just kind of crazy you definitely don't want to do that. So what do you do here sort of you just try to reconcile the single hash which is at the root of the project. And then if if something mismatches then you go you find where all the things disagree maybe you look at the children and see if the hashes match and if the hashes don't match go look at their children and so on. But you only do that in the scenario where things don't match and for most people most of the time the hashes match. So it's kind of like hierarchical reconciliation. Yeah something like that. Yeah it's called the Merkel's free. Yeah. Yeah. I mean so yeah this is cool to see that you kind of have to think through all these problems. And I mean the point of like the reason it's gotten hard is just because like the number of people using it and you know some of your customers have really really large code bases to the point where we know we originally wrote a jar code base which is big but I mean it's just not the size of some company that's been there for 20 years and sort of has a enormous number of files and you sort of want to scale that across programmers. There's all these details where like building the simple thing is easy but sort of scaling it to a lot of people like a lot of companies is obviously a difficult problem which is sort of independent of actually so that there's part of this scaling our current solution is also coming up with new ideas obviously we're working on but then scaling all of that in the last few weeks months. Yeah and there are a lot of clever things like additional things that go into this indexing system. For example the bottleneck in terms of costs is not soaring things in the factory database or the database is actually embedding the code and you don't want to re-embed the code base for every single person in a company that is using the same exact code except for maybe there it is for branch with a few different files or they've made a few local changes and so because again embeddings of the bottleneck you can do is one clever trick and not have to worry about like the complexity of like dealing with branches and the other databases where you just have some cash on the actual vectors computed from the hash of a given chunk and so this means that when the end person that a company goes into a vendor code base it's really really fast and you do all this without actually storing any code on our servers at all no code data stored we just store the vectors in the vector database and the vector cash. What's the biggest gains at this time you get from indexing the code base? It could just out of curiosity like what benefit users have it seems like longer term there'll be more and more benefit but in the short term just asking questions of the code base what's the usefulness of that? I think the most obvious one is just you want to find out where something is happening in your large code base and you sort of have a fuzzy memory of okay I want to find the place where we do X but you don't exactly know what to search for in a normal text search and so you ask a chat you hit command enter to ask with with the code base chat and then very often it finds the right place that you were thinking of. I think like you like you mentioned in the future I think this is only going to get more and more powerful where we're working a lot on improving the quality of our tree bowl and I think the ceiling for that is really really much higher than people give credit for. One question that's good to ask here have you considered and why having you much done sort of local stuff to where you can do the it seems like everything was just discussed is exceptionally difficult to do to go to the cloud you have to think about all these things with the caching and the you know large code base with a large number of programmers are using the same code base you have to figure out the puzzle of that a lot of it you know most software just does stuff this heavy computational stuff locally. I think you consider doing sort of embedding locally. Yeah we thought about it and I think it would be cool to do it locally I think it's just really hard and one thing to keep in mind is that you know some of our users use the latest MacBook Pro and but most of our users like more than 80% of our users are in Windows machines which and many of them are not very powerful and and so local models really only works on the on the latest computers and it's also a big overhead to to to build that in and so even if we would like to do that it's currently not something that we are able to focus on and I think there are there are some people that that that do that and I think that's great but especially as models get bigger and bigger and you want to do fancier things with like bigger models it becomes even harder to do locally. Yeah and it's not a problem with like weaker computers it's just that for example if you're some big company you have big company code base it's just really hard to process big company code base even on the beefiest MacBook Pros. So even if it's not even a matter of like if you're just like a student or something I think if you're like the best programmer at a big company you're still going to have a horrible experience if you do everything locally when you could you could do edge and sort of scrape by but like again it wouldn't be fun anymore. Yeah like it approximate nearest neighbors on this massive code base is going to just eat up your memory and your CPU and that's and that's and that's just that like let's talk about like also the modeling side where Zarbid said there are these massive headwinds against local models where one things that seems to move towards MOEs which like one benefit is maybe there are more memory bandwidth bound which plays in favor of local versus using GPUs or using Nvidia GPUs but the downside is these models are just bigger in total and you know they're going to need to fit often not even on a single node the multiple nodes there's no way that's going to fit inside of even really good MacBooks and I think especially for coding it's not a question as much of like does it clear some bar of like the models good enough to do these things and then like we're satisfied which may be the case for other other problems and maybe we're local models shine but people are always going to want the best the most intelligent the most capable things and that's going to be really really hard to run for almost all people locally. Don't you want the most capable model like you want you want Sonnet you and also with oh I like how you're pitching me would you be satisfied with an inferior model I listen I yeah I'm yes I'm one of those but there's some people that like to do stuff locally especially like yeah that really there's a whole obviously open source movement that kind of resists and it's good that they exist actually because you want to resist the power centers that are growing are there's actually an alternative to local models that I am particularly fond of I think it's still very much in the research stage but you could imagine to do homomorphic encryption for language model inference so you encrypt your input on your local machine then you send that up and then the server can use loss of computation they can run models that you cannot run locally on this encrypted data but they cannot see what the data is and then they send back the answer and you decrypt the answer and only you can see the answer so I think that's still very much research and all of it is about trying to make the overhead lower because right now the overhead is really big but if you can make that happen I think that would be really really cool and I think it would be really really impactful because I think one thing that's actually kind of worrisome is that as these models get better and better you're going to become more and more economically useful and so more and more of the world's information and data will flow through you know one or two centralized actors and then there are worries about you know there can be traditional hacker attempts but it also creates this kind of scary part where if all of the world's information is flowing through one node in plaintext you can have surveillance in very bad ways and sometimes that will happen for you know initially will be like good reasons like people will want to try to protect against like bad actors using AI models in bad ways and then you will add in some surveillance code and then someone else will come in and you know you're in a slippery slope and then you start doing bad things with a lot of the world's data and so I I'm very hopeful that we can solve homeomorphic encryption for language modeling and doing privacy preserving machine learning but I would say that's the challenge we have with all software these days it's like there's so many features that can be provided from the cloud and all this increasingly rely on it and make our life awesome but there's downsides and that's why you rely on really good security to protect from basic attacks but there's also only a small set of companies that are controlling that data you know and they obviously have leverage and they could be infiltrated in all kinds of ways that's the world we live in yeah I mean the thing I'm just actually quite worried about is sort of the world where means the entropic has this responsible scaling policy and where we're on like the low ESLs which is the entropic security level or whatever of like of the models but as we get you like couldn't cut ESL 3 ESL 4 whatever models which are sort of very powerful but for for mostly reasonable security reasons you would want to monitor all the prompts but I think I think that's that's sort of reasonable and understandable where where everyone is coming from but Matt it'd be really horrible if sort of like all the world's information is sort of modern that heavily it's way too centralized it's like sort of this like really fine line you're walking where on the one side like you don't want the models to go rogue on the other side like men's humans like I don't know if I trust like all the world's information to pass through like three three model providers yeah what do you think it's different than cloud providers because I think this is a lot of this data would never have gone to the cloud providers in the in the first place where this is often like you want to give more data to the EA models you want to give personal data that you would never have put online in the first place to these companies or to these models and it also centralizes control where right now for for cloud you can often use your own encryption keys and it like it just can't really do much but here is just centralized actors that see the exact plaintext of everything on the topic of context that that's actually been a friction for me when I'm writing code you know in Python there's a bunch of stuff imported there's you could probably intuit the kind of stuff I would like to include in the context is there like how hard is it to auto figure out the context it's tricky I think we can do a lot better at computing the context automatically in the future one thing that's important to notice there trade-offs with including automatic context so the more context you include for these models first of all the slower they are and the more expensive those requests are which means you can then do less model calls and do less fancy stuff in the background also for a lot of these models they get confused if you have a lot of information in the prompt so the bar for accuracy and for relevance of the context you include should be quite high but this is already we do some automatic contexts in some places within the product it's definitely something we want to get a lot better at and I think that there are a lot of cool ideas to try there both on the learning better retrieval systems like better embedding models better re-rankers I think that there are also cool academic ideas you stuff we've tried out internally but also the field is grappling with writ large about can you get language models to a place where you can actually just have the model itself like understand a new corpus of information and the most popular talked about version of this is can you make the context when it's infinite then if you make the context when it's infinite can you make the model actually pay attention to the infinite context and then after you can make it pay attention to the infinite context to make it somewhat feasible to actually do it can you then do caching for that infinite context you don't have to recompute that all the time but there are other cool ideas that are being tried that are a little bit more analogous to fine tuning of actually learning this information in the weights of the model and it might be that you actually get sort of a qualitatively different type of understanding if you do it more the weight level then if you do it at the in context learning level I think the journey the jury's still a little bit out on how this is all going to work in the end but in the interim us as a company we are really excited about better retrieval systems and picking the parts of the code base that are most relevant to what you're doing we could do that a lot better like one interesting proof of concept for the learning this knowledge directly in the weights is with the s code so we're in a vs code fork and vs code the code is all public so these models in pre training have seen all the code they've probably also seen questions and answers about it and then they've been fine tuned and hourly shift you'll be able to answer questions about code in general so when you ask a question about the s code you know sometimes it'll hallucinate but sometimes it actually does a pretty good job at answering the question and I think like this is just by it happens to be okay but what if you could actually like specifically trainer post train model such that it really was built to understand this code base it's an open research question one that we're quite interested in and then there's also uncertainty of like do you want the model to be the thing that end to end is doing everything i.e. it's doing the retrieval and its internals and then kind of answering a question creating the code or do you want to separate the retrieval from the frontier model where maybe you know you'll get some really capable models that are much better than like the best open source ones in a handful of months and then you'll want to separately train a really good open source model to be the retriever to be the thing that feeds in the context to these larger models can you speak a little more to the post training a model to understand the code base like what do you mean by that with is this synthetic data direction is this yeah I mean there are many possible ways you could try doing it there's certainly no shortage of ideas it's just a question of going in and like trying all them and being empirical about which one works best you know one one very naive thing is to try to replicate what's done with the s code and these frontier models so let's like continue pre training some kind of continued pre training that includes general code data but also throws in a lot of the data of some particular repository that you care about and then in post training meaning in let's just start with instruction fine tuning you have like a normal instruction fine tuning data set about code but you throw in a lot of questions about code in that repository so you could either get ground truth ones which might be difficult or you could do what you kind of hinted at or suggested using synthetic data i.e kind of having the model ask questions about various pieces of the code so you kind of take the pieces of the code then prompt the model or have a model propose a question for that piece of code and then add those as instruction fine tuning data points and then in theory this might unlock the modus ability to answer questions about that code base let me ask you about open i oh one what do you think is a role of that kind of test time compute system in programming i think test time compute is really really interesting so there's been the pre training regime which will kind of as you scale up the amount of data and the size of your model get you better and better performance both on loss and then on downstream benchmarks and just general performance we'll use it for coding or or or or other tests um we're starting to hit a bit of a data wall meaning it's going to be hard to continue scaling up this regime and so scaling up 10 test time compute is an interesting way of now you know increasing the number of inference time flops that we use but still getting like like yeah as you increase the number flops use inference time getting corresponding uh improvements in in the performance of these models traditionally we just had to literally train a bigger model that always uses uh that always used that many more flops but now we could perhaps use the same size model um and run it for longer to be able to get uh an answer at the quality of a much larger model and so the really interesting thing i like about this is there are some problems that perhaps require a hundred trillion parameter model intelligence train on a hundred trillion tokens um but that's like maybe one percent maybe like point one percent of all queries so are you going to spend all of this effort all this compute train a model uh that cost that much and then run it so infrequently it feels completely wasteful when it said you get the model that can that is that you train the model those capable doing the 99.9 percent of queries then you have a way of inference time running it longer for those few people that really really want max intelligence how do you figure out which problem requires what level of intelligence that possible the dynamic you figure out when to use gpt4 when to use like wait when to use a small model and when you need the the oh one i mean yeah that's that's an open research problem certainly uh i don't think anyone's actually cracked this model routing problem quite well uh we'd like to we we have like kind of initial implementations of this for things for something like cursor tab um but at the level of like bit going between four oh sonnet to oh one uh it's a bit trickier perhaps like there's also a question like what level of intelligence do you need to determine if the thing is uh oh too hard for the for the four level model maybe you need the oh one level model um it's really unclear because but you mentioned that so there's uh there's a there's a pre-training process and there's post-training and there's like test time compute that fair to sort of separate where's the biggest gains um well it's weird because like test time compute there's like a whole training strategy needed to get test time to compute to work and the really the other really weird thing about this is no one like outside of the big labs and maybe even just open AI knowing really knows how it works like there've been some really interesting papers that uh show hints of what they might be doing and so it perhaps they're doing something with research using process reward models but yeah i just i think the issue is we don't quite know exactly what it looks like so it would be hard to kind of comment on like where it fits in i would put it in post-training but maybe like the compute spent for this kind of for getting test time compute to work for a model is going to dwarf pre-training eventually so we don't even know if oh one is using just like chain of thought or we don't know how they're using any of these i don't know anything it's fun to speculate like if you were to build a competing model what would you do yeah so one thing to do would be i think you probably need to train a process reward model which is so maybe we can get into reward models and outcome reward models versus process reward models outcome reward models are kind of traditional reward models that people are trained for these for for language models language modeling and it's just looking at the final thing so if you're doing some math problem let's look at that final thing you've done everything and let's assign a great to it how likely we think like what's the reward for this this this outcome process reward models instead try to grade the chain of thought and so open AI had some preliminary paper on this i think last summer where they use human lablers to get this pretty large several hundred thousand data set of grading chains of thought um ultimately it feels like i haven't seen anything interesting in the ways that people use process reward models outside of just using it as a means of affecting how we choose between a bunch of samples so like what people do uh in all these papers is they sample a bunch of outputs from the language model and then use the process reward models to grade uh all those generations alongside maybe some other heuristics and then use that to choose the best answer the really interesting thing that people think might work and people want to work is research with these process reward models because if you really can grade every single step of the chain of thought then you can kind of branch out and you know explore multiple paths of this chain of thought and then use these process reward models to evaluate how good is this branch that you're taking yeah when the quality of the branch is somehow strongly correlated with the quality of the outcome at the very end so like you have a good model of knowing which branch you take so not just this in the short term in like in the long term yeah and like the interesting work that I think has been done is figuring out how to properly train the process or the interesting work that has been open source in people I think uh talk about is uh how to train the process reward models um maybe in a more automated way um I could be wrong here could not be mentioning some people so I haven't seen anything super uh that seems to work really well for using the process reward models creatively to do tree searching code um this is kind of an AI safety maybe a bit of a philosophy question so open AI says that they're hiding the chain of thought from the user and they've said that that was a difficult decision to make they instead of showing the chain of thought they're asking the model to summarize the chain of thought they're also in the background saying they're going to monitor the chain of thought to make sure the model is not trying to manipulate the user which is a fascinating possibility but anyway what do you think about hiding the chain of thought one consideration for open AI and this is completely speculative could be that they want to make it hard for people to distill these capabilities out of their model it might actually be easier if you had access to that hidden chain of thought uh to replicate the technology um because that's pretty important data like seeing seeing the steps that the model took to get to the final result so you could probably train on that also and there was sort of a mirror situation with this with some of the large language model providers and also this speculation but um some of these APIs used to offer easy access to log probabilities for other tokens that they're generating um and also log probabilities of for the prompt tokens and then some of these APIs took those away uh and again complete speculation but um one of the thoughts is that the reason those were taken away is if you have access to log probabilities um similar to this hidden chain of thought that can give you even more information to to try and distill these capabilities out of the APIs out of these biggest models into models you control as an asterisk on also the the previous discussion about uh us integrating a one i think that we're still learning how to use this model so we made a one available in cursor because like we were when we got the model we were really interested in trying it out i think a lot of programmers are gonna be interested in trying it out but um uh oh one is not part of the default cursor experience in any way up um and we still haven't found a way to get integrated into an editor into the editor in a way that we we we reach for sort of you know every hour maybe even every day and so i think that the jury's still out on how to how to use the model um and uh i we haven't seen examples yet of of people releasing things where it seems really clear like oh that's that's like now the use case um the obvious one to try to turn to is maybe this can make it easier for you to have these background things running right to have these models and loops to have these models be a gen tech um but we're still um it's a little scuffering to be clear we have ideas we just need to when you try and get something incredibly useful before we we put it out there but it has these significant limitations like even like barring capabilities uh it does not stream and that means it's really really painful to use for things where you want to supervise the output and instead you're just waiting for the wall text to show up um also it does feel like the early innings of test time computing search where it's just like a very very much of e0 um and there's so many things that like like don't feel quite right nice respect um in parallel to people increasing uh the amount of pre-training data and besides the models and pre-training and finally for xterre you'll now have this other thread of getting searched to work better and better so let me ask you about strawberry tomorrow eyes so it looks like get hub uh copilot might be integrating a one in some kind of way and i think some of the comments are saying this this mean cursor is done i think i saw one comment saying that i thought i'd time to shut down cursory shut down cursor i think this piece is a little bit different from past software spaces over the the 20 times um where i think that the ceiling here is really really really incredibly high and so i think that the best product in three to four years will just be soon much more useful than the best product today and you can like wax poetic about modes this and brand that and you know this is our uh advantage but i think in the end just if you don't have like if you stop innovating on the product you will you will lose and that's also great for startups um that's great for people trying to to enter this market um because it means you have an opportunity um to win against people who have you know lots of users already by just building something better um and so i think yeah over the next few years it's just about building the best product building the best system and that both comes down to the modeling engine side of things and it also comes down to the editing experience yeah i think most of the additional value from cursor versus everything else out there is not just integrating the new model fast like a one it comes from all of the kind of depth that goes into these custom models that you don't realize are working for you in kind of every facet of the product as well as like the really uh thoughtful UX with every single feature all right uh from that profound answer let's descend back down to the technical you mentioned you have a taxonomy of synthetic data oh yeah uh can you please explain yeah i think there are three main kinds of synthetic data the first uh so what is synthetic data first so there's normal data like non synthetic data which is just data that's naturally created i.e. usually it'll be from humans having done things so from some human process you get this data synthetic data the first one would be distillation so having a language model kind of output tokens or probability distributions ever tokens um and then you can train some less capable model on this this approach is not going to get you a net like more capable model than the original one that is produced the tokens um but it's really useful for if there's some capability you want to elicit from some really expensive high latency model you know that distill that down into some smaller task specific model um the second kind is when like one direction of the problem is easier than the reverse and so a great example of this is bug detection like we mentioned earlier where it's a lot easier to introduce reasonable looking bugs than it is to actually detect them and this is this is probably a case for humans too um and so what you can do is you can get a model that's not training that much data that's not that smart to introduce a bunch of bugs and code and then you can use that to then train use a synthetic data to train a model that can be really good at detecting bugs um the last category I think is I guess the main one that feels like the big labs are doing for synthetic data which is um producing text with language models that can then be verified easily um so like you know extreme example of this is if you have a verification system that can detect if language is Shakespeare level and any of a bunch of monkeys typing and type writers like you can eventually get enough training data to train a Shakespeare level language model and I mean this is the case like very much the case for math where verification is is is actually really really easy for formal um formal languages and then what you can do is you can have an okay model uh generated a ton of rollouts and then choose the ones that you know have actually proved the grand truth here and there I'm just gonna train that further uh there's similar things you can do for code with lead code like problems or uh where if you have some set of test that you know correspond to if if something passes these tests it is actually solved a problem you could do the same thing where you verify that it's passed the test and then train the model and the outputs that have passed the test um I think it's gonna be a little tricky getting this to work in all domains or just in general like having the perfect verifier feels really really hard to do with just like open-ended miscellaneous tasks you get the model or more like long horizon tasks even encoding that's because you're not as optimistic as arvain but yeah uh so yeah so that that that that third category of a quiz having a verifier yeah verification is it feels like it's best when you know for a fact that it's correct and like then it like it wouldn't be like using a language model to verify it'd be using tests or uh formal systems or running the thing too doing like the human form of verification where you just do manual quality control yeah yeah but like the the language model version of that where it's like running the thing it's actually understands the app but yeah no that's sort of somewhere between yeah yeah I think that that's the category that is um most likely to result in like massive gains what about RL with feedback side RLHF versus RLIF um what's the role of that in um getting better performance on the models yeah so RLHF is when the reward model you use uh is trained from some labels you've collected from humans giving feedback um I think this works if you have the ability to get a ton of human feedback for this kind of task that you care about RLIF is interesting uh because you're kind of depending on like this is actually kind of uh going to this it's depending on the constraint that verification is actually a decent bit easier than generation because it feels like okay like what are you doing you're using this language model to look at the language model outputs and then prove the language model but no it actually may work if the language model uh has a much easier time verifying some solution uh then it does generating it then you actually could perhaps get this kind of recursive but I don't think it's going to look exactly like that um the other the other thing you could do is that we kind of do is like a a little bit of a mix of RLIF and RLHF where usually the model is actually quite correct and this is in the case cursor tap at picking uh between like two possible generations of what is what is what is the better one and then it just needs like a hand a little bit of human nudging with only like on the on the order of 50 100 uh examples um to like kind of align that prior the model has with exactly with what what you want it looks different than I think normal RLHF were usually usually training these reward models and tons of examples. What's your intuition when you compare generation and verification and generation and ranking is is ranking way easier in generation. My intuition would just say yeah it should be like this is kind of going back to like if you if you believe p does not equal np then there's this massive class of problems that are much much easier to verify given a proof than actually proving it. I wonder if the same thing will prove p not equal temp or p equal temp. That would be that would be really cool. That'd be of whatever feels metal by AI who gets the credit. None of their open philosophical question. I'm actually surprised. I'm actually surprisingly curious what what what like a good bet for one one AI will get the feels metal will be actually don't have a mon specialty. I don't know what a mon sped here is. Oh sorry Nobel Prize or feels metal first. Feels metal. Feels metal level. Feels metal. He should solve. He should think. Feels metal comes first. Well you would say that of course. It's also this like isolated system. You know, or if I and you know, sure. Like I don't even know if I don't need to do so. I feel like I have much more to do there. I felt like the path to get to IMO was a little bit more clear because it already could get a few IMO problems and there are a bunch of like there's a bunch of low-hing fruit given the literature at the time of like what what tactics people could take. I think I'm one much less first in the space of theorem proving now and to yeah less intuition about how close we are just solving these really really hard open problems. So you think it'll be feels metal first. It won't be like in physics or in. Oh 100%. I think I think that's probably what we like to be like it's probably much more likely to get then yeah. Well I think it puts to like I don't know like BSD which is a BERT's winter and diet conjecture like we want iPods or any one of these like hard hard math problem researchers it's actually really hard. It's sort of unclear what to pass you to get even a solution looks like like we don't even know what a path looks like let alone. And you know by the idea that this is like an isolated system and you can actually you have a good reward system and it feels like it's easier to train for that. I think we might get feels metal before the age of I think I mean I'm you're very happy. I'm you're very happy. But I don't know if I I think 20 20 H 20 30. What feels metal feels metal. All right. It feels like forever from now giving how fast things have been going. Speaking of how fast things have been going let's talk about scaling laws. So for people who don't know maybe it's good to talk about this whole idea of scaling laws. What are they? What do you think stand and where do you think things are going? I think it was interesting the original scaling laws paper by OpenAI was slightly wrong because I think of some issues they did with learning rate schedules and then Chinchilla showed a more correct version. And then from then people have again kind of deviated from doing the compute optimal thing because people start now optimizing more so for making the thing work really well given a given inference budget. And I think there are a lot more dimensions to these curves than what we originally used of just compute number of parameters and data like inference computers is the obvious one. I think context length is another obvious one. So if you care like let's say you care about the two things of inference compute and and then context window. Maybe the thing you want to train is some kind of SM because they're much much cheaper and faster at super super long context. And even if maybe it is 10x worth scaling properties during training many have spent 10x more compute to train the thing to get the same level of capabilities. It's worth it because you care most about that inference budget for really long context windows. So it'll be interesting to see how people kind of play with all these dimensions. So yeah let me just pick to the multiple dimensions obviously the original conception was just looking at the variables of the size of the model as measured by parameters and the size of the data as measured by the number of tokens and looking at the ratio of the two. Yeah. And it's kind of a compelling notion that there is a number or at least a minimum and it seems like one was emerging. Do you still believe that there is a kind of bigger is better? I mean I think bigger is certainly better for just raw performance and raw intelligence and raw intelligence. I think that the path that people might take is I'm particularly bullish on distillation and like that yeah how many knobs can you turn to if we spend like a ton ton of money on training like get the most capable cheap model right like really really caring as much as you can because like the the naive version of caring as much as you can about inference time compute is what people have already done with like the Lama models or just overtraining the shit out of 70 models on way way way more tokens than essential obstacle right but if you really care about it maybe think to do is what gamma did which is let's just not let's not just train on tokens let's literally train on minimizing minimizing the KL divergence with the distribution of gamma 27b right so knowledge distillation there and you're spending the compute of literally training this 27 billion model billion parameter model on all these tokens

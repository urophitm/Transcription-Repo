 You just realized that your business needed to hire someone yesterday. How can you find amazing candidates fast? Easy. Just use Indeed. Because when it comes to hiring, Indeed is all you need. Indeed's Sponsored Jobs will help you stand out and hire fast. With Sponsored Jobs, your post jumps to the top of the page for your relevant candidates so you can reach the people you want faster. And it makes a huge difference. According to Indeed data, Sponsored Jobs posted directly on Indeed have 45% more applications than non-sponsored jobs. There's no need to wait any longer. Speed up your hiring right now with Indeed. And listeners of Mindscape will get a $75 sponsored job credit to get your jobs more visibility at Indeed.com slash Mindscape. Just go to Indeed.com slash Mindscape right now and support our show by saying you heard about Indeed on this podcast. Indeed.com slash Mindscape. Terms and conditions apply. Hiring, Indeed, is all you need. Here's a tip for growing your business. Get the VentureX Business Card from Capital One and start earning unlimited double miles on every purchase. That's right. With unlimited double miles, the more your business spends, the more miles you earn. Plus, the VentureX Business Card has no preset spending limit, so your purchasing power can adapt to meet your business needs. The VentureX Business Card also includes access to over 1,000 airport lounges. Just imagine where the VentureX Business Card from Capital One can take your business. Capital One. What's in your wallet? Terms and conditions apply. Find out more at CapitalOne.com slash VentureX Business. Did you know Fast Growing Trees is the biggest online nursery in the U.S. With thousands of different plants and over 2 million happy customers. They have all the plants your yard needs, like fruit trees, privacy trees, flowering trees, shrubs, and so much more. Whatever plants you're interested in, Fast Growing Trees has you covered. Find the perfect fit for your climate and space. Fast Growing Trees makes it easy to get your dream yard. Order online and get your plants delivered directly to your door in just a few days, without ever leaving home. We've just received a white dogwood tree. We've not had a chance to plant it yet, but are very excited to do so. This spring, they have the best deals for your yard, up to half off on select plants and other deals. And listeners to our show get 15% off their first purchase while using the code MINDSCAPE at checkout. That's an additional 15% off at FastGrowingTrees.com using the code MINDSCAPE at checkout. FastGrowingTrees.com, code MINDSCAPE. Now's the perfect time to plant. Use MINDSCAPE to save today. Offers valid for a limited time. Terms and conditions may apply. Hello, everyone, and welcome to the MINDSCAPE podcast. I'm your host, Sean Carroll. There's a kind of history myth that sometimes gets promulgated in, I don't know, elementary schools, maybe, or just folktales we tell each other. According to which, when the first European explorers landed in the New World, the indigenous folks saw them and thought, oh my goodness, these are gods coming to visit us, and we need to worship them, and they're too powerful to deal with. Turns out, nothing like that is actually true. This is a story that the Europeans made up after the fact to make themselves look good, to justify some of the things that happened. Nowadays, we are being faced with a new set of visitors from another world, namely artificial intelligences. Whether it's large language models or some other kind of constructed program that in many ways can act human, but has a different set of capacities, and we're learning to deal with them. And unlike the myth of the European explorers landing in the Western Hemisphere, today, there are a bunch of people who, quite literally, who are very willing to say that these are gods coming to deal with us. I know there's also plenty of skepticism out there, but there are people who think not only that AIs are going to be in our human-level intelligence and agency, but well beyond that, superhuman, godlike creatures that we're going to have to deal with. I am myself not of that opinion. I do not think that that is actually what is going on. But just like the landing explorers, AIs do have different capacities than we do. They're trained, of course. They're designed. They're made to, in many ways, act very human, but they're really not. They're thinking in a different way. They're capable of some things much better than we are, and other things not nearly as good as we are. So how do we think about this world in which interacting with AIs, interacting with computerized systems more broadly, is going to be a crucially important part of how we live our lives? Today's guest is Tina Eliassi-Rod, who is a computer scientist whose work spans the space, and this is why I really like it, from very technical stuff, just, you know, how do you better detect certain nodes or communities in an abstract network that you have embedded in some sort of data, but then also the human side of how you deal with this stuff, how these computer systems, how these AIs are going to affect our lives, and we're going to affect them, all the way up to human-AI co-evolution. You know, once we build these systems, and then we interact with them, and then we use them to decide how to go shopping or decide how to find a romantic partner, guess what? That affects who we are, how we live our lives, and the survival strategies we're going to have to move forward in this very brave new world. Again, many positive aspects here. There are things that, you know, we don't want to do, we don't want to bother doing, or as hard to do for us as human beings that we can outsource to the AIs. There are other ways in which it's very dangerous. The biases, the bad things that we have in our own brains can be inherited by the AIs, and they can have new failure modes that we human beings don't have. It's a world that is changing super-duper rapidly, obviously, as a lot of research is coming in and a lot of influences are out there. It's not all about necessarily writing the best program. Some people who are very good at writing programs want to optimize for making the best money, right? And we have to take that into consideration when we consider what to do, how to regulate, how to control, how to optimize for our own actual goals, rather than just seeing what happens next and living with the consequences. So the more informed we are about what the possibilities are and how to deal with them, the more we'll be able to do that. So let's go. Tina Eliassi-Rod, welcome to the Mindscape Podcast. Thank you. Thank you for having me. You know, normally I like to start the conversation with someone, you know, talking about like the most basic stuff, the things everyone knows about. For your stuff, I kind of feel like going in reverse order. Like, you know, we'll end with the fun stuff about AI and democracy and things like that. But let's start with understanding graphs and networks and things like that, especially using neural networks to understand things that human brains can't quite wrap their minds around. So like, what is the most general way of stating what it is that you're trying to understand when it comes to thinking about graphs and networks? Well, when you're trying to understand the phenomena, usually you have multiple entities, like multiple people, and they have relationships with each other, right? And so when we're looking at graph, like machine learning with graphs or graph mining, we're trying to find those what we're calling relational dependencies. That like the probability of you and me being friends, given that we both like Apple products, is greater than the probability of you and me just being friends. Okay. Right. Or the probability of me liking Apple products, given that we're friends, is more than the probability, the prior probability of each of us liking an Apple product. So the second one that is, you are, we are friends, you influence me. And so I like Apple products and I buy Apple products, or I buy this headphone, right? Headset. And the first one is that because we like similar things, we become friends. This notion of homophily or like birds of a feather flock together. But in a nutshell, like people who work on machine learning on graphs, network scientists, who are interested in understanding phenomena, network science is an interdisciplinary discipline. Um, it is about these relational dependencies and like, what can we find? What are the patterns? What are the anomalies in the relationships that get formed? So for the audience who wasn't there, uh, what Tina is not telling you is that we spent 10 minutes before the podcast struggling with our Apple products to make the recording work, but we still use them. So, you know, I guess take whatever lessons from that. Okay. But I guess in the current era, the issue is you have too much data, or at least in principle, one would like to imagine having too much data. There's like so much stuff, right? Is, is a large part of the worry, like how to pick and choose what to pay attention to, what to draw connections between. Yeah, there, there's some of that. I would say that, so I have this thing I call the paradox of big data, which is like, there's a lot of data, but to predict specifically for what Tina wants, it's difficult, right? You don't have maybe as much information about Tina. Now, if Tina belongs into some majority group, then maybe you can aggregate from the majority and say, well, Tina is part of this flock. And so Tina will like whatever this flock likes. Um, but really I feel like the problem these days is more about, uh, exploitation and going with things that are popular, um, than, um, exploration, right? Like in the past, we would go to the library or the bookstore and you're looking for a book and you would find other things. And those were, you know, they, they basically the, the, um, the, the cherry on top of the cake, right? The cream is like, Oh yeah, I found this. Right. And now we're really not getting that. Right. So when you use all these recommendation systems, whether it's Google or any other, um, Amazon, et cetera, um, they oftentimes show you what is popular or what they, they believe you would like. And so in a past life, I worked at Lawrence Livermore national laboratory, which is a physics laboratory. And like when I would do searches there, and this is many years ago, um, I would get more like physics books than like when I lived elsewhere, they would sell me, they wouldn't show me as much physics books, right. Just based on the location, the zip code. And so there's some of that that's going on. And I feel like that is more of the problem of like, not really serving the individual or exploring, uh, as much as possible. So thinking though, like purely like a mathematician or a computer scientist, um, faced with these big networks, how should we think about them? What are the tools that we use to, to tease out what are the important relationships? Yeah. So, you know, it depends on what kind of network it is, right? So in social networks, for example, we know that there are two dominant processes that form social networks. One is closing of what we're calling wedges. So if I am friends with you and you are friends with Jennifer, then I will become friends with Jennifer, right? We close that triangle. And in fact, if you and I have, for example, many common friends, or let's say me and Jennifer, in my example, we have many common friends and we are not friends, then there is something going on that there was lots of opportunities that we could become friends, but we chose not to become friends, right? Now there's also of course, partial observability and that like, maybe I didn't observe it, right? However big your data is, you're not omniscient. You don't see things, but we do expect that, uh, friend of a friend is also a friend. That's one. The other one is this notion of preferential attachments, right? That everybody wants to connect to a star. And so, um, you're interested in, uh, in like, the, basically those are the two big patterns. And then you look at deviations from that. So a work that was done by, um, John Kleinberg at Cornell. He's a very well-known computer science professor. This is a while back was, um, think Facebook, for example, who is your romantic partner on Facebook? And he, he and his colleagues showed that basically you are the center of a flower and you have petals around you. These petals could be your high school buddies or college buddies, et cetera. They have just more triangles in them. And people who fall outside of these petals and have a lot of connections to these petals are either your sibling or your romantic partner. That is, you are introducing them to other facets of your life. Ah, yes, okay. And they showed that when that connections, the, um, stopped, establishment of those connections stopped, it's a leading indicator that you will break up. Uh-oh. So, uh, so you were talking about which connections to pay attention to, right? It's like, so those are some of the things that are fun when you look at social networks. I mean, biological networks are totally different. So in biological networks, it's a whole other ball of wax. And there's not like, you're not looking for common friends. You're looking more for like complementarity between different proteins that serve some function. So it's interesting because it seems like an attempt to go from syntax to semantics in some sense, right? You're going from structure to meaning, broadly speaking. You're trying to understand what is going on. What is the underlying process that is happening in this network and why these links exist? Now, the one thing that makes studying of graphs and networks really interesting is that, um, it is not a closed world. So just because like you didn't see a link between me and Jennifer doesn't mean that we're not friends. And so for machine learning where you need both positive examples and both negative examples, which negative examples do you pick becomes difficult because the edges or the links or the friendships that don't exist may, because like they don't want to be friends or for other reasons. And so this, what are the negative examples becomes an important aspect of things. Well, or as you were giving the example, I was thinking, I don't interact with my romantic partner on social media that much because we interact in real world. Like we don't need that. Indeed. Indeed. So there are lots of assumptions being made, obviously in terms of like how the network is being observed. And in fact, this is one of the big differences between computer scientists that study graphs and network scientists that are typically, uh, physicists or a social scientist where, for example, they're like, well, there's a distribution and this graph fell from it versus like the machine learning, uh, graph mining folks typically don't question where the graph came from. They're like, oh, here's data and they run with it. Right. And it's just, it, it boggles the mind that like, you should think about where this data came from, how it was collected, what were maybe the errors in collecting it. Um, and if I, this touches on a, uh, sore point for me, because what happens is they don't question the data, right? They just like feed it into their machine learning AI models. And then on the other end, they don't measure any uncertainty. So like, if you have something like, let's say a social network that you've observed, there's all this stuff about like representation learning, right? Where basically I take Tina in the social network and I represent her as a vector in a Euclidean space, right? Like maybe with, uh, 60,000, uh, a vector with, uh, 16,000 elements in it. So the cardinality is 16,000. And there's no uncertainty. They're like, no, Tina falls exactly here. And it just doesn't make sense at all. Right. And so then those kinds of models, given that you didn't start with, okay, well, my data could have some noise in it, some uncertainty in it. And then you don't even capture the uncertainty of the model at the end. It just, there are lots of problems that can occur, including, for example, um, at, at the serial attacks or like, um, your model is not just going to be, you're not, your model is not going to be robust. Let's just put it that way. A lot of us start the new year saying that we will learn a new language, but it's hard to actually commit to it. Vaple makes it easy to learn one in less time than you think. Vaple's quick 10 minute lessons, handcrafted by over 200 language experts, get you to begin speaking your new language in three weeks or whatever pace you choose. And because conversing is the key to really understanding each other in new languages, Vaple is designed using practical real world conversations. What I love about Vaple is you can either dive in deeply and truly get fluent, or you can just master some of the basics before going on a trip. So let's get more of you talking in a new language. Vaple is gifting our listeners 60% off subscriptions at Vaple.com slash Mindscape. Get up to 60% off at Vaple.com slash Mindscape, spelled B-A-B-B-E-L dot com slash Mindscape. That's Vaple.com slash Mindscape. Rules and restrictions may apply. Well, this sounds just like full employment for enthusiastic graduate students, right? Because how hard could it be? I mean, it could be hard, but it's very well defined, the problem that you just set out. I mean, allow for the existence of noise in these descriptions and see how your answers change. Yeah, I think in part, one of the reasons that folks, at least in the CS side, the computer science and the machine learning side, aren't too bothered by it these days, is because we are going through this era where prediction is everything. Prediction accuracy is everything. And so, you know, there are these benchmarks and it's basically benchmark hacking or state of the art hacking, right? And that's basically what is going on. You know, that's the reality of it. You know, and, and so, so there's a lot of that kind of engineering going on as opposed to like really thinking about, well, what is the phenomena that I'm interested in? How is the data coming to me? What are the sources of noise? Should I, how should I take them into account? Should I even take them into account? And what are the uncertainties in terms of the predictions that I'm outputting? Let's help the audience understand the idea of benchmark hacking, because that's probably a cool, but important one. I mean, what's a benchmark and how do you hack it? Yeah. So basically you create a bunch of data and you get a buy-in from the community that these are good data sets to test a machine learning or an AI model on. And then there's a leaderboard and you want to be number one, right? And so you hack the systems that exist or you hack your own system, you create your own to be number one, you know, as, as much as possible. Um, and that's basically what is going on. And I like this metaphor. So my colleague, um, Barabashi said, it's like, there are two camps. There's like a toolbox. It's a finite toolbox, right? And the machine learning, the AI people, the engineers put tools into that toolbox. And because it's finite, it's very competitive. That is my tool beats your tool. Even if it's like 1% by 1% that it's not clear if it's statistically significant or not. And I may be king for only 30 seconds because another tool comes in. Right. And then there's like the scientists on the other end that just opened the toolbox and say, okay, well, what is good for whatever, you know, whatever prediction task I want to do. And then they pick a tool out of that. And so a lot of this like benchmark hacking or state of the art hacking happens on the engineering, on the AI machine learning side, the computer science side, because you want your tool in that finite toolbox. But on the science side, the physicist or social science side, uh, the people who are interested in these models that create the sets of data you have, there's also, as I understand it, um, a lot of worry about degeneracy or, or overdetermination or underdetermination where, very different physical models could give you essentially the same kind of graph or network. How, how big of a problem is that? Um, it is a very big problem. I mean, there, there are multiple angles to this. So one is, for example, because of all the hype, oftentimes people on the engineering side, don't talk about the assumptions that they have made or, you know, the technical limitations of, um, their system. And in fact, because of that, we have this reproducibility problem. So not even a replicability problem, but a reproducibility problem, which is just a code. Can I just reproduce your code as you have it? Right. And even with your training data, even with like how you broke it up with these different like folds or whatever, you know? And so, which is like very, very, uh, a very low bar to pass, but that doesn't happen because there are lots of assumptions that are being made, et cetera. And then there's this notion of, um, you know, we, we are living through this era of like big models, right? So I want to model that has many, many, many parameters, you know, even if I don't need all those, all those many parameters, or for example, maybe I do care about, um, interpretability. That is, I want to know what the model is actually doing. Um, but because again, for that, uh, one or two percentage point on the prediction side, you let go of it and, you know, you go with the bigger models, but yes, it's a big, big problem of, you know, for me, like the lowest bar would be, um, that we require, at least with federal funding, you know, and in some of the service that I do for the federal government, I've been pushing this. I'm not going to be a very popular person, but that if you get taxpayer dollars in your reports to the government, you have to have a section on assumptions and technical limitations. Um, because the problem is the way the peer review culture goes is that if I have a technical limitation section in my paper, the reviewer will just copy and paste it and say reject. Yep. Right. But the federal government isn't going to do that. Right. And NSF isn't going to do that. NSF has already given you the money and you're doing the annual report. And so it has to be, come on, just be honest. Yeah. Right. Like I did not test this method on biological networks and they're very different than social networks. So like caution. Right. Well, this is because what you do for a living matters a lot to the real world and to money and things like that. Unlike the foundations of quantum mechanics that I do. Like I don't, I don't need to worry about people being overly concerned with the results. It's a, they're all, they're all willing to give me a hard time anyway. Okay. So, um, one, I have this sort of philosophical mathematical problem. I don't know. I mean, if I have a graph, a big graph, so some nodes, some edges that are relationships and I have a different graph, how is there, are there measures of similarity between them? Like if I add one node to the graph, is it a completely different graph or is there a metric I could put on there? How much is that even understandable? Yeah. I love that problem. I've, I've thought about that problem. What the, so the issue there is similarity is an eye of the beholder, right? And it depends on the task itself. So similarity is an ill-defined problem. And so you can say, okay, well, I can go with something like an edit distance. Like, okay, how many new nodes do I have to add? To graph number two. And how many new edges do I have to add or remove to make it look like the other graph and then try to solve the computationally hard problem of isomorphism. In fact, alignment, right? And in many cases you don't need alignment. So for example, you can think about two networks and you have started a process of information diffusion on it. Like you started a rumor, let's say, right? And you would just measure like how similar does this rumor, the same rumor travel through network one versus network two. And if like, you know, it travels similarly, let's say, you know, I'm going to throw some jargon, like the stationary distribution of a random walker that is spreading. This rumor becomes the same at the end. You would say the networks are similar enough, right? And so you don't need to have like the sizes exactly be the same. So it could be, for example, you have a social network of France and a social network of Luxembourg, and you started a rumor in France and in Luxembourg, and they are processing the same way. And you would say the networks are similar, even though one is much, much bigger than the other. That makes sense. In fact, because that, because I was going to ask about when you have a big graph and you somehow coarse grain it, right? Or, you know, you group subgroups into single nodes. You want to somehow have the feeling that it's still representing the same thing, even though you've thrown away a lot of information. Yeah. Yeah. Now, the problem with like grouping nodes, this is a very important problem, and it's been studied by lots of people. Within like graphs, it's called community detection. Basically, you want to group similar nodes together. Now, you can have different functions that you define about what similarity there means. It could mean that like these people just talk to each other more, right? So there's more connections between them than what you would expect in a random world, right? Or just more connections between them than other folks. Now, this kind of community detection, Aaron Closet, who's a professor at Colorado, showed that there's a no free lunch theorem there. And actually it was Aaron Closet and others. And I think actually Aaron was the last author. So I think the first author is little peel, but you know how it is. You usually just name your friend. Yeah, I do know. My apologies to the other authors, but they showed it in no free lunch theorem, which basically means that it is not the case that there's like one particular group of, or one particular collection of nodes that you're grouping that would give you the best or the, the true communities. You see what I mean? So because when you are doing these grouping of nodes, you have some objective function that you're trying to maximize. And basically the idea is that there is no one peak there. So there's not like one particular community that you can put Tina on and say, okay, Tina belongs here. That's where she has to sit. And so they become some of, some of that becomes an issue, but this notion of what is, what is, what does it mean for one network to be similar to another network is, has its tentacles to community detection, to clustering of nodes, and all of those are ill-defined. So it really is driven by the task at hand. Okay. I mean, I guess I'm spoiled by caring about what probably in your world would be the simplest possible case. Cause I think about, you know, the emergence of space from some set of quantum entanglements or something like that. And it sounds all very fancy and highbrow, but basically something is entangled with something else if it's next to it. And there's this very similar spatial or a very simple minded spatial coherence. But of course in social networks, I can be connected to people anywhere that makes it a more complicated problem. Yeah. And that becomes the, what we call the small world problem, right? That you, or the Kevin Bacon or the, or the, um, Erdush number, right? Like you, that you don't have to go that far out to be connected, uh, to famous people. And so, I mean, how good are we these days at detecting real clusters, uh, communities, uh, figuring out, what's going on just from knowing about a graph and the connections between the nodes? I mean, for downstream tasks that you can like, uh, have some, uh, let's say confusion matrix where you can draw like true positives, false positives, true negatives, false negatives. We're actually very good at it. But if it's about like, okay, I found these communities and do, and do those, these communities make sense. Um, it kind of breaks down into whether they're like hard clustering, where you put Tina into just one community or you put Tina into multiple communities. And then there's a little bit of just like eyeballing it in a way. If you do not have this downstream task that you can say, okay, here are the true positives, here are the false positives, and so on and so forth. Um, but any, in many cases, it's difficult to place, uh, a person in a social network only in one community because people are multifaceted. Right. Right. But, and you, you started with, uh, example of being given recommendations by Amazon or whatever. And sometimes it, it, the algorithm fails because it's not picking up our idio, idio, individual idiosyncrasies. It's just giving us the most popular thing. And is that tie in to the well-known problem of, um, polarization or, uh, extremization of network recommendations? Like you're, you're, everyone is pushed to some slightly more extreme set of YouTube videos or Reddit posts or whatever. I think they're, um, in part, they just want your attention. And so the objective function is such that, you know, they just want to hold your attention. And so they will show you whatever necessary that will, will keep your attention. And so if they believe that like my tie to Brandon is very strong, that we have a strong relationship, and Brandon found these things interesting, then they will show it to me as well to just test it, to see whether, you know, uh, they can capture my attention. And then through that, they can show me more ads, for example. I guess that makes perfect sense. So like the point is, uh, if Amazon wants to recommend things to me, it's not maximizing the chance that I want this. It's maximizing its profit. Exactly. Exactly. And so they kind of go hand in hand. And in fact, this touches on, um, this issue that we have written, uh, a couple of times about, there was a nature perspective piece, uh, a while back and more recently an AI journal piece on, uh, in a way like human AI co-evolution. So if you think about it, when you're using Amazon, when you're using YouTube, when you're using Google, you're providing data for them. We talked about this, right? And they take that data into account and they make recommendations. Those recommendations then affect what you do in, in the real life. And then you go back and you provide them more training data. And so there's this kind of feedback loop that goes on and on. And it's oftentimes not captured in terms of who's influencing who most. And, uh, one example that I like here is like, think about dating apps. There was a story recently from Stanford that like most people are meeting on online, uh, dating apps these days instead of like through college or through their friends, family, et cetera, or at the local bar. Now those dating apps have recommendation systems, right? And based on those recommendation systems, perhaps you meet somebody, you partner up and you have babies. And so over time, these recommendation systems actually have an impact on our gene pool. Oh, okay. Yeah. I had not quite gotten that far. Right. Yeah. But it's like, as opposed. And so, and because these recommendation systems are all about exploitation and not exploration, but maybe you would say like my aunt or my grandmother, or my college were also all based on exploitation and not exploration. Right. But there is this, this notion that there are these algorithms that we can't understand what they're doing. And perhaps a hundred years from now, they may, uh, influence how our gene is, how our genome is evolving. Well, we are part of the world and we create the world and it reflects back on us. Right. I mean, it reminds me a little bit of discussions about, uh, extended cognition theories where, you know, you count your calculator, and your pad of paper and whatever is part of your brain because you, you keep information there, you do calculations, et cetera. And so our, our environment and who we are is being increasingly populated by these artificial algorithms that we put out there. Yeah. I don't know, like how far do we think certain things are going and this society has to design it. Like for example, New York times had this article a while back about how this, uh, there's a person who's trying to, uh, set up a company and online dating company where like on the first or second dates, which are usually, you know, not very good. my avatar and your avatar will go on the date and then they will report back. And only if, you know, both avatars are happy. Then on the third date, we actually go out on the date. And so like how much of actually our human behavior are these things going to take over? It's a, so I didn't see this article. Do you, what's your actual opinion? Is there any chance that that would help? I think like I'm an introvert. So I'm like, well, and also I'm a computer scientist. I'm like, Oh, this is great. Let somebody else do the dirty work. And then maybe, you know, if it's a good day, I'll get out of my cave and I'll like go and talk to, but you know, I, I, for extroverts, they don't like it at all. So my husband was an extrovert. Like, what is, what are you talking about? Am I just a brain in a vat now? Like what's happening? You know? So I think it depends on where you are in this extrovert. We should also reveal to the audience that Tina has the good or bad fortune of being married to a philosopher. Indeed. Indeed. For 30 plus years, it's been fantastic. So, yeah. So the evolution, I mean, I was going to get that later, but it's so good. We have to talk about it now. Co-evolution of humans and AI. And my guess was when I heard that phrase, we were thinking more about cultural evolution, right? Memes more than genes, but of course they're interconnected with each other. Now that you say it, it's obvious because our cultural effects of our behavior, our behavior affects how we pass genes onto the next generation. So AI is going to be affecting the population genome of, of human beings. Yeah. And I think in particular with, for example, generative AI, as it's generating content, whether it's texts or video or images, there's this notion. And the late Dan Dennett, who you had on your podcast, very famous cognitive scientists, called these generative AI models, counterfeit people. Yeah. He had an Atlantic article a few years back about it. And so, and also because people treat these generative AI systems, these counterfeit people, as if they're more objective somehow, they know more than me that, you know, people tend to give their agency to them. And so, and also these AI systems evolve faster than us. And so it's not quite clear, not that it's a race, but it's that they're evolving a lot quicker. Yeah. Their objective functions are different, like attention, money, et cetera, than perhaps the objective function of people, like maybe the good of the society or public good or something else than just like money or some like GDP or some, some, some measure like that. This episode of Mindscape is sponsored by BetterHelp. When it comes to relationships, we often hear about the red flags we should avoid. But what if we focus more on looking for the green flags in friends and partners? If you're not sure what green flags look like, therapy can help you identify them, actively practice them in your relationships, and embody the green flag energy yourself. Whether you're dating, married, building a friendship, or just working on yourself, it's time to form relationships that love you back. One of the great things about therapy is by looking inside yourself, you can both learn to take those warning signs seriously, but also learn to be open to new experiences and new things, to know when something might be worth pursuing. BetterHelp is a fully online service that makes therapy affordable and convenient, serving over 5 million people worldwide. You can easily switch therapists anytime at no extra cost. So discover your relationship green flags with BetterHelp. Visit BetterHelp.com slash Mindscape today to get 10% off your first month. That's BetterHelp, H-E-L-P dot com slash Mindscape. Are we good enough that we could at least imagine some kind of new equilibria that we get into when we're tightly coupled with our AIs, that there is some happier state of being we could at least aim for if we're working together well, or is it too much in flux these days to know much about that? I think these days it's too much in flux, but I think, for example, there are certain things that can be done to improve it. Whenever you or another human being asks me a question, perhaps I would come back with another question. I'm like, did you mean this, Sean? Or did you mean that, right? But for example, with Charity PT or these large language models, they never come back and say, like, did you mean this? The reason is that it reduces their utility, right? Me as a human being, when I ask the question, I want an answer and I want it now, right? Or like it never comes back and says, I don't know, or I'm not sure of it. And maybe you would accept that from a human being, but you don't accept it from a large language model. You're like, oh, you're a tool. You need to tell me. Like, I asked you about this and I want the answer now. And, you know, and so there's some of that going on. But like the big tech companies could add those features to make it more equal in terms of this conversation that is going on. But at this point, utility is winning over all this other thing. But utility is tricky. You know, I was talking with ChatGPT or whatever the other day and I was trying to get it to imagine, and maybe I didn't try too hard. I don't, you know, I didn't really put that much effort into it. But like I was trying to imagine a character in a fictional narrative who was very insulting and who would, you know, give out some good insults. And I said, what are some good insults that it could give out? But it wouldn't tell me. It's like, oh, no, you shouldn't give out insults. You should talk to people politely. Like it's clearly programmed not to go down that road. But yes, there are actually other generative AI systems, especially for programming, that I've heard where like, it tells you like, okay, if you want to code X, this is how you code it. And then you code it and you're like, oh, it didn't work. That was, you were stupid to the generative AI. Like the human says, you're stupid. And then the human, the generative AI says to the human, you're not a good programmer. You know, and so then there's some kind of a, you know, then they get at it. It's in a loop. But that's only like for, you know, specific ones. You're absolutely right. With chat GPT, it's not going to be that kind of antagonistic. And I know, I mean, this is probably related to the big worry that a lot of people have had about bias in AI algorithms. I mean, if you've trained, well, if you train AI on human discourse and human beings are biased, then of course the algorithm is going to be biased. It's not because the computer is biased. It's because you've trained it on data that is. And is that something that your tools can help us deal with? I mean, you can try to find biases. I mean, there's a lot of work in that, like these large language models are sexist, misogynist. We wrote a report for UNESCO for last year's International Women's Day about how sexist and misogynist these large language models are. The problem was that is whenever like I get somebody asks me that question that, oh, well, look, humans are biased too. The problem is that I can hold a human accountable. I can sue a human being. Who am I going to sue? You know what I mean? And especially in America, we're very litigious. And so then this gets into accountability. And in fact, there's a lot of work in the government. For example, our government is putting a lot of our tax dollars into like trustworthy machine learning, trustworthy AI, etc., etc. And to me, it rings a little hollow because there's no accountability. Like, how can I trust you if there's no accountability? I feel like they go hand to hand. And so there's some of that going on, which is like, you know, who am I going to sue? Am I going to sue open AI because it's sexist and misogynist? Like one of its products is sexist and misogynist? You know, that's not the case right now. Well, and human beings, I mean, this is an ongoing cultural flashpoint. So, I mean, there's a lot of different opinions about it. But human beings might at some point think of something to say that we know is inappropriate and then we're smart enough or we have enough controls that we don't say it. Is that a kind of thing that it makes sense to try to implement in the context of a large language model? Perhaps, right? The thing is, at this point, what it gives out is what's the most probable and what it believes you will like, right? So it's a two-place function, what's probable and what you will like. But yes, you could definitely do that. And there's this comedian, unfortunately, I forget his name now, but he was saying the secret to a long marriage is to never say what comes to your mind first or second. Always say the third thing that comes to your mind, right? And this goes back to what you were just saying. Maybe you should just say this third thing, the third most probable thing. And in fact, along those lines, usually the students who use these generative AI tools for math problems, math homeworks, the first answer is usually wrong. Because a lot of the answers that have been uploaded into Course Hero, et cetera, et cetera, they're wrong. Usually it's the second answer that's the correct answer. Oh, that's very interesting. Is that actually true or is that like a feeling that people have? These are just anecdotal, right? Like I haven't had anybody do like a systemic study of this, but that like usually the first answer is not quite there. Well, it's interesting because one of the things we discover, you discover, we in the Royal We, thinking about these very, very large data sets is a sort of, sometimes you can predict even more than maybe you thought you'd be able to. I mean, I want to ask you about this paper that you wrote about using sequences of life events to predict human lives. That sounds interesting, but also maybe scary. Yeah. Yeah. So in the true like computer science, AI, machine learning sense, we're very good at coming up with names for our systems. So we called it Life2Vec. So we're just putting your life into a vector space, whether you like it or not. Yeah, that's okay. But you're just a vector in this vector space. Now, basically the idea is that if you look at these large language models, right? So they're analyzing sequences. And so as human beings, we also have a life story. That's a sequence, right? And so I was lucky enough to work with a group of scientists in Denmark. So if America has a surveillance capitalism, in Denmark, they have surveillance socialism. So there is a department there, Department of Statistics, they call it, like Ministry of Statistics that collects information about people. And so we had information for about 6 million people who have lived in Denmark from 2008 to 2020. And we were like, well, can we write stories for these people in a way? And then feed it to what is the heart of these large language models, a transformer model, which is basically just the architecture of a neural network that learns association weights for within some context window. And that's what we did. So, but instead of, so for example, chat GPT goes online and gobbles up all this bad data that people have put in, all the misogynistic sexist data. We didn't do that. So we had very good data from this department of statistics, and we created our own artificial symbolic language. And then we fed that artificial symbolic language for these like 6 million people into a transformer model. And we, then we were able to like predict life events. And so one of them that caught the media's eye was, will somebody between the age of 35 and 65 pass away in the next four years? Yeah. And we picked that, that age range because that's a harder age range to predict for. Like if you're over 65, then it's easier to predict whether you're going to pass away in the next four years. And if you're younger than 35, it's also easy. The other way, right? You're, you're unlikely to pass away. And so that's one of the things. So the other prediction task was like, will you leave Denmark? You know, so then you can predict for that. But it had this similar technology as these large language models, which is like you have this one, what they call like predefined, where you just learn based on the data that you have, what's likely to happen next. And then you fine tune it for whatever prediction tasks that you have. What does it mean in artificial symbolic language, like literally a human language or it's like some logical encoding? It's a logical encoding because it, because the data that the Department of Statistics has in Denmark is all tables. So it is not like this kind of sequence. So then you could say like Tina was born in Copenhagen in December, blah, blah, blah. Right. And we could like generate like a natural language, but that's difficult. Why would we do that? So then we generated a vocabulary for this artificial symbolic language. And then, and that was actually a lot of the intellectual property of the work is like, okay, well, how do you take these tables and then create this artificial symbolic language that then you could give to a transformer model? And, and, and what's the answer? Are we likely to die if we're 38 years old? How do we know? Well, the thing, the thing that we found, which was very interesting, I think, so like the accuracy in terms of the model was about like 78%, et cetera. And I think that's why people were showing a lot of interest in it. Um, but to me, that wasn't really the takeaway. The takeaway actually was that labor data is a very good indication of whether somebody in that age range is going to pass away in the next four years or not, because health data is very noisy and inconsistent. So even in Denmark, where they have universal healthcare, it's not like everybody goes to the doctor all the time and you have good data for them. And so some of the, some of the, uh, indicators of like, whether you're going to pass away, one was whether you're male. We know this, right? Males tend to do more crazy things than females. Oh yes, I can jump over this ravine. No problem. Right. And then, um, the other stuff was basically just what, which sector you were working in. Right. So if you're like an electrician, it's a bad thing. It's not a very good thing. Right. Like as opposed to like an office worker. Right. So some of the, so the labor data was actually very, very helpful, um, than the health data. How important is it to extract causality from these relationships? Like maybe risky or minded people just become electricians. Yeah, maybe. Yeah. We didn't do any, any, any kind of causal stuff, right? Like a lot of the work, a lot of the hype that's happening now in AI and machine learning, they're all on the correlation side, not on the causation side. So we didn't look at that at all about what causes what that's very difficult. And I haven't touched, um, the field of causation in part because I'm married to a philosopher. And so it's like, no, like I ain't going there. Because every time I try to approach the topic, I just heard nightmares. And so I haven't, I haven't gone that way yet. There are some issues there. Yeah, no, absolutely. But I guess, I mean, it's interesting. Is it, is it too much to draw a general lesson that by looking at these large data sets, we might find simpler indications of what we're looking for than we expected? Like, you know, you might, like, you might have said, okay, uh, how many calories is somebody ingesting is the important thing to look at. But then you look at the data and you learn, no, what is their job? That's what's the important thing to look at. Yeah, I think, um, that there's some of that. I think the best way of using this is perhaps government policy, right? When government issues a policy and then like maybe 20 years from that, you have, if you have good data, you could see, okay, what has been some of the correlations that have come about based on this policy? And then maybe, you know, the actual social scientists and political scientists can then draw some causal diagrams from what we find. Um, because the one thing is usually like from the computer science, AI machine learning, we treat causation and correlation as a binary, right? As it's like a coin this way or that way, but that is really not the case, right? It's more of a spectrum. And so if you have a model that is producing robust predictions, there, there is some underlying causal model. You just don't know it. And then maybe, um, that could, um, steer you into the right direction for that kind of work. But we didn't, we didn't look at that for this particular work. So, um, yeah. Human beings, of course, are examples of complex systems themselves. Uh, but this raises the, you know, the larger question of human beings will eventually die for whatever reason. Complex systems have their lifespans, right? Or I mean, maybe they're infinite, I don't know, but they can also change dramatically and die. And that's something else you're interested in trying to tease out in a general way. Yeah. And I am very interested in the feedback that we were talking about and like, how do we capture that feedback between, for example, when I go and I'm using Amazon and Amazon is making me these recommendations and then I buy things, I tell my friends, and then all of that data goes back into Amazon and like, how much does like my contributions or my friends contributions amplifying what Amazon is doing. And so, um, there's some of that going on. And then there's also in terms of like society is a complex system and the place of these tools in these systems. So, um, the tools that help us spread misinformation and disinformation make our society unstable. Um, in that then you're not quite sure, uh, what you are reading is true or not. Right. So right now with the fires in LA, there's a lot of misinformation and disinformation going on. And it's like, who do I believe? And maybe like you believe LA times and you, you believe, you know, um, what you read in CA.gov and so on and so forth, but not what you're seeing on Instagram. And so there's this notion of the place of these AI tools within our society and whether they're making our society better or worse. And by better or worse here, I mean, stable versus not stable, more chaotic. And I think we can all agree that we would like to live in societies that are more stable than not. Right. Yeah. So it, so, so there's some of that that is going on. Um, and I have a new project along those lines, which actually touches on philosophy, which is called epistemic instability, which is what are some stability conditions of what you know? So if you genuinely know that whales are mammals, no matter what I show you, um, perhaps I won't be able to convince you that a whale laid an egg. You're like a whale is a mammal and mammals do not lay eggs. Right. And you're very sure about it. Right. But then you start talking to me and to chat GPT. And maybe if you don't know something, then you're like, as, as well as you thought, right. Then I, then you're malleable, right. Then I can like change your mind. And then now you have groups of people who are talking to these within themselves and with these generative AI tools. And then basically you go from like individual to groups to this hypergraph notion. And, uh, what I'm interested in is when are phase transitions in this hypergraph in terms of what the society believe, like maybe the society believe that vaccines are good. Right. And now all of a sudden the society doesn't believe the vaccines are good. Right. And what are the leading indicators of those kinds of phase transitions in our society as it's being modeled by conversations formally represented as these hypergraphs. Thumbtack presents the ins and outs for caring for your home. Out. Indecision. Overthinking. Second guessing every choice you make. In. Plans and guides that make it easy to get home projects done. Out. Beige on beige on beige. In. Knowing what to do, when to do it, and who to hire. Start caring for your home with confidence. Download Thumbtack today. Yeah. I mean, I guess that's a, it's a good example. I hadn't quite thought of the vaccine thing. The traditional example that I hear for sort of a social phase transition is opinions about gay marriage. Right. Where it was universally against. It somewhat rapidly changed to generally four. But this is, the vaccine stuff is more subtle. Right. Because it's not that the whole society is going against them, but about half or whatever. Right. There's this political polarization and there's sort of more than one consensus being built up. Is that, is that just my impression or is there some idea that the modern informational ecosystem lets us have these sub, larger sub communities where they have their own sets of beliefs different from other communities? Yeah. I think it's the second one in that, like in the past, when you did have people that tend to be on the fringe, they would, people wouldn't hear them. But now, even if you're on the fringe, because of the information technology that we have, you can connect to other people who are on the fringe and then you believe, oh no, we're bigger than the fringe. Yeah. We're actually in the middle. Right. And then that kind of a thing spreads. So, so that is one of the things I'm interested in regarding gay marriage. One of the things that was interesting is I was talking to a philosopher who had just taught for a very long time at the Ohio State University and he, and he was teaching ethics and issues related to gay marriage and abortion, et cetera. And he was saying that with gay marriage, similar to what you were saying, he saw a shift in terms of opinions for or against gay marriage, mostly for, but he didn't see any change when it came to abortion. And I think that had to do with the vagueness of when is, let's call it the thing, a baby, right? When is the actual fetus a baby or whatever, you know? And so, and that vagueness, cause like we could all agree that maybe like the day before you're about to give birth, obviously you're not going to do anything. We all believe it's a baby, but that vagueness is something that doesn't shift the opinion on, on abortion so much for, yes. And I like that vagueness aspect of it. So there are certain things that are vague and maybe you will never have that kind of phase transition. Okay. And then there are certain things like the vaccine where like there are people in the fringe that our information technology allows them to connect to each other. And so it feels like a bigger thing. And then maybe there are other aspects of information that really do make people change their mind just based on talking to other people. And so they're not as sure or as stable in their knowledge. So I like the hypothesis that the vagueness of the proposition makes it harder to have a phase transition. How would we test that hypothesis? Is that something that we can sort of sift through the data and figure out whether or not that's on the right track? So it's a work in progress right now for us on this. I'm trying to stay away from making it a psychology or a social science problem because then you get all these confounding factors. And that's what I said, it has more tentacles to philosophy. Yeah. So in terms of what people ought to do in terms of their knowledge and how sure they are of their knowledge. And so right now, the way that we're representing the knowledge or like what you know these things as vectors, because I'm a computer scientist. Everything's a vector. It's okay. It's all in your algebra. So basically, how much does this vector space move in one direction versus another? So as you talk with others, so you can build these like kind of simulations, right? Right. Not kind of. You can build these simulations in terms of conversations and see how much the vector space shifts. So, I mean, one thing about complex systems is they can survive a long time. Like the human body, you know, fends off attacks pretty well because it's complex enough to catch things. The other thing is that they can sort of go into this wild negative, positive feedback loop, I guess, and crash, right? Like the economy or something like that. So is this something, maybe this question is too vague, but are we learning general purpose lessons about complex systems concerning what features they need to be stable versus what features they need to be delicate? Yeah. So there's a book by Ladyman and Wiesner. And I know that you had James Ladyman on your podcast as well. He's a philosopher at Bristol and Caroline Wiesner is a mathematician at Potsdam now about what is a complex system. And their book that came out, I think, in 2020 talked about complex systems in terms of features and how there are certain like necessary features and there are certain like emergent features. And then there's some functional features where like, for example, our human brain is a complex system. And as you were saying, like if it has a shock, it adapts and it's still perhaps can function unless the shock is like catastrophic. And so what we are not seeing if we tie this to, for example, the AI models and how they are operating within this system is we don't know even the role of this AI system. Like how much instability is it causing in the system? Right. How much feedback is it causing in the system? How much memory does it have? Right. Because they're evolving so quickly that it's not it's not quite clear. So this is like an open area of study of like going through these different features of a complex system and trying to see, OK, well, how do I measure it for, let's say, a chat GPT? Right. Yeah. In fact, a lot of people say, oh, well, you know, it doesn't have a good memory based on like what I told it yesterday. Kind of a thing. Right. So memory is one of those features that a complex system has. OK. So I guess, you know, and one of the important applications here that you have talked about explicitly is democracy. Right. Democracy is a complex system and democracies do fail sometimes. And I guess one way of putting the worry is that, or at least the the interest, is that the introduction of AI as a new feature, in some sense, opens the possibility of a new instability. It could it could lead to sort of a runaway disaster that destroys democracy, not to put it in too alarmist terms. Yeah, I think where it comes and in fact, this is how it links to my new project on epistemic instability is that it introduces epistemic instability. Right. Right. Like when my dad was getting his Ph.D. in America back in the 60s, the most trusted man in America was Walter Cronkite. Right. If he said something, you believed him. Now we don't have such a thing. Yeah. Right. We don't have a person or an institution where you say, OK, I read it here and I believe it. And then there's also like depending on where you are on the left or the right, you're like maybe you you believe New York Times, you believe Fox News. And so because of that, I feel like one of the things that we need to do if we value our democracy is teach our kids critical thinking. Right. It's just like don't believe what you read or what you hear. Question it. Right. Does it make sense? Talk to different people and make your own decision and don't give up your agency. But that's a hard task. Right. Thinking is not easy. And people don't want to think in the age of TikTok. Well, is that true? I mean, maybe it is true. I'm certainly willing to believe that's true. But again, I always worry about comparing eras. Right. Because I was a different person in the 70s and the 70s were also a different time. But I don't know what things are common between different eras and things are not like. Did we really want to think more back in the 1970s than we did in the TikTok era? I don't know. I think there was less distraction for sure. Right. Than it is now. I think the dopamine hit that we get by just scrolling through Instagram, TikTok, etc. is something that has been studied. And I'm not a psychologist or a cognitive scientist, but that people, it's just like you let your brain go to motion. You just spend hours on it instead of maybe actually sitting quietly and thinking about a problem. You know, it's boring. Yeah. Okay, good. So this is another aspect. So, okay, that's actually nice. Despite not really trying to, I think that I see a bunch of threads coming together here. Like technology broadly, not just AI, is giving us new ways to fulfill our own objective functions. Maybe it's a dopamine hit or whatever. But its objective function might not be ultimately our flourishing. So there's an absolutely danger mode there. Yeah. In fact, that's such a perfect thing. You said, I always say to my students, what is your objective function? Right. Because we all have an objective function. And that objective function changes over time. Right. And perhaps if like all of us just think, okay, did my objective function change from yesterday or from last month or whatever, you know, it would be helpful for society. So as a computer scientist, as a machine learning person, I always think about objective functions. And in fact, I cannot look at a mountain range now and not think, okay, if you drop me there, will I find the peak or not? The global peak? Probably not. But, you know, like, please drop me at a nice place. You've co-evolved with your network. That makes perfect sense to me. Yeah. So the gradient is with me. Exactly. Exactly right. So, okay. So you've said many things about this already, but I just want to get it as clear as possible. The trust, the community of trust idea that is so central to a democracy is one of the things that is in danger of being undermined by AI, right? Like, you probably saw the story about Instagram having its AI accounts. The sassy black lesbian lady who, you know, was programmed by a bunch of people who were near, neither black nor lesbian and just pure AI. And that one was admitted, right? Like, they said that was AI. Right. And do you personally worry that people are just going to mostly become friends with non-existent human beings in the long term? I mean, as an introvert, I'm fine with it. But, you know, yeah, no, I think we see this in society now where, like, people aren't as good as interacting with other people or they're not as courteous to other people. Perhaps as before. I don't know. Maybe I'm of an age now where I'm like, oh, yeah, people are not as courteous as they were before. But, you know, the more you interact with people, the better you get at them. Unless you interact with them, the worse you get at them. And so if we don't put a premium on, like, oh, look, like, Tina can actually pick up the phone and call somebody and get something done, you know, as opposed to just, like, sending a zillion emails or text messages. I think there's a value to that. And I think there is this notion of trust. Like, even the most introvert among us, right, there are a few people that we do trust. And so if it comes to a point where you trust an AI system that we don't know how it works and that it's vulnerable to attacks, then that is a problem. Right. And so, in fact, this gets us to this phrase called the red teaming that we hear all the time now. Oh, well, don't worry about it. They will red team it. And so the phrase red teaming came from the Cold War era. Right. So the Soviet Union, the red team, America, the blue team. Right. So and there was a lot of this red team, blue teaming, for example, for cybersecurity. But this phrase red teaming is not well defined when it comes to these generative AI systems. And my friend and colleague, Professor Hoda Hederi at Carnegie Mellon, has written extensively about this because there's no guarantee. Right. So you cannot guarantee that somebody cannot jailbreak chat GPT. And jailbreaking is basically that chat GPT has put in some kind of guardrails. Right. Like you shouldn't it shouldn't tell you how to like rob a bank, but you can jailbreak that and it will tell you how to rob a bank. Right. But there's no guarantees. It's not like, oh, here's a theorem, the proof. QED, go home. You cannot jailbreak this. And so if you're getting all of your information from these AI systems that we know can be manipulated and we don't know how they exactly work, then you may not have a shared reality with other citizens. And that's, I think, the worst for democracy. We really do need a shared reality to be able to withstand our democracy, to hold it and not lose it. So how do we get that? What do we do? This sounds very scary, but I'm not quite sure what to do about it. Well, I guess as a professor, to me, it's education. Yeah. I think actually, you know, educating the public. And I spend a lot of my time educating the general public and not just the students at my university, but educating the public about how these tools work, what they're good at, what they're not good at, not giving their agency to these tools, and critical thinking skills. I think that that's the way forward. But the problem with that, of course, is that the value of getting an education is also susceptible to this loss of trust. I don't know if you saw the recent—people were getting upset because there was a poll that showed that young men were becoming less and less interested in going to college. But then someone else pointed out that if you go into the cross tabs, if you look at, you know, other questions that were asked, there's actually no relationship between male and female versus going to college. It's all about Republican versus Democrat. It's that there are more—it's a Simpsons paradox kind of thing where most of the young Republicans are male, and those are the ones who have become very polarized against wanting to go to college. So that's part of the problem you've been talking about, right? Like there's a whole new epistemic community out there that is forming, and it seems to be solidifying over time. Yeah. Perhaps we should think about how we educate people, and maybe they'll see the value of education, right? And that education is about enlightenment. Education is about empowering yourself, right? So education isn't like a teacher just pouring knowledge into your head. It's about you learning about the world, and so you could do better in the world. You know, like as a teacher, I'm already 11 on my guitar, right? I just want you to do better, and if you do better, then I will also do better. The society will do better, and we will all do better, right? And so I think part of that is maybe we should rethink about how we sell education. Do you think that AI and associated technologies can be a force for good in education? Yeah, I think so. I mean, there are certain things that I have heard. So, for example, now, there's some privacy aspects to this, but if you are a college and you are tracking how students are doing on their homeworks, etc., and let's say Tina took calculus and she didn't do very well on differential equations, and now she's taking machine learning and they're going to talk about differential equations, that you could tell Tina, oh, you know, maybe you should go brush up on differential equations. Because they're going to talk about differential equations. Yeah, okay. You know, so there's some of that kind of a thing to help you. And then there's also basically personalized tutoring that I think AI can be helpful there. Do you yourself use ChatGPT or something equivalent to help figure things out, to learn about things? I use it for fun. Like, you know, like, give me a bio of Sean Carroll in the King James style or whatever. You know, just for, I don't use it. I haven't used it for any real, like, work stuff or anything. I actually... I don't trust it. That's the problem. You don't trust it. I certainly don't trust it. But sometimes, you know, I did realize that there was a good use case because I was trying to understand, you know, in mathematical things, they will often tell you true things, but you don't understand what the point of it is, right? And I was trying to understand type 3 von Neumann algebras. And so I asked, and I got ChatGPT to explain to me not just what the definition was, but why it was important in this particular case. And that was actually very helpful. Oh, that's great. Yeah. Oh, that's great. Yeah. I asked it some stuff about linear algebra and matrix norms, and I was really bad at it. And I was like, wait, what? Like, there's so much about linear algebra. I think that's... You should know about matrix norms. There's no problem. There's too much, like you just said. I mean, there's too much junk out there. In some sense, if you get technical enough that it knows about it, but not so technical, all the stuff that's been written about it is sensible. Like, no one's going to make up stuff about type 3 von Neumann algebras. What would be the point? Exactly. Yeah, yeah. So I guess maybe the point is, let's not teach linear algebra to kids. No, no, no. Because the whole of machine learning is basically linear algebra. It's all linear algebra and quantum mechanics also. So yeah. Linear algebra, kids. That's your lesson for today from Mindscape. Learn more linear algebra, I think. It's the key to everything. Yeah, exactly. Exactly. But it's very good at, like, basically admin stuff. So if you, like, show it some, like, picture of, like, Google Scholar, like, put it into BibTech, put these references into BibTech, it does it for you. So some of those kind of admin stuff it's good at. Yeah, I think that the weird thing is we're trying to use it for creative work, whereas the most obvious use case is for the least creative things that we don't want to do. Indeed. Indeed. Yeah. All right. It's all very complex, and it's evolving, and there's a lot of degrees of freedom. So Tina Eliazirad, thanks very much for helping us all figure it out. Thank you. Thank you for having me on, Sean. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.
The following is a conversation with George Hots. He's the founder of Kama AI, a machine learning based vehicle automation company. He is most certainly an outspoken personality in the field of AI and technology in general. He first gained recognition for being the first person to carry our on-lock an iPhone. And since then, he has done quite a few interesting things at the intersection of hardware and software. This is the Artificial Intelligence Podcast. If you enjoy it, subscribe on YouTube, give it 5 stars on iTunes, support it on Patreon, or simply connect with me on Twitter. Alex Friedman spelled F-R-I-D-M-A-N. And I'd like to give a special thank you to Jennifer from Canada for her support of the podcast on Patreon. Merci beaucoup Jennifer. She's been a friend and an engineering colleague for many years since I was in grad school. Your support means a lot and inspires me to keep this series going. And now here's my conversation with George Hots. Do you think we're living in a simulation? Yes, but it may be unfalseifiable. What do you mean by unfalseifiable? So if the simulation is designed in such a way that they did like a formal proof to show that no information can get in and out. And if their hardware is designed for the anything in the simulation to always keep the hardware in spec, it may be impossible to prove whether we're in a simulation or not. So they've designed it such that it's a closed system you can't get outside the system? Well, maybe it's one of three worlds. We're either in a simulation which can be exploited. We're in a simulation which not only can't be exploited, but like the same thing is true about VMs. A really well designed VM, you can't even detect if you're in a VM or not. That's brilliant. So the simulation is running on a virtual machine. But now in reality, all VMs have ways to detect. That's the point. I mean, you've done quite a bit of hacking yourself. So you should know that really any complicated system will have ways in and out. So this isn't necessarily true going forward. I spent my time away from comma. I learned a cock. It's a dependently typed like it's a language for writing math proofs. And if you write code that compiles in a language like that, it is correct by definition. The types check its correctness. So it's possible that the simulation is written in a language like this. In which case, yeah, but that can't be sufficiently expressive a language like that. Oh, it can. It can be. Yeah. Okay. Well, so, yeah. All right. So the simulation doesn't have to be tiring complete if it has a scheduled end date. Looks like it does actually with entropy. I mean, I don't think that a simulation that results in something as complicated as the universe would have a formal proof of correctness. Right? It is possible. Of course, we have no idea how good their tooling is. And we have no idea how complicated the universe computer really is. It may be quite simple. It's just very large, right? It's very, it's definitely very large. But the fundamental rules might be super simple. Yeah. Conway's going to live kind of stuff. Right. So if you could hack, so imagine simulation that is hackable. If you could hack it, what would you change about the environment? How would you approach hacking a simulation? The reason I gave that talk. I have it, by the way, I'm not familiar with the talking game. I just read the talked about escaping the simulation or something like that. So maybe you can tell me a little bit about the theme and the message there too. It wasn't a very practical talk about how to actually escape a simulation. It was more about a way of restructuring an us versus them narrative. If we continue on the path we're going with technology, I think we're in big trouble. Like as a species, and not just as a species, but even as me as an individual member of the species. So if we could change rhetoric to be more like to think upwards, like to think about that we're in a simulation and how we could get out already. We'd be on the right path. What you actually do once you do that, while I assume I would have acquired way more intelligence in the process of doing that. So I'll just ask that. So the thinking upwards, what kind of ideas, what kind of breakthrough ideas do you think thinking in that way could inspire? And what did you say upwards? Upwards, into space, I used to think in sort of exploration in all forms. Do you space narrative that held for the modernist generation doesn't hold as well for the postmodern generation? What's the space narrative? I was talking about the same space, the theater match on the space. Don't know the space, like going out space, like building like Elon Musk, like we're going to build rockets, we're going to go to Mars, we're going to colonize the universe. And the narrative you're referring to the race to space. The race to space. Yeah, explore. Okay. That was a great modernist narrative. It doesn't seem to hold the same weight in today's culture. I'm hoping for good postmodern narratives that replace it. So think let's think. So you work a lot with AI. So AI is one formulation of that narrative. There could be also, I don't know how much you do in VR and AR. Yeah. That's another, I know less about it, but every time I play with it and our research is fascinating, that virtual world. Are you interested in the virtual world? I would like to move to a virtual reality in terms of your work. No, I would like to physically move there. The apartment I can rent in the cloud is way better than the apartment I can rent in the real world. Well, it's all relative, isn't it? Because others will have very nice apartments too, so you'll be inferior in the virtual world. That's not how I view the world. I don't view the world. It's a very like almost zero summation weight of view the world. Say my great apartment isn't great because my neighbor has one too. My great apartment is great because look at this dishwasher, man. You just touched the dish and it's washed. That is great in and of itself if I have the only apartment or if everybody had the apartment. I don't care. So you have fundamental gratitude. The world first learned of Geoha, George Hots in August 2007, maybe before then, but certainly in August 2007 when you were the first person to unlock carry on lock and iPhone. How did you get into hacking? What was the first system you discovered vulnerabilities for and broken to? So that was really kind of the first thing. I had a book in 2006 called Greyhat Hacking. I guess I realized that if you acquired these sort of powers, you could control the world. But I didn't really know that much about computers back then. I started with electronics. The first iPhone hack was physical. You had to open it up and pull an address line high. It was because I didn't really know about software exploitation. I learned that all in the next few years. I got very good at it. But back then, I knew about how memory chips are connected to processors and stuff. We knew about software and programming. He didn't know. You have view of the world and computers as physical. It was hard work. Actually, if you read the code that I released with that in August 2007, it's atrocious. Well, language was in a C. And in a broken state machine, I didn't know how to program. Yeah. So how did you learn to program? What was your journey? I mean, we'll talk about it. You've live streamed some of your programming. This is a chaotic, beautiful mess. How did you arrive at that? Years and years of practice. I interned at Google after the summer after the iPhone unlock. I did a contract for them where I built a hardware for Street View. I wrote a software library to interact with it. It was terrible code. And for the first time, I got feedback from people who I respected saying, don't write code like this. Of course, just getting that feedback is not enough. The way that I really got good was I wanted to write this thing that could emulate and then visualize like arm binaries because I wanted to hack the iPhone better. I didn't like that. I couldn't see what that I couldn't single-step through the processor because I had no debugger on there, especially for the low-level things like the boot run and the boot loader. So I tried to build this tool to do it. And I built the tool once and it was terrible. I built the tool second times. It was terrible. I built the tool third time. This was by the time I was at Facebook. It was kind of okay. And then I built the tool fourth time when I was a Google intern again in 2014. And that was the first time I was like, this is finally usable. How do you pronounce this, Kira? Kira, yeah. So it's essentially the most efficient way to visualize the change of state of the computer as the program is running. That's what you mean by debugger. Yeah, it's a timeless debugger. So you can rewind just as easily as going forward. Think about if you're using GDB, you have to put a watch on a variable. If you want to see if that variable changes, in Kira, you can just click on that variable and then it shows every single time when that variable was changed or accessed. Think about it like Git for your computer's the run lock. So there's like a a deep log of the state of the computer as the program runs and you can rewind. Why is that maybe it is maybe you can educate me what isn't that kind of debugging used more often? Ah, because the tooling's bad. Well, two things. One, if you're trying to debug Chrome, Chrome is a 200 megabyte binary that runs slowly on desktops. So that's going to be really hard to use for that. But it's really good to use for like CTFs and for boot roms and for small parts of code. So it's hard if you're trying to debug like massive systems. What's the CTF and what's the boot rom? The boot rom is the first code that executes the minute you give power to your iPhone. Okay. And CTF were these competitions that I played capture the flag. Capture the flag. I was going to ask you about that. What are those look at? I'll watch a couple videos on YouTube. Those look fascinating. What have you learned about maybe at the high level of vulnerability of systems from these competitions? The like I feel like like in the heyday of CTFs, you had all of the best security people in the world challenging each other and coming up with new toy exploitable things over here and then everybody, okay, who can break it? And when you break it, you get like there's like a file in the server called flag. And then there's a program running listening on a socket that's vulnerable. So you write an exploit, you get a shell, and then you cat flag and then you type the flag into like a web-based scoreboard and you get points. So the goal is essentially to find an exploit in a system that allows you to run shell, to run arbitrary code on that system. That's one of the categories. That's like the ponable category. Ponable? Yeah, ponable. It's like you know you pon the program. It's a program. You know, for personal, I apologize. I'm going to say it's because I'm Russian, but maybe you can help educate me some video game like misspelled own way back in the day. Yeah, and it's just, I wonder if there's a definition and I have to go to Urban Dictionary for it. Okay, so what was the heyday of CTF, by the way, but was it what decade are we talking about? I think like, I mean, maybe I'm biased because it's the era that I played. But like 2011 to 2015, because the modern CTF scene is similar to the modern competitive programming scene. You have people who like do drills. You have people who practice. And then once you've done that, you've turned it less into a game of generic computer skill and more into a game of, okay, you remember, you drill on these five categories. And then before that, it wasn't, it didn't have like as much attention as it had. I don't know, there were like, I won $30,000 in Korea for one of these competitions. Oh, we crap. Yeah, they were, they were, that means, I mean, money's money, but that means it was, that was probably good people there. Exactly. Yeah. Are the challenges human constructed or are they grounded in some real flaws in real systems? Usually, they're human constructed, but they're usually inspired by real flaws. What kind of systems are imagined is really focused on mobile. Like, what has vulnerabilities these days? Is it primarily mobile systems like Android? Oh, everything does. Still. Yeah, of course. The price has kind of gone up because less and less people can find them. And what's happened in security is now if you want to like jailbreak an iPhone, you don't need one exploit anymore, you need nine. Nine, chain together. What would it be? Yeah. Wow. Okay. So it's really, what's the, what's the benefit of speaking higher level philosophically about hacking? I mean, it sounds from everything I've seen about you. You just love the challenge and you don't want to do anything. You don't want to bring that exploit out into the world and doing the actual letter run wild. You just want to solve it and then you go on to the next thing. Oh, yeah. I mean, doing criminal stuff is not really worth it. And I'll actually use the same argument for why I don't do defense for why I don't do crime. If you want to defend a system, say the system has 10 holes, right? If you find nine of those holes as a defender, you still lose because the attacker gets in through the last one. If you're an attacker, you only have to find one out of the 10. But if you're a criminal, if you log on with a VPN, nine out of the 10 times, but one time you forget, you're done because you're caught. Okay. Because you only have to mess up once to be caught as a criminal. That's why I'm not a criminal. But okay, let me, because let's have any discussion with somebody just at a high level about nuclear weapons, actually. Why we're having blown ourselves up yet. And my feeling is all the smart people in the world, if you look at the distribution of smart people, smart people are generally good. And then some of the person I was talking to Sean Carroll, the physicist, and you was saying no good and bad people are evenly distributed amongst everybody. My sense was good hackers are in general good people. And they don't want to mess with the world. What's your sense? I'm not even sure about that. Like, if a nice life crime wouldn't get me anything. But if you're good and you have these skills, you probably have a nice life too, right? Right. You can use the farther things. But is there an ethical? Is there some, is there a little voice in your head that says, well, yeah, if you could hack something to where you could hurt people. And you could earn a lot of money doing it though. Not hurt physically, perhaps, but disrupt their life in some kind of way. Is there a little voice that says, well, two things. One, I don't really care about money. So like the money wouldn't be an incentive. The thrill might be an incentive. But when I was 19, I read crime and punishment. Right. Good. That was another, that was another great one that talked me out of ever really doing crime. Because it's like, that's going to be me. I get away with it, but I'm just running my head. Even if I got away with it, you know, and then you do crime for long enough, you'll never get away with it. That's right. In the end, that's a good reason to be good. I wouldn't say I'm good. I would just say I'm not bad. You're a talented programmer and a hacker in a good positive sense of the word, word you've played around, fond vulnerabilities in various systems. What have you learned broadly about the design of systems and so on from that from that whole process? You learn to not take things for what people say they are, but you look at things for what they actually are. Yeah. I understand that's what you tell me it is, but what does it do? And you have nice visualization tools to really know what it's really doing. Oh, I wish I'm a better programmer now than I was in 2014. I said, that was the first tool that I wrote that was usable. I wouldn't say the code was great. I still wouldn't say my code is great. So how was your evolution as a programmer? Except practice. You started with C. What's your point? Did you pick up Python? Because you're pretty big in Python now. Now, yeah. In college, I went to Carnegie Mellon when I was 22. I went back. I'm like, I'm going to take all your hardest CS courses. We'll see how I do. Did I miss anything by not having a real undergraduate education? It took operating systems, compilers, AI, and they're like freshman, weeder math course. And... Operating systems, some of those classes you mentioned, you tell, actually. They're great. At least the 2012, circa 2012 operating systems and compilers were two of the, they were the best classes I've ever taken in my life. Because you write an operating system and you write a compiler. I wrote my operating system in C and I wrote my compiler in Haskell, but Haskell somehow I picked up Python that semester as well. I started using it for the CTFs, actually. That's when I really started to get into CTFs and CTFs you're all to race against the clock. So I can't write things in C. Oh, there's a clock opponent. So you really want to use the programming language just so you can be fastest. 48 hours. Pone is many of these challenges as you can. Pone. Yeah. You got like 100 points of challenge. Whatever team gets the most. You were both a Facebook and Google for a brief stint. Yeah. With Project Zero actually at Google for five months. Where you develop Kira. What was Project Zero about in general? Just curious about the security efforts in these companies. Well, Project Zero started the same time I went there. What years are there? 2015. 2015. So that was right at the beginning of Project Zero. It's small. It's Google's offensive security team. I'll try to give the best public facing explanation that I can. So the idea is basically these vulnerabilities exist in the world. Nation states have them. Some high-powered bad actors have them. Sometimes people will find these vulnerabilities and submit them in bug bounties to the companies. But a lot of the companies don't only care. They don't even fix the bug. They just hurt for there to be a vulnerability. So Project Zero is like we're going to do it different. We're going to announce a vulnerability and we're going to give them 90 days to fix it. And then whether they fix it or not, we're going to drop the zero day. Oh, wow. We're going to drop the weapon. That's so cool. That is so cool. I love that deadlines. Oh, that's so cool. Give them real deadlines. Yeah. And I think it's done a lot for moving the industry forward. I watched your coding sessions on the eStreamed online. You code things up. Basic projects usually from scratch. I would say sort of as a program myself just watching you that you type really fast and your brain works in both brilliant and chaotic ways. I don't know if that's always true, but certainly for the live streams. So it's interesting to me because I'm more I'm much slower and systematic and careful. And you just move, I mean, probably in order of magnitude faster. So I'm curious, is there a method to your madness? Is this is just who you are? There's pros and cons. There's pros and cons to my programming style. And I'm aware of them. If you ask me to like, like get something up and working quickly with like an API that's kind of undocumented, I will do this super fast because I will throw things at it until it works. If you ask me to take a vector and rotate it 90 degrees and then flip it over the xy plane, I'll spam program for two hours and won't get it. Oh, because it's something that you could do with a sheet of paper or think through design. And then just you really just throw stuff at the wall and you get so good at it that it usually works. I should become better at the other kind as well. Sometimes I will do things methodically. It's nowhere near as entertaining on the Twitch streams. I do exaggerated a bit on the Twitch streams as well. The Twitch streams, I mean, what do you want to see a game or you want to see actions for me? Right? I'll show you APM for programming. Yeah. I recommend people go to it. I think I watched probably several hours of you. Like I've actually left you programming in the background while I was programming because you made me, it was like watching a really good gamer. It's like, energizes you because you're like moving so fast. So it's awesome. It's inspiring. And it made me jealous that like because my own programming is inadequate in terms of speed. So I was like, so I'm twice as frantic on the live streams as I am when I code without. Oh, it's super entertaining. So I wasn't even paying attention to what you were coding, which is great. So it's just watching you switch windows and VAM I guess these are the most. Yeah, the Viven screen. I've developed a workflow Facebook and to talk about how do you learn new programming tools, ideas, techniques these days? What's your like methodology for learning new things? So I wrote for comma, the distributed file systems out in the world are extremely complex. Like if you want to install something like like like, like, Seph. Seph is I think the like open infrastructure, distributed file system or there's like newer ones like seaweed FS. But these are all like 10,000 plus line projects. I think some of them are even 100,000 line. And just configuring them was a nightmare. So I wrote one. It's 200 lines. And it's it uses like engine X to the line servers and has a little master server that I wrote and go. And the way I go. Wow. This if I would say that I'm proud per line of any code I wrote, maybe there's some exploits that I think are beautiful and then this is 200 lines. And just the way that I thought about it, I think was very good. And the reason it's very good is because that was the fourth version of it that I wrote and I had three versions that I threw away. You mentioned, do you say go? I've already go. Yeah. And go. So that was at a functional language. I forget what go is that go is Google's language, right? Um, it's not functional. It's some it's like in a way it's C plus plus, but easier. It's it's strongly typed. It has a nice ecosystem around it. When I first looked at it, I was like, this is like Python, but it takes twice as long to do anything. Yeah. Now that I've open pilot is migrating to C, but it still has large Python components, I now understand why Python doesn't work for large code bases and why you want something like go. Interesting. So why why doesn't Python work for so even most, uh, speaking for myself at least, like we do a lot of stuff, basically demo level work with autonomous vehicles. And most of the work is Python. Yeah. Why doesn't Python work for large code bases? Because well, lack of type checking is is a big air is creeping. Yeah. And like you don't know the compiler can tell you like nothing. Right. So everything is either, uh, you know, like like syntax errors, fine. But if you misspell a variable and Python, the compiler won't catch that. There's like linter is that can catch it some other time. There's no types. This is really the biggest, uh, downside. And then we'll Python slow, but that's not related to it. So well, maybe it's kind of related to it. So it's like, so what's what's in your toolbox these days? There's a Python. What else go? I need to move to something else. My my adventure into dependent type languages. I love these languages. They just have like syntax from the 80s. What do you think about JavaScript? ES6, like the modern type script. JavaScript is the whole ecosystem is unbelievably confusing. Right. NPM updates a package from 022 to 025. And that breaks your babble linter which translates your ES5 into ES6, which doesn't run on. So why do I have to compile my JavaScript again? Huh? It may be the future though. You think about, I mean, uh, I've embraced JavaScript recently, just because just like I've continued to embrace PHP, it seems that these worst possible languages live on for a long, is that cockroaches never die. Yeah. Well, it's in the browser and it's fast. It's fast. Yeah. It's in the browser and compute might say become, you know, the browser. It's unclear what the role of the browsers in terms of distributed computation in the future. So JavaScript is definitely here to stay. Yeah. It's interesting. If a time's vehicles will run on JavaScript one day, I mean, you have to consider these possibilities. Well, all our debug tools are JavaScript. Uh, we actually just open source them. We have a tool explorer which you can annotate your disengagement and we have tool cabana, which lets you analyze the canned traffic from the car. So basically anytime you're visualizing something about the log you're using JavaScript. Well, the web is the best UI toolkit by far. Yeah. Um, so, and then, you know what, you're coding a JavaScript. We have a React guy. He's good for React. Nice. Let's get into it. So let's talk a ton of vehicles. Uh, you found a comma AI. Let's, uh, at a high level, how did you get into the world of vehicle automation? Can you also just, for people who don't know, tell the story of comma AI? Sure. So I was working at this, hey, I start up and, uh, a friend approached me and he's like, dude, I don't know where this is going, but the coolest apply day I problem today is self-driving cars. I'm like, well, absolutely. You want to meet with Elon Musk and, uh, uh, he's looking for somebody to build a vision system for, uh, autopilot. This is when there was still an AP one. They were still using mobile. I, Elon back then was looking for a replacement. And he brought me in and we talked about a contract where I would deliver something that meets mobile eye level performance. Uh, I would get paid $12 million if I could deliver it tomorrow. And I would lose $1 million for every month that didn't deliver. Yeah. Um, so I was like, okay, this is a great deal. This is a super exciting challenge. You know what, even if it takes me 10 months, I get $2 million, it's good. Maybe I can finish up in five. Maybe I don't finish it at all. And I get paid nothing and I'll work for 12 months for free. So maybe, uh, just take a pause on that. I'm also curious about this because I've been working in robotics for a long time. And I'm curious to see a person like you just step in and sort of, um, somewhat naive, but brilliant, right? So that's the best place to be because you basically full steam take on a problem. How confident, how, from that time, because you know, a lot more now, at that time, how hard do you think it is to solve all of the time I was driving? I remember I suggested to Elon in the meeting, um, putting a GPU behind each camera to keep the compute local. This is an incredibly stupid idea. I leave the meeting 10 minutes later and I'm like, I could have spent a little bit of time thinking about this problem. What is it? What I wanted to, uh, just send all your cameras to one big GPU. You're much better off doing that. Oh, sorry. You said behind every camera. Have a small GPU. I was like, oh, I'll put the first few layers of my com there. Like, why'd I say that? That's possible. It's possible, but it's a bad idea. It's not obviously a bad idea. Pretty obviously bad. But whether it's actually a bad idea or not, I left that meeting with Elon, like beating myself up. I'm like, why'd I say something stupid? Yeah, you haven't, like you haven't at least like thought through every aspect. He's very sharp, too. Like usually in life, I get away with saying stupid things and then kind of course, oh, right, right away, you call me out about it. And like usually in life, I get away with saying stupid things and then like people will, uh, you know, a lot of times people don't even notice. And I'll like correct it and bring the conversation back. But with Elon, it was like, nope. Like, okay, well, that's not at all why the contract fell through. I was much more prepared the second time I met him. Yeah. But in general, how hard did you think it, like 12 months is, uh, is a tough, uh, timeline? Oh, I just thought I'd clone mobile I IQ three. I didn't think I solved level five self-driving or anything. So the goal there was to do lane keeping, uh, good, good lane keeping. I saw my friend showed me the outputs from a mobile I and the outputs from a mobile I was just basically two lanes at a position of a lead car. I'm like, I can, I can gather a data set and train this net in, in weeks. And I did. Well, first time I tried the implementation of mobile I and a Tesla was really surprised how good it is. Uh, it's quite incredibly good because I thought it's, just because I've done a lot of computer vision, I thought it'd be a lot harder to create a system that that's stable. Uh, so that was personally surprised. It's, you know, uh, have to admit it, because I was kind of skeptical before trying it. Because I thought, uh, it would go in and out a lot more. It would get disengaged a lot more. And it's pretty robust. Uh, so what, how, how, how, how hard is the problem when you, when you tackle it? So I think AP one was great. Like, uh, Elon talked about disengagement on the 405 down in LA with like the lane marks are kind of faded. Um, and the mobile I system would drop out. Uh, like I had something up and working that I would say was like the same quality in three months. Same quality, but how do you know? You, you, you say stuff like that? Yeah. But you can't, uh, and I love it. But, uh, well, the question is you can't, you're kind of going by feel because you go out absolutely. Absolutely. Like, like I would take, I hadn't, I borrowed my friend's Tesla. Yeah. I would take AP one out for a drive. Yeah. And then I would take my system out for a drive and seems reasonably like the same. So the 405, how hard is it to create something that could actually be a product that's deployed? I mean, uh, I've read an article where Elon just, uh, responded, said something by you saying that, um, to build autopilot is, uh, is more complicated than a single George Hodge. A level job. How hard is that job to create something that would work across the globally? Um, what do the globally is the challenge? But Elon followed that up by saying it's going to take two years in a company of 10 people. Yeah. And here I am four years later with a company of 12 people. And I think we still have another two to go. Two years. So yeah. So what do you think, uh, what do you think about, uh, how does this progressing with autopilot of V2, V3? I think we've kept pace with them pretty well. I think Navigate and autopilot is terrible. We had some demo features internally of the same stuff. And we would test it. And I'm like, I'm not shipping this even as like open source software to people. What do you think is terrible? Consumer reports does a great job of describing it. Like when it makes a lane change, it does it worse than a human. You shouldn't ship things like autopilot, open pilot, they lane keep better than a human. What if you turn it on for a stretch of, highway, like an hour long, it's never going to touch a lane line. Human will touch probably a lane line twice. You just inspired me. I don't know if you're grounded in data on that. I read a paper. Okay. But no, but that's interesting. Uh, I wonder actually how often we touch lane lines in general, like a little bit because it is, I could answer that question pretty easily with the common data. Yeah. I'm curious. I never answered it. I don't know. Yeah. I just too was like my personal. It feels, it feels right. But that's interesting because every time you touch a lane, it's a source of a little bit of stress and kind of lane keeping is removing that stress. That's all to me. The big, the biggest value add honestly is just removing the stress of having the stain lane. And I think honestly, I don't think people fully realize, first of all, that that's a big value add. Uh, but also that that's all it is. And that not only, I find it a huge value add. I drove down, when we moved to San Diego, I drove down in an enterprise or an intercar and I missed it. So I missed having the system so much. It's so much more tiring to drive without it. It's, it is that lane centering that's the key feature. Yeah. And in a way, it's the only feature that actually adds value to people's lives in autonomous vehicles today. Waymo does not add value to people's lives. It's a more expensive slower, slower Uber. Maybe someday it'll be this big cliff where it adds value. But I don't usually be this fast. I haven't talked to this is good. Uh, because I haven't, I haven't intuitively, but I think we're making it explicit now. I, I actually believe that really good lane keeping is a reason to buy a car. We'll be a reason to buy a car. And it's a huge value add. I've never, until we just started talking about it, I haven't really quite realized that that I've felt with Elon's chase of level four is not the correct chase. It was on because you should just say Tesla has the best as if from a Tesla perspective, say, Tesla has the best lane keeping. Kamai, I should say, Kamai is the best lane keeping. And that is it. Yeah. Yeah. So do you think you have to do the longitudinal as well? You can't just lane keep. You have to do ACC, but ACC is much more forgiving than lane keep, especially in the highway. By the way, are you, uh, Kamai eyes camera only, correct? Uh, no, we use the radar. We from the car, you're able to get the, oh, we can't do a camera only now. It's gotten to the point, but we leave the radar there as like a, it's, it's fusion now. Okay. So let's maybe talk through some of the system specs on the hardware. What, what's, what's the hardware side of, uh, when you're providing, what's the capabilities on the software side with open pilot and so on? So open pilot, as the, the box that we sell that it runs on, it's a phone in a plastic case. It's nothing special. We sell it without the software. So you're like, you know, you buy the phone, it's just easy. It'll be easy setup, but it's sold with no software. Open pilot right now is about to be 0.6. When it gets to 1.0, I think we'll be ready for a consumer product. We're not going to add any new features. We're just going to make the lane keeping really, really good. Okay. I got. So what do we have right now? It's a Snapdragon 820. It's a Sony IMX 298 forward facing camera driver monitoring camera. It's just a selfie cam on the phone and a can transceiver. It's a thing called pandas. And they talk over USB to the phone and then they have three canvases that they talk to the car. One of those canvases is the radar can bus. One of them is the main car can bus and the other one is the proxy camera can bus. We leave the existing camera in place so we don't turn AEB off. Right now we still turn AEB off if you're using our longitudinal, but we're going to fix that before 1.0. Got it. Wow. That's cool. So in its can both ways. So how are you able to control vehicles? So we proxy the vehicles that we work with already have lane keeping assist system. So lane keeping assist can mean a huge variety of things. It can mean it will apply a small torque to the wheel after you've already crossed a lane line by a foot, which is the system in the older Toyota's versus like I think Tesla still calls it lane keeping assist where it'll keep you perfectly in the center of the lane on the highway. You can control like you would enjoy the cars. So these cars already have the capability of drive by wire. So is it is it trivial to convert a car that it operates with it's open pile is able to control the steering. Oh, a new car or a car that we so we have support now for 45 different makes of cars. What are what are the cars general? Mostly Honda's and Toyota's. We support almost every Honda and Toyota made this year. And then a bunch of GMs, a bunch of Subaru's, a bunch of GMS. It just doesn't have to be like a pre-s it could be Corolla as well. The 2020 Corolla is the best car with open pilot. It just came out there. The actuator has less lag than the older Corolla. I think I start watching a video with you. I mean the way you make videos awesome. Literally at the dealership streaming. Yeah, and basically like if stuff goes a little wrong you just go with it. Yeah, I love it. It's real. Yeah, it's real. That's so beautiful and it's so in contrast to the way other companies would put together a video like that. How do I like to do it good. I mean if you become super rich one day in success, I hope you keep it that way because I think that's actually what people love that kind of genuine. Oh, it's all that has value to me. Yeah. Money has no if I sell out to like make money. I sold out. It doesn't matter. What do I get? God, I don't want God. And I think Tesla has actually has a small inkling of that as well. With autonomy day, they did reveal more than, of course, this marketing communications you can tell, but it's more than most companies will reveal, which is I hope they go towards that direction more other companies GM Ford. Oh, Tesla is going to win level five. They really are. So let's talk about it. You think the year focused on level two. Currently, currently. We're going to be one to two years behind Tesla, getting to level five. Okay. We're Android, right? We're Android. You're Android. I'm just saying once Tesla gets it, we're one to two years behind. I'm not making any time one on when Tesla is going to get that's right. You did that's brilliant. I'm sorry Tesla investors. If you think you're going to have an autonomous robot taxi fleet by the end of the year. Yes, that's all bet against that. So what do you think about this? The most level four companies are kind of just doing their usual safety driver doing full autonomy kind of testing. And then Tesla does basically trying to go from lane keeping to full autonomy. What do you think about that approach? How successful would it be? It's a ton better approach. Because Tesla is gathering data on a scale that none of them are. They're putting real users behind the behind the wheel of the cars. It's I think the only strategy that works. The incremental. Well, so there's a few components to test approach that's more than just incremental as we spoke with is the one is the software. So over the air air software updates necessity. I mean, wait a moment. Crews have those two. Those aren't. But those differentiated things from the automakers. Right. No lane keeping assistance systems have no cars will be in keeping system have that except Tesla. Yeah. And the other one is the data the other direction, which is the ability to query the data. I don't think they're actually collecting as much data as people think, but the ability to turn on collection and turn it off. So I'm both in the robotics world and the psychology human factors world. Many people believe that level to autonomy is problematic because of the human factor. Like the more the task is automated, the more there's a vigilance decrement, you start to fall asleep, you start to become complacent, start texting more and so on. Do you worry about that? Because if you're talking about transition from lane keeping to full autonomy, if you're spending 80% of the time, not supervising machine. Do you worry about what that means to say to the other drivers? One, we don't consider open pilot to be one point. I want to we have 100% driver monitoring. You you can cheat right now, our driver monitoring system. There's a few ways to cheat it. They're pretty obvious. We're working on making that better before we ship a consumer product that can drive cars. I want to make sure that I have driver monitoring that you can't cheat. What's like a successful driver monitoring system look like? Is it all a budget? Keep your eyes on the road? Well, a few things. So that's what we went with at first for driver monitoring. I'm checking. I'm actually looking at where your head is looking. The camera's not that high resolution eyes are a little bit hard to get. Well, head is this big. I mean that head is good. And actually a lot of it just is psychology wise to have that monitor constantly there. It reminds you that you have to be paying attention. But we want to go further. We just hired someone full time to come on to do the driver monitoring. I wanted to detect phone in frame. And I want to make sure you're not sleeping. How much does the camera see of the body? This one not enough. Not enough. The next one. Everything. Well, it's interesting. Fish Act as we have we're doing just data collection that real time. But fish eye is a beautiful being able to capture the body. And the smartphone is really like the biggest problem. I'll show you I can show you one of the pictures from our new system. Awesome. So you're basically saying the driver monitoring will be the answer to that. I think the other point that you're raising your papers is good as well. You're not asking a human to supervise a machine without giving them the they can take over at any time. Right. How our safety model you can take over we disengage on both the gas or the brake. We don't disengage on steering. I don't feel you have to. But we just engage on gas or brake. So it's very easy for you to take over and it's very easy for you to reengage. That switching should be super cheap. The cars that require even autopilot requires a double press. That's almost I see how to like that. And then the cancel to cancel in autopilot. You either have to press cancel which no one knows what that is. So they press the brake. But a lot of times you don't want to press the brake. You want to press the gas. So you should cancel the gas for wiggle the steering wheel which is bad as well. Wow. That's brilliant. Haven't heard anyone articulate that point. Oh, I thought it's all I think about. It's the because I think I think actually Tesla has done a better job than most automakers at making that frictionless. But you just described that it could be even better. I love supercruise as an experience once it's engaged. Yeah. I don't know if you've used it but getting the thing to try to engage. Yeah. I've used the driven supercruise a lot. So what's your thoughts on the supercruise system? I do disengage supercruise and it falls back to ACC. So my car is still accelerating. It feels weird. Otherwise, when you actually have supercruise engaged on the highway, it is phenomenal. We bought that Cadillac. We just sold it. But we bought it just to experience this. And I wanted everyone in the office to be like, this is what we're striving to build. GM pioneering with the driver monitoring. You like their driving monitoring system? It has some bugs. If there's a sun shining back here, it'll be blind to you. But overall, mostly, yeah. That's so cool that you know all this stuff. I don't often talk to people that because it's such a rare car, unfortunately. We lost like 25k in the deprecation, but it feels worth it. I was very pleasantly surprised that GM system was so innovative. And really, it wasn't advertised much, wasn't talked about much. And I was nervous that it would die, that it would disappear. Well, I did. They put it on the wrong car. They should have put it on the bolt and not some weird Cadillac that nobody bought. I think that's going to be into, they're saying at least it's going to be into their entire fleet. So what do you think about, as long as we're on the driver monitoring, what do you think about, you know, a must claim that driver monitoring is not needed? Normally, I love his claims. That one is stupid. That one is stupid. And you know, he's not going to have his level five fleet by the end of the year. Hopefully, he's like, okay, I was wrong. I'm going to add driver monitoring because when these systems get to the point that they're only messing up once every thousand miles, you absolutely need driver monitoring. So let me play, because I agree with you, but let me play devil's advocate. One possibility is that without driver monitoring, people are able to monitor self-regulate monitor themselves. You know, that your ideas are seeing all the people sleeping in tassels? Yeah. Well, I'm a little skeptical of all the people sleeping in tassels because I've stopped paying attention to that kind of stuff because I want to see real data. It's too much glorified. It doesn't feel scientific to me. So I want to know, you know, how many people are really sleeping in tassels versus sleeping? I was driving here, sleep deprived in a car with no automation. I was falling asleep. I agree that it's hypey. It's just like, you know what? If you want to put driver monitoring, I ran it. My last autopilot experience was I ran it a model three in March and drove it around. The wheel thing is annoying and the reason the wheel thing is annoying, we use the wheel thing as well, but we don't disengage on wheel. For Tesla, you have to touch the wheel just enough to trigger the torque sensor to tell it that you're there, but not enough as to disengage it, which don't use it for two things. Don't disengage on wheel. You don't have to. That whole experience, wow, beautiful put. They, all those elements, even if you don't have driver monitoring, that whole experience needs to be better. Driver monitoring, I think, would make, I mean, I think supercruises a better experience once it's engaged over autopilot. I think supercruises are transition to engagement and disengagement are significantly worse. There's a tricky thing because if I were to criticize supercruises, it's a little too crude and I think like six seconds or something, if you look off road, they'll start warning you. It's a some ridiculously long period of time. And just the way, I think it's basically, it's a binary, it should be adapted. Yeah, it needs to learn more about you and needs to communicate what it sees about you more. Tesla shows what it sees about the external world. It would be nice to supercruise with tell us what it sees about the internal world. It's even worse than that. You press the button to engage and it just says supercruise unavailable. Yeah, why? Yeah, that transparency is good. We've renamed the driver monitoring packet to driver state. Driver state. We have car state packet, which has the state of the car and you're going to driver state packet, which has the driver. So what does it estimate their BAC? What's BAC? Blood alcohol coming up. You think that's possible with computer vision? Absolutely. To me, it's an open question. I haven't looked into too much. I seriously looked at the literature. It's not obvious to me that from the eyes and so on, you can tell. You might need stuff from the car as well. Yeah. You might need how they're controlling the car, right? And that's fundamentally at the end of the day what you care about. But I think, especially when people are really drunk, they're not controlling the car nearly as smoothly as they would look at them walking, right? They're the cars like an extension of the body. So I think you could totally detect. And if you could fix people who are drunk distracted asleep, if you fix those three, yeah, that's huge. That's huge. So what are the current limitations of open pilot? What are the main problems that still need to be solved? We're hopefully fixing a few of them in 06. We're not as good as auto pilot at stopgars. So if you're coming up to a red light at like 55, so it's the radar stopped car problem, which is responsible for two auto pilot accidents. It's hard to differentiate a stopped car from a like side post. Yeah, static object. So you have to fuse. You have to do this visually. There's no way from the radar data to tell the difference. Maybe you can make a map, but I don't really believe in mapping at all anymore. So wait, wait, wait. What? You don't believe in mapping. No. So you basically, the open pilot solution is saying react to the environment as you see. It's just like a human doing beings. And then eventually when you want to do navigate on open pilot, I'll train the net to look at ways. I'll run ways in the background. I'll train a content away using GPS at all. We use it to ground truth. We use it to very carefully ground truth the paths. We have a stack which can recover relative to 10 centimeters over one minute. And then we use that to ground truth exactly where the car went in that local part of the environment, but it's all local. How are you testing in general, just for yourself, like experiments and stuff. Where are you located? San Diego. San Diego. Yeah. Okay. What's basically a driver on there that collects some data and watch on the farm. We have a simulator now. And we have our simulators. Really cool. Our simulator is not, it's not like a unity based simulator. Our simulator lets us load in real estate. What do you mean? We can load in a drive and simulate what the system would have done on the historical data. Oh, nice. Interesting. So what? Yeah. Right now, we're only using it for testing. But as soon as we start using it for training, that's it. That's it. That's for testing. What's you're feeling about the real world versus simulation? Do you like simulation for training if this moves to training? So we have to distinguish two types of simulators. Right. There's a simulator that is completely fake. I could get my car to drive around in GTI. I feel that this kind of simulator is useless. You're never, there's so many. My analogy here is like, okay, fine. You're not solving the computer vision problem, but you're solving the computer graphics problem. Right. And you don't think you can get very far by creating ultra realistic graphics. No, because you can create ultra realistic graphics of the road. Now create ultra realistic behavioral models of the other cars. Oh, well, I'll just use myself driving. No, you won't. You need real, you need actual human behavior because that's what you're trying to learn. The driving does not have a spec. The definition of driving is what humans do when they drive. Whatever Waymo does, I don't think it's driving. Right. Well, I think it's your Waymo and others if there's any use for reinforcement learning, I've seen it used quite well. I study pedestrian is a lot to is try to train models from real data of how pedestrians move and try to use reinforcement learning models to make pedestrians move in human-like ways. By that point, you've already gone so many layers. You detected a pedestrian. Did you, did you hand code the feature vector of their state? Did you guys learn anything from computer vision before deep learning? Well, okay, I feel like this is a perception to you is the sticking point. What's the hardest part of this tag here? There is no human understandable feature vector separating perception and planning. That's the best way I can put that. There is no, so it's all together and it's a joint problem. So you can take localization. Localization and planning, there is a human understandable feature vector between these two things. I have three degrees position, three degrees orientation, and those derivatives, maybe those second derivatives. That's human understandable. That's physical. The between perception and planning. So Waymo has a perception stack and then a planner. One of the things Waymo does right is they have a simulator that can separate those two. They can replay their perception data and test their system, which is what I'm talking about about like the two different kinds of simulators. There's the kind that can work on real data and this kind of can't work on real data. Now, the problem is that I don't think you can hand code a feature vector. Like you have some list of like, well, here's my list of cars in the scenes. Here's my list of pedestrians in the scene. This isn't what humans are doing. What are humans doing? Global. Some some saying that's too difficult to hand a engineer. I'm saying that there is no state vector given a perfect, I could give you the best team of engineers in the world to build a perception system and the best team to build a planner. All you have to do is define the state vector that separates those two. I'm missing the state vector that separates those two. What do you mean? What is the output of your perception system? I'll put it in the perception system. There's several ways to do it. One is the slammer bonus localization. The other is drivable area, drivable space. And then there's the different objects in the scene. And different objects in the scene over time, maybe to give you input, to then try to start modeling the trajectories of those objects. Sure. That's it. I can give you a concrete example of something you missed. What's that? So say there's a bush in the scene. Humans understand that when they see this bush that there may or may not be a car behind that bush. Drivable area and a list of objects does not include that. Humans are doing this constantly at the simplest intersections. So now you have to talk about a cluded area. Right. Right. But even that, what do you mean by a cluded? Okay. So I can't see it. Well, if it's the other side of a house, I don't care. What's the likelihood that there's a car in that a cluded area? Right. And if you say, okay, we'll add that, I can come up with 10 more examples that you can't add. Certainly a cluded area would be something that simulator would have because it's simulating the entire, you know, the occlusion is part of it. Occlusion is part of a vision stack. But what I'm saying is if you have a hand engineered, if your perception system output can be written in a spec document, it is incomplete. Yeah. I mean, I certainly it's hard to argue with that because in the end, that's going to be true. Yes. And I'll tell you what the output of our perception system is. It's a 1000, it's a 1024 dimensional vector train binarily. Oh, you're on that. No, no, it's the 1024 dimensions of who knows what? Because it's operating on real data. Yeah. And that's the perception. That's the perception state. Right. Think about a, think about an autoencoder for faces. Right. If you have an autoencoder for faces and you say it has 256 dimensions in middle, and I'm taking a face over here and projecting it to a face over here, yeah. Can you hand label all 256 of those dimensions? Well, no, but those are generally automatically, but they, but even if you tried to do it by hand, could you come up with a spec for your end between your encoder and your decoder? No, no, because that's how it was in design, but they're, no, no, but if you could design it, if you could design a face reconstructor system, could you come up with a spec? No, but I think we're missing here a little bit. I think that you're just being very poetic about expressing a fundamental problem of simulators, that they're going to be missing so much that the feature vector will just look fundamentally different from in the simulated world and the real world. I'm not making a claim about simulators. I'm making a claim about the spec division between perception and planning and planning, even in your system. Just in general. Right. Just in general. If you're trying to build a car that drives, if you're trying to hand code the output of your perception system, like saying like, here's a list of all the cars in the scene, here's a list of all the people, here's a list of the included areas, here's a vector of drivable areas, it's insufficient. And if you start to believe that, you realize that what Waymo and Cruiser doing is impossible. Currently, what we're doing is the perception problem is converting the scene into a chess board. And then you reason some basic reasoning around that chess board. And you're saying that really, there's a lot missing there. First of all, why are we talking about this? Because isn't this full autonomy? Is this something you think about? Oh, I want to win self-driving cars. So you're a real thing. So your definition of win includes level four or five. Level five. I don't think level four is a real thing. I want to build the alpha-go driving. So alpha-go is really end to end. Yeah. Yeah, it's end to end. And do you think this whole problem is that also kind of what you're getting at with the perception and the planning? Is that this whole problem, the right way to do it is really to learn the entire thing. I'll argue that not only is it the right way, it's the only way that's going to exceed human performance. Well, it's certainly true for Go. Everyone who tried to handcode Go things built human inferior things. And then someone came along and wrote some 10,000 line thing that doesn't know anything about Go that beat everybody. It's 10,000 lines. True. In that sense, the open question then that maybe I can ask you is driving is much harder than Go. The open question is how much harder? So how, because I think the Elon Musk approach here with planning and perception is similar to what you're describing, which is really turning into not some kind of modular thing, but really do formulate is learning problem and solve the learning problem with scale. So how many years at a put one is how many years would it take to solve this problem or just how hard is this freaking problem? Well, the cool thing is I think there's a lot of value that we can deliver along the way. I think that you can build lane keeping assist actually plus adaptive cruise control. Plus, okay, looking at ways extends to like all of driving. Yeah, most of driving. Oh, your adaptive cruise control treats red lights like cars. Okay. So let's jump around with you mentioned that you didn't like navigate an autopilot. Yeah. What advice? How would you make it better? Do you think as a feature that if it's done really well, it's a good feature? I think that it's too reliant on like hand coded hacks for like how does navigate an autopilot do a lane change? It actually does the same lane change every time and it feels mechanical. Humans do different lane changes. Humans sometime will do a slow one, sometimes do a fast one. Navigate an autopilot, at least every time I use it, it is the identical lane change. How do you learn? I mean, this is a fundamental thing actually is the breaking and accelerating something that's still testable probably does a better than most cars, but it still doesn't do a great job of creating a comfortable natural experience and navigate an autopilot is just lane changes and extension of that. So how do you learn to do natural lane change? So we have it and I can talk about how it works. So I feel that we have the solution for lateral, but we don't yet have the solution for longitudinal. There's a few reasons longitudinal is harder than lateral. The lane change component, the way that we train on it very simply is like our model has an input for whether it's doing a lane change or not. And then when we train the end to end model, we hand label all the lane changes because you have to. I struggled a long time about not wanting to do that, but I think you have to because you order the training data for the training data, right? We actually have an automatic ground truth or which automatically labels all the lane changes. Was that possible to automatically a little lane change? Yeah. And just like the lane I see when I cross this right and I don't have to get that high percent accuracy, but it's like 95 good enough. Okay. Now I set the bit when it's doing the lane change in the end to end learning. And then I set it to zero when it's not doing lane change. So now if I want it to do a lane change test time, I just put the bit to a one and it'll do a lane change. Yeah, but so if you look at the space of lane change, you know, some percentage, not 100% that we make as humans is not a pleasant experience because we messed some part of it up. Yeah, it's nerve wracking to change. If the look you're the sea, the desert accelerate, how do we label the ones that are natural and feel good? You know, that's the, because that's your ultimate criticism. The current navigate and autopilot just doesn't feel good. Well, the current navigate and autopilot is a hand coded policy written by an engineer in a room who probably went out and tested it a few times on the 280. Probably a more a better version of that, but yes. That's how we would have written it. Yeah, maybe Tesla, they tested it in. They might have been two engineers. Yeah. Um, no, but so if you learn the lane change, if you learn how to do a lane change from data, just like, just like you have a label that says lane change and then you put it in when you want to do the lane change, it'll automatically do the lane change. That's appropriate for the situation. Now to get at the problem of some humans do bad lane changes, we haven't worked too much on this problem yet. It's not that much of a problem in practice. My theory is that all good drivers are good in the same way and all bad drivers are bad in different ways. And we've seen some data to back this up. Well, beautifully put. So you just basically, if that's true, hypothesis, then you're tasked with the discover the good drivers. The good drivers stand out because they're in one cluster and the bad drivers are scattered all over the place and your net lines the cluster. Yeah, that's so you just learn from the good drivers and they're easy to cluster. In fact, we learned from all of them and then automatically learns the policy that's like the majority, but we'll eventually probably have to filter out. So if that theory is true, I hope it's true because the the counter theory is there is many clusters, maybe arbitrarily many clusters of good drivers because if there's one cluster of good drivers, you can at least discover a set of policies. You can learn a set of policies which would be
